Using TensorFlow backend.
/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
 
Found train data with correct size
 
 
Found test data with correct size
 
/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.
  warn("The default mode, 'constant', will be changed to 'reflect' in "
Augmentation data size (4750, 2) (794, 2)
/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)
(3800, 51, 51, 3) (3800,)
 
Pulling kfold 0 from previous runs
2018-08-09 17:31:00.382262: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-08-09 17:31:00.382639: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
 
Bad saved trial. Testing acc <0.9%. Rerunning ...
 
Train on 3800 samples, validate on 950 samples
Epoch 1/500
576s - loss: 1.4005 - acc: 0.5171 - val_loss: 3.8974 - val_acc: 0.1621
Epoch 2/500
574s - loss: 0.9569 - acc: 0.6711 - val_loss: 1.4523 - val_acc: 0.5158
Epoch 3/500
574s - loss: 0.7284 - acc: 0.7574 - val_loss: 2.1294 - val_acc: 0.4442
Epoch 4/500
574s - loss: 0.6033 - acc: 0.7887 - val_loss: 0.9769 - val_acc: 0.6895
Epoch 5/500
574s - loss: 0.5173 - acc: 0.8271 - val_loss: 1.4395 - val_acc: 0.6137
Epoch 6/500
574s - loss: 0.4750 - acc: 0.8355 - val_loss: 1.3208 - val_acc: 0.5937
Epoch 7/500
574s - loss: 0.4439 - acc: 0.8495 - val_loss: 0.6482 - val_acc: 0.7684
Epoch 8/500
574s - loss: 0.3752 - acc: 0.8634 - val_loss: 0.8883 - val_acc: 0.7189
Epoch 9/500
574s - loss: 0.3587 - acc: 0.8695 - val_loss: 0.6496 - val_acc: 0.7853
Epoch 10/500
574s - loss: 0.3206 - acc: 0.8842 - val_loss: 0.6054 - val_acc: 0.8074
Epoch 11/500
574s - loss: 0.2887 - acc: 0.8924 - val_loss: 0.7412 - val_acc: 0.7674
Epoch 12/500
574s - loss: 0.2758 - acc: 0.9042 - val_loss: 0.5126 - val_acc: 0.8284
Epoch 13/500
574s - loss: 0.2427 - acc: 0.9150 - val_loss: 0.6219 - val_acc: 0.8116
Epoch 14/500
574s - loss: 0.2358 - acc: 0.9121 - val_loss: 0.4286 - val_acc: 0.8747
Epoch 15/500
574s - loss: 0.2172 - acc: 0.9213 - val_loss: 0.4315 - val_acc: 0.8737
Epoch 16/500
574s - loss: 0.1733 - acc: 0.9405 - val_loss: 0.4704 - val_acc: 0.8589
Epoch 17/500
574s - loss: 0.1696 - acc: 0.9376 - val_loss: 0.3896 - val_acc: 0.8863
Epoch 18/500
574s - loss: 0.1239 - acc: 0.9547 - val_loss: 0.3075 - val_acc: 0.9084
Epoch 19/500
574s - loss: 0.0905 - acc: 0.9753 - val_loss: 0.3195 - val_acc: 0.9000
Epoch 20/500
574s - loss: 0.0755 - acc: 0.9745 - val_loss: 0.3244 - val_acc: 0.9063
Epoch 21/500
574s - loss: 0.0652 - acc: 0.9808 - val_loss: 0.3106 - val_acc: 0.9021
Epoch 22/500
574s - loss: 0.0632 - acc: 0.9797 - val_loss: 0.3237 - val_acc: 0.9084
Epoch 23/500
574s - loss: 0.0664 - acc: 0.9779 - val_loss: 0.3379 - val_acc: 0.9032
Epoch 24/500
574s - loss: 0.0585 - acc: 0.9826 - val_loss: 0.3312 - val_acc: 0.9042
Epoch 25/500
574s - loss: 0.0589 - acc: 0.9839 - val_loss: 0.3360 - val_acc: 0.9063
Epoch 26/500
574s - loss: 0.0526 - acc: 0.9855 - val_loss: 0.3448 - val_acc: 0.9084
Epoch 27/500
574s - loss: 0.0530 - acc: 0.9845 - val_loss: 0.3472 - val_acc: 0.9042
Epoch 28/500
574s - loss: 0.0466 - acc: 0.9876 - val_loss: 0.3871 - val_acc: 0.8989
Epoch 29/500
574s - loss: 0.0405 - acc: 0.9900 - val_loss: 0.3476 - val_acc: 0.9032
Epoch 30/500
574s - loss: 0.0403 - acc: 0.9876 - val_loss: 0.3629 - val_acc: 0.9084
Epoch 31/500
574s - loss: 0.0395 - acc: 0.9887 - val_loss: 0.4121 - val_acc: 0.8947
Epoch 32/500
574s - loss: 0.0375 - acc: 0.9892 - val_loss: 0.3916 - val_acc: 0.9042
Epoch 33/500
574s - loss: 0.0333 - acc: 0.9921 - val_loss: 0.3724 - val_acc: 0.9074
Epoch 34/500
574s - loss: 0.0313 - acc: 0.9926 - val_loss: 0.3648 - val_acc: 0.9074
Epoch 35/500
574s - loss: 0.0285 - acc: 0.9942 - val_loss: 0.3623 - val_acc: 0.9074
Epoch 36/500
574s - loss: 0.0279 - acc: 0.9939 - val_loss: 0.3648 - val_acc: 0.9116
Epoch 37/500
574s - loss: 0.0274 - acc: 0.9942 - val_loss: 0.3717 - val_acc: 0.9063
Epoch 38/500
574s - loss: 0.0253 - acc: 0.9932 - val_loss: 0.3653 - val_acc: 0.9032
Epoch 39/500
574s - loss: 0.0259 - acc: 0.9937 - val_loss: 0.3656 - val_acc: 0.9053
Epoch 40/500
574s - loss: 0.0258 - acc: 0.9932 - val_loss: 0.3731 - val_acc: 0.9084
Epoch 41/500
574s - loss: 0.0246 - acc: 0.9947 - val_loss: 0.3703 - val_acc: 0.9053
Epoch 42/500
574s - loss: 0.0248 - acc: 0.9945 - val_loss: 0.3792 - val_acc: 0.9095
Epoch 43/500
574s - loss: 0.0227 - acc: 0.9950 - val_loss: 0.3770 - val_acc: 0.9053
Epoch 44/500
574s - loss: 0.0188 - acc: 0.9961 - val_loss: 0.3924 - val_acc: 0.9074
Epoch 45/500
574s - loss: 0.0180 - acc: 0.9966 - val_loss: 0.3864 - val_acc: 0.9063
Epoch 46/500
574s - loss: 0.0261 - acc: 0.9942 - val_loss: 0.3849 - val_acc: 0.9074
Epoch 47/500
574s - loss: 0.0228 - acc: 0.9955 - val_loss: 0.3904 - val_acc: 0.9021
Epoch 48/500
574s - loss: 0.0200 - acc: 0.9955 - val_loss: 0.3901 - val_acc: 0.9032
Epoch 49/500
574s - loss: 0.0186 - acc: 0.9961 - val_loss: 0.3885 - val_acc: 0.9042
Epoch 50/500
574s - loss: 0.0182 - acc: 0.9961 - val_loss: 0.3874 - val_acc: 0.9053
Epoch 51/500
574s - loss: 0.0235 - acc: 0.9937 - val_loss: 0.3855 - val_acc: 0.9063
Epoch 52/500
574s - loss: 0.0215 - acc: 0.9961 - val_loss: 0.3873 - val_acc: 0.9063
Epoch 53/500
574s - loss: 0.0212 - acc: 0.9963 - val_loss: 0.3849 - val_acc: 0.9084
Epoch 54/500
574s - loss: 0.0226 - acc: 0.9955 - val_loss: 0.3819 - val_acc: 0.9063
Epoch 55/500
574s - loss: 0.0202 - acc: 0.9974 - val_loss: 0.3830 - val_acc: 0.9084
Epoch 56/500
574s - loss: 0.0205 - acc: 0.9961 - val_loss: 0.3818 - val_acc: 0.9084
Epoch 57/500
574s - loss: 0.0229 - acc: 0.9953 - val_loss: 0.3843 - val_acc: 0.9095
Epoch 58/500
574s - loss: 0.0182 - acc: 0.9958 - val_loss: 0.3838 - val_acc: 0.9074
Epoch 59/500
574s - loss: 0.0221 - acc: 0.9947 - val_loss: 0.3870 - val_acc: 0.9074
Epoch 60/500
574s - loss: 0.0238 - acc: 0.9947 - val_loss: 0.3903 - val_acc: 0.9042
Epoch 61/500
574s - loss: 0.0201 - acc: 0.9942 - val_loss: 0.3868 - val_acc: 0.9074
Epoch 62/500
574s - loss: 0.0200 - acc: 0.9955 - val_loss: 0.3861 - val_acc: 0.9074
Epoch 63/500
574s - loss: 0.0237 - acc: 0.9942 - val_loss: 0.3878 - val_acc: 0.9053
Epoch 64/500
574s - loss: 0.0192 - acc: 0.9953 - val_loss: 0.3875 - val_acc: 0.9084
Epoch 65/500
574s - loss: 0.0257 - acc: 0.9939 - val_loss: 0.3865 - val_acc: 0.9084
Epoch 66/500
574s - loss: 0.0189 - acc: 0.9971 - val_loss: 0.3868 - val_acc: 0.9042
Epoch 67/500
574s - loss: 0.0203 - acc: 0.9958 - val_loss: 0.3869 - val_acc: 0.9084
Epoch 68/500
574s - loss: 0.0194 - acc: 0.9971 - val_loss: 0.3850 - val_acc: 0.9084
Epoch 69/500
574s - loss: 0.0167 - acc: 0.9971 - val_loss: 0.3835 - val_acc: 0.9084
Epoch 70/500
574s - loss: 0.0201 - acc: 0.9963 - val_loss: 0.3835 - val_acc: 0.9084
Epoch 71/500
574s - loss: 0.0205 - acc: 0.9955 - val_loss: 0.3842 - val_acc: 0.9074
Epoch 72/500
574s - loss: 0.0222 - acc: 0.9968 - val_loss: 0.3848 - val_acc: 0.9084
Epoch 73/500
574s - loss: 0.0201 - acc: 0.9966 - val_loss: 0.3864 - val_acc: 0.9084
Epoch 74/500
574s - loss: 0.0194 - acc: 0.9961 - val_loss: 0.3838 - val_acc: 0.9084
Epoch 75/500
574s - loss: 0.0213 - acc: 0.9963 - val_loss: 0.3852 - val_acc: 0.9084
Epoch 76/500
575s - loss: 0.0210 - acc: 0.9955 - val_loss: 0.3857 - val_acc: 0.9084
Epoch 77/500
583s - loss: 0.0201 - acc: 0.9955 - val_loss: 0.3854 - val_acc: 0.9084
Epoch 78/500
583s - loss: 0.0213 - acc: 0.9953 - val_loss: 0.3843 - val_acc: 0.9074
Epoch 79/500
583s - loss: 0.0190 - acc: 0.9963 - val_loss: 0.3844 - val_acc: 0.9084
Epoch 80/500
583s - loss: 0.0215 - acc: 0.9953 - val_loss: 0.3855 - val_acc: 0.9084
Epoch 81/500
584s - loss: 0.0186 - acc: 0.9971 - val_loss: 0.3864 - val_acc: 0.9084
Epoch 82/500
584s - loss: 0.0196 - acc: 0.9958 - val_loss: 0.3851 - val_acc: 0.9084
Epoch 83/500
584s - loss: 0.0237 - acc: 0.9937 - val_loss: 0.3851 - val_acc: 0.9063
Epoch 84/500
584s - loss: 0.0215 - acc: 0.9953 - val_loss: 0.3858 - val_acc: 0.9084
Epoch 85/500
584s - loss: 0.0183 - acc: 0.9966 - val_loss: 0.3849 - val_acc: 0.9084
Epoch 86/500
584s - loss: 0.0252 - acc: 0.9947 - val_loss: 0.3857 - val_acc: 0.9084
Epoch 87/500
584s - loss: 0.0194 - acc: 0.9963 - val_loss: 0.3855 - val_acc: 0.9063
Epoch 88/500
584s - loss: 0.0216 - acc: 0.9945 - val_loss: 0.3856 - val_acc: 0.9074
Epoch 89/500
584s - loss: 0.0210 - acc: 0.9955 - val_loss: 0.3864 - val_acc: 0.9074
Epoch 90/500
584s - loss: 0.0209 - acc: 0.9968 - val_loss: 0.3874 - val_acc: 0.9084
Epoch 91/500
584s - loss: 0.0259 - acc: 0.9950 - val_loss: 0.3861 - val_acc: 0.9084
Epoch 92/500
584s - loss: 0.0188 - acc: 0.9953 - val_loss: 0.3851 - val_acc: 0.9084
Epoch 93/500
584s - loss: 0.0188 - acc: 0.9961 - val_loss: 0.3867 - val_acc: 0.9063
Epoch 94/500
584s - loss: 0.0250 - acc: 0.9937 - val_loss: 0.3869 - val_acc: 0.9084
Epoch 95/500
584s - loss: 0.0220 - acc: 0.9958 - val_loss: 0.3847 - val_acc: 0.9084
Epoch 96/500
584s - loss: 0.0290 - acc: 0.9924 - val_loss: 0.3841 - val_acc: 0.9084
Epoch 97/500
584s - loss: 0.0167 - acc: 0.9976 - val_loss: 0.3860 - val_acc: 0.9084
Epoch 98/500
584s - loss: 0.0195 - acc: 0.9955 - val_loss: 0.3876 - val_acc: 0.9074
Epoch 99/500
584s - loss: 0.0167 - acc: 0.9971 - val_loss: 0.3865 - val_acc: 0.9084
Epoch 100/500
584s - loss: 0.0195 - acc: 0.9961 - val_loss: 0.3845 - val_acc: 0.9084
Epoch 101/500
584s - loss: 0.0203 - acc: 0.9955 - val_loss: 0.3864 - val_acc: 0.9084
Epoch 102/500
584s - loss: 0.0148 - acc: 0.9987 - val_loss: 0.3860 - val_acc: 0.9084
Epoch 103/500
584s - loss: 0.0241 - acc: 0.9937 - val_loss: 0.3871 - val_acc: 0.9074
Epoch 104/500
584s - loss: 0.0236 - acc: 0.9942 - val_loss: 0.3870 - val_acc: 0.9084
Epoch 105/500
584s - loss: 0.0153 - acc: 0.9982 - val_loss: 0.3854 - val_acc: 0.9074
Epoch 106/500
584s - loss: 0.0213 - acc: 0.9939 - val_loss: 0.3851 - val_acc: 0.9074
Epoch 107/500
584s - loss: 0.0207 - acc: 0.9963 - val_loss: 0.3854 - val_acc: 0.9084
Epoch 108/500
584s - loss: 0.0208 - acc: 0.9955 - val_loss: 0.3857 - val_acc: 0.9074
Epoch 109/500
584s - loss: 0.0181 - acc: 0.9963 - val_loss: 0.3844 - val_acc: 0.9063
Epoch 110/500
584s - loss: 0.0196 - acc: 0.9961 - val_loss: 0.3862 - val_acc: 0.9063
Epoch 111/500
584s - loss: 0.0173 - acc: 0.9968 - val_loss: 0.3851 - val_acc: 0.9074
Epoch 112/500
584s - loss: 0.0197 - acc: 0.9947 - val_loss: 0.3864 - val_acc: 0.9084
Epoch 113/500
584s - loss: 0.0231 - acc: 0.9942 - val_loss: 0.3858 - val_acc: 0.9053
Epoch 114/500
584s - loss: 0.0220 - acc: 0.9947 - val_loss: 0.3850 - val_acc: 0.9074
Epoch 115/500
584s - loss: 0.0231 - acc: 0.9947 - val_loss: 0.3859 - val_acc: 0.9084
Epoch 116/500
584s - loss: 0.0205 - acc: 0.9958 - val_loss: 0.3879 - val_acc: 0.9074
Epoch 117/500
584s - loss: 0.0161 - acc: 0.9976 - val_loss: 0.3867 - val_acc: 0.9084
Epoch 118/500
584s - loss: 0.0211 - acc: 0.9958 - val_loss: 0.3876 - val_acc: 0.9084
Epoch 119/500
584s - loss: 0.0234 - acc: 0.9945 - val_loss: 0.3882 - val_acc: 0.9063
Epoch 120/500
584s - loss: 0.0199 - acc: 0.9955 - val_loss: 0.3866 - val_acc: 0.9074
Epoch 121/500
584s - loss: 0.0167 - acc: 0.9974 - val_loss: 0.3863 - val_acc: 0.9063
Epoch 122/500
584s - loss: 0.0181 - acc: 0.9958 - val_loss: 0.3866 - val_acc: 0.9084
Epoch 123/500
584s - loss: 0.0155 - acc: 0.9976 - val_loss: 0.3861 - val_acc: 0.9074
Epoch 124/500
584s - loss: 0.0188 - acc: 0.9958 - val_loss: 0.3859 - val_acc: 0.9084
Epoch 125/500
584s - loss: 0.0208 - acc: 0.9955 - val_loss: 0.3855 - val_acc: 0.9074
Epoch 126/500
584s - loss: 0.0167 - acc: 0.9976 - val_loss: 0.3877 - val_acc: 0.9084
Epoch 127/500
584s - loss: 0.0202 - acc: 0.9955 - val_loss: 0.3859 - val_acc: 0.9084
Epoch 128/500
584s - loss: 0.0224 - acc: 0.9947 - val_loss: 0.3870 - val_acc: 0.9084
Epoch 129/500
584s - loss: 0.0206 - acc: 0.9958 - val_loss: 0.3864 - val_acc: 0.9074
Epoch 130/500
584s - loss: 0.0217 - acc: 0.9961 - val_loss: 0.3850 - val_acc: 0.9074
Epoch 131/500
584s - loss: 0.0180 - acc: 0.9958 - val_loss: 0.3850 - val_acc: 0.9074
Epoch 132/500
584s - loss: 0.0222 - acc: 0.9947 - val_loss: 0.3848 - val_acc: 0.9074
Epoch 133/500
584s - loss: 0.0199 - acc: 0.9968 - val_loss: 0.3845 - val_acc: 0.9084
Epoch 134/500
584s - loss: 0.0212 - acc: 0.9945 - val_loss: 0.3858 - val_acc: 0.9074
Epoch 135/500
584s - loss: 0.0206 - acc: 0.9950 - val_loss: 0.3872 - val_acc: 0.9074
Epoch 136/500
584s - loss: 0.0214 - acc: 0.9961 - val_loss: 0.3878 - val_acc: 0.9084
Epoch 137/500
584s - loss: 0.0205 - acc: 0.9950 - val_loss: 0.3851 - val_acc: 0.9084
Epoch 138/500
584s - loss: 0.0226 - acc: 0.9942 - val_loss: 0.3867 - val_acc: 0.9074
Epoch 139/500
584s - loss: 0.0228 - acc: 0.9942 - val_loss: 0.3869 - val_acc: 0.9074
Epoch 140/500
584s - loss: 0.0245 - acc: 0.9937 - val_loss: 0.3857 - val_acc: 0.9084
Epoch 141/500
584s - loss: 0.0208 - acc: 0.9950 - val_loss: 0.3863 - val_acc: 0.9074
Epoch 142/500
585s - loss: 0.0197 - acc: 0.9950 - val_loss: 0.3842 - val_acc: 0.9084
Epoch 143/500
584s - loss: 0.0223 - acc: 0.9937 - val_loss: 0.3861 - val_acc: 0.9084
Epoch 144/500
584s - loss: 0.0197 - acc: 0.9961 - val_loss: 0.3867 - val_acc: 0.9084
Epoch 145/500
584s - loss: 0.0205 - acc: 0.9955 - val_loss: 0.3859 - val_acc: 0.9074
Epoch 146/500
584s - loss: 0.0203 - acc: 0.9958 - val_loss: 0.3864 - val_acc: 0.9084
Epoch 147/500
584s - loss: 0.0196 - acc: 0.9963 - val_loss: 0.3861 - val_acc: 0.9084
Epoch 148/500
584s - loss: 0.0180 - acc: 0.9963 - val_loss: 0.3859 - val_acc: 0.9084
Epoch 149/500
584s - loss: 0.0199 - acc: 0.9968 - val_loss: 0.3875 - val_acc: 0.9084
Epoch 150/500
584s - loss: 0.0219 - acc: 0.9950 - val_loss: 0.3863 - val_acc: 0.9084
Epoch 151/500
584s - loss: 0.0234 - acc: 0.9950 - val_loss: 0.3872 - val_acc: 0.9084
Epoch 152/500
585s - loss: 0.0183 - acc: 0.9968 - val_loss: 0.3851 - val_acc: 0.9084
Epoch 153/500
585s - loss: 0.0240 - acc: 0.9939 - val_loss: 0.3870 - val_acc: 0.9074
Epoch 154/500
584s - loss: 0.0224 - acc: 0.9947 - val_loss: 0.3878 - val_acc: 0.9053
Epoch 155/500
585s - loss: 0.0206 - acc: 0.9961 - val_loss: 0.3833 - val_acc: 0.9084
Epoch 156/500
585s - loss: 0.0193 - acc: 0.9958 - val_loss: 0.3845 - val_acc: 0.9084
Epoch 157/500
585s - loss: 0.0163 - acc: 0.9976 - val_loss: 0.3862 - val_acc: 0.9042
Epoch 158/500
585s - loss: 0.0180 - acc: 0.9968 - val_loss: 0.3871 - val_acc: 0.9074
Epoch 159/500
584s - loss: 0.0219 - acc: 0.9950 - val_loss: 0.3861 - val_acc: 0.9074
Epoch 160/500
585s - loss: 0.0218 - acc: 0.9953 - val_loss: 0.3879 - val_acc: 0.9074
Epoch 161/500
584s - loss: 0.0223 - acc: 0.9947 - val_loss: 0.3875 - val_acc: 0.9074
Epoch 162/500
584s - loss: 0.0227 - acc: 0.9945 - val_loss: 0.3873 - val_acc: 0.9084
Epoch 163/500
584s - loss: 0.0208 - acc: 0.9958 - val_loss: 0.3845 - val_acc: 0.9084
Training loss for fold 0 is 0.004674230396600539 with percent 99.94736842105263
Testing loss for fold 0 is 0.3648348343999762 with percent 91.15789479958384
 
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
585s - loss: 0.0990 - acc: 0.9763 - val_loss: 0.0898 - val_acc: 0.9747
Epoch 2/500
585s - loss: 0.1004 - acc: 0.9729 - val_loss: 0.0902 - val_acc: 0.9747
Epoch 3/500
585s - loss: 0.1084 - acc: 0.9753 - val_loss: 0.0903 - val_acc: 0.9747
Epoch 4/500
586s - loss: 0.1107 - acc: 0.9729 - val_loss: 0.0914 - val_acc: 0.9747
Epoch 5/500
586s - loss: 0.1087 - acc: 0.9737 - val_loss: 0.0910 - val_acc: 0.9747
Epoch 6/500
586s - loss: 0.1106 - acc: 0.9721 - val_loss: 0.0907 - val_acc: 0.9747
Epoch 7/500
585s - loss: 0.1043 - acc: 0.9758 - val_loss: 0.0904 - val_acc: 0.9747
Epoch 8/500
585s - loss: 0.1133 - acc: 0.9705 - val_loss: 0.0914 - val_acc: 0.9747
Epoch 9/500
585s - loss: 0.1064 - acc: 0.9732 - val_loss: 0.0906 - val_acc: 0.9747
Epoch 10/500
585s - loss: 0.1066 - acc: 0.9721 - val_loss: 0.0911 - val_acc: 0.9747
Epoch 11/500
585s - loss: 0.1076 - acc: 0.9753 - val_loss: 0.0904 - val_acc: 0.9747
Epoch 12/500
585s - loss: 0.1118 - acc: 0.9721 - val_loss: 0.0902 - val_acc: 0.9747
Epoch 13/500
585s - loss: 0.1001 - acc: 0.9711 - val_loss: 0.0908 - val_acc: 0.9747
Epoch 14/500
585s - loss: 0.1022 - acc: 0.9763 - val_loss: 0.0905 - val_acc: 0.9747
Epoch 15/500
585s - loss: 0.1168 - acc: 0.9711 - val_loss: 0.0907 - val_acc: 0.9747
Epoch 16/500
585s - loss: 0.1144 - acc: 0.9729 - val_loss: 0.0903 - val_acc: 0.9747
Epoch 17/500
585s - loss: 0.1123 - acc: 0.9708 - val_loss: 0.0901 - val_acc: 0.9747
Epoch 18/500
585s - loss: 0.1010 - acc: 0.9745 - val_loss: 0.0896 - val_acc: 0.9747
Epoch 19/500
585s - loss: 0.1087 - acc: 0.9732 - val_loss: 0.0896 - val_acc: 0.9747
Epoch 20/500
585s - loss: 0.1023 - acc: 0.9750 - val_loss: 0.0903 - val_acc: 0.9747
Epoch 21/500
585s - loss: 0.1057 - acc: 0.9745 - val_loss: 0.0907 - val_acc: 0.9747
Epoch 22/500
585s - loss: 0.1006 - acc: 0.9724 - val_loss: 0.0903 - val_acc: 0.9747
Epoch 23/500
585s - loss: 0.1005 - acc: 0.9745 - val_loss: 0.0902 - val_acc: 0.9747
Epoch 24/500
585s - loss: 0.1033 - acc: 0.9753 - val_loss: 0.0900 - val_acc: 0.9747
Epoch 25/500
585s - loss: 0.1066 - acc: 0.9724 - val_loss: 0.0903 - val_acc: 0.9747
Epoch 26/500
585s - loss: 0.1076 - acc: 0.9724 - val_loss: 0.0912 - val_acc: 0.9747
Epoch 27/500
585s - loss: 0.1078 - acc: 0.9729 - val_loss: 0.0908 - val_acc: 0.9747
Epoch 28/500
585s - loss: 0.1054 - acc: 0.9737 - val_loss: 0.0904 - val_acc: 0.9747
Epoch 29/500
585s - loss: 0.1045 - acc: 0.9745 - val_loss: 0.0901 - val_acc: 0.9747
Epoch 30/500
585s - loss: 0.1131 - acc: 0.9718 - val_loss: 0.0904 - val_acc: 0.9747
Epoch 31/500
585s - loss: 0.1073 - acc: 0.9718 - val_loss: 0.0913 - val_acc: 0.9747
Epoch 32/500
585s - loss: 0.0983 - acc: 0.9779 - val_loss: 0.0905 - val_acc: 0.9747
Epoch 33/500
585s - loss: 0.1151 - acc: 0.9718 - val_loss: 0.0903 - val_acc: 0.9747
Epoch 34/500
585s - loss: 0.1102 - acc: 0.9755 - val_loss: 0.0907 - val_acc: 0.9747
Epoch 35/500
585s - loss: 0.1063 - acc: 0.9742 - val_loss: 0.0899 - val_acc: 0.9747
Epoch 36/500
585s - loss: 0.1046 - acc: 0.9739 - val_loss: 0.0900 - val_acc: 0.9747
Epoch 37/500
585s - loss: 0.1073 - acc: 0.9700 - val_loss: 0.0896 - val_acc: 0.9747
Epoch 38/500
585s - loss: 0.1038 - acc: 0.9742 - val_loss: 0.0908 - val_acc: 0.9747
Epoch 39/500
585s - loss: 0.1087 - acc: 0.9750 - val_loss: 0.0901 - val_acc: 0.9747
Epoch 40/500
585s - loss: 0.1094 - acc: 0.9718 - val_loss: 0.0900 - val_acc: 0.9747
Epoch 41/500
585s - loss: 0.1062 - acc: 0.9716 - val_loss: 0.0898 - val_acc: 0.9747
Epoch 42/500
585s - loss: 0.1081 - acc: 0.9739 - val_loss: 0.0906 - val_acc: 0.9747
Epoch 43/500
585s - loss: 0.1153 - acc: 0.9721 - val_loss: 0.0901 - val_acc: 0.9747
Epoch 44/500
585s - loss: 0.1072 - acc: 0.9721 - val_loss: 0.0901 - val_acc: 0.9747
Epoch 45/500
586s - loss: 0.1028 - acc: 0.9763 - val_loss: 0.0908 - val_acc: 0.9747
Epoch 46/500
585s - loss: 0.1168 - acc: 0.9708 - val_loss: 0.0911 - val_acc: 0.9747
Epoch 47/500
585s - loss: 0.1151 - acc: 0.9700 - val_loss: 0.0901 - val_acc: 0.9747
Epoch 48/500
585s - loss: 0.1106 - acc: 0.9726 - val_loss: 0.0904 - val_acc: 0.9747
Epoch 49/500
585s - loss: 0.1004 - acc: 0.9745 - val_loss: 0.0904 - val_acc: 0.9747
Epoch 50/500
2018-08-11 03:51:38.021176: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[25,21,21,128]
Traceback (most recent call last):
  File "cnnAug.py", line 168, in <module>
    callbacks=callbacks)
  File "/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/keras/engine/training.py", line 1507, in fit
    initial_epoch=initial_epoch)
  File "/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/keras/engine/training.py", line 1156, in _fit_loop
    outs = f(ins_batch)
  File "/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py", line 2269, in __call__
    **self.session_kwargs)
  File "/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    run_metadata_ptr)
  File "/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1124, in _run
    feed_dict_tensor, options, run_metadata)
  File "/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1321, in _do_run
    options, run_metadata)
  File "/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[25,21,21,128]
	 [[Node: batch_normalization_3/moments/SquaredDifference = SquaredDifference[T=DT_FLOAT, _device="/job:localhost/replica:0/task:0/cpu:0"](conv2d_3/BiasAdd, batch_normalization_3/moments/StopGradient)]]

Caused by op u'batch_normalization_3/moments/SquaredDifference', defined at:
  File "cnnAug.py", line 118, in <module>
    model = dataPrep.getcnnKERAS(dim, featSize)
  File "/users/PAS1383/osu10171/PlantSeedlings/dataPrep.py", line 309, in getcnnKERAS
    conv2 = conv_layer(conv1, 64, zp_flag=False)
  File "/users/PAS1383/osu10171/PlantSeedlings/dataPrep.py", line 298, in conv_layer
    else:
  File "/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/keras/engine/topology.py", line 596, in __call__
    output = self.call(inputs, **kwargs)
  File "/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/keras/layers/normalization.py", line 177, in call
    epsilon=self.epsilon)
  File "/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py", line 1648, in normalize_batch_in_training
    shift=None, name=None, keep_dims=False)
  File "/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_impl.py", line 621, in moments
    math_ops.squared_difference(y, array_ops.stop_gradient(mean)),
  File "/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py", line 2610, in squared_difference
    result = _op_def_lib.apply_op("SquaredDifference", x=x, y=y, name=name)
  File "/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 767, in apply_op
    op_def=op_def)
  File "/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File "/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[25,21,21,128]
	 [[Node: batch_normalization_3/moments/SquaredDifference = SquaredDifference[T=DT_FLOAT, _device="/job:localhost/replica:0/task:0/cpu:0"](conv2d_3/BiasAdd, batch_normalization_3/moments/StopGradient)]]

