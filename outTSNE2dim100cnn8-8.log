Using TensorFlow backend.
/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
 
Found train data with correct size
 
 
Found test data with correct size
 
/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.
  warn("The default mode, 'constant', will be changed to 'reflect' in "
/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)
 
No saved model. Generating...
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_1 (InputLayer)             (None, 100, 100, 3)   0                                            
____________________________________________________________________________________________________
conv2d_1 (Conv2D)                (None, 98, 98, 16)    448         input_1[0][0]                    
____________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)   (None, 49, 49, 16)    0           conv2d_1[0][0]                   
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 49, 49, 16)    0           max_pooling2d_1[0][0]            
____________________________________________________________________________________________________
conv2d_2 (Conv2D)                (None, 47, 47, 16)    2320        dropout_1[0][0]                  
____________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)   (None, 23, 23, 16)    0           conv2d_2[0][0]                   
____________________________________________________________________________________________________
dropout_2 (Dropout)              (None, 23, 23, 16)    0           max_pooling2d_2[0][0]            
____________________________________________________________________________________________________
conv2d_3 (Conv2D)                (None, 21, 21, 32)    4640        dropout_2[0][0]                  
____________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)   (None, 10, 10, 32)    0           conv2d_3[0][0]                   
____________________________________________________________________________________________________
dropout_3 (Dropout)              (None, 10, 10, 32)    0           max_pooling2d_3[0][0]            
____________________________________________________________________________________________________
conv2d_4 (Conv2D)                (None, 8, 8, 32)      9248        dropout_3[0][0]                  
____________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)   (None, 4, 4, 32)      0           conv2d_4[0][0]                   
____________________________________________________________________________________________________
dropout_4 (Dropout)              (None, 4, 4, 32)      0           max_pooling2d_4[0][0]            
____________________________________________________________________________________________________
flatten_1 (Flatten)              (None, 512)           0           dropout_4[0][0]                  
____________________________________________________________________________________________________
batch_normalization_1 (BatchNorm (None, 512)           2048        flatten_1[0][0]                  
____________________________________________________________________________________________________
input_2 (InputLayer)             (None, 100)           0                                            
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 612)           0           batch_normalization_1[0][0]      
                                                                   input_2[0][0]                    
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 64)            39232       concatenate_1[0][0]              
____________________________________________________________________________________________________
dropout_5 (Dropout)              (None, 64)            0           dense_1[0][0]                    
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 32)            2080        dropout_5[0][0]                  
____________________________________________________________________________________________________
dropout_6 (Dropout)              (None, 32)            0           dense_2[0][0]                    
____________________________________________________________________________________________________
dense_3 (Dense)                  (None, 12)            396         dropout_6[0][0]                  
====================================================================================================
Total params: 60,412
Trainable params: 59,388
Non-trainable params: 1,024
____________________________________________________________________________________________________
2018-08-08 15:32:23.316135: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-08-08 15:32:23.316504: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
Created and saved model.
 
 
Pulling kfold 0 from previous runs
 
Bad saved trial. Testing acc <0.9%. Rerunning ...
 
Train on 3800 samples, validate on 950 samples
Epoch 1/500
70s - loss: 0.6615 - acc: 0.7661 - val_loss: 1.0768 - val_acc: 0.6411
Epoch 2/500
69s - loss: 0.6689 - acc: 0.7661 - val_loss: 1.0438 - val_acc: 0.6579
Epoch 3/500
69s - loss: 0.6458 - acc: 0.7766 - val_loss: 1.1074 - val_acc: 0.6347
Epoch 4/500
69s - loss: 0.6409 - acc: 0.7834 - val_loss: 0.9997 - val_acc: 0.6642
Epoch 5/500
69s - loss: 0.6281 - acc: 0.7847 - val_loss: 1.0059 - val_acc: 0.6695
Epoch 6/500
69s - loss: 0.6380 - acc: 0.7782 - val_loss: 1.0611 - val_acc: 0.6505
Epoch 7/500
69s - loss: 0.6091 - acc: 0.7897 - val_loss: 0.9512 - val_acc: 0.6832
Epoch 8/500
69s - loss: 0.6296 - acc: 0.7789 - val_loss: 1.0833 - val_acc: 0.6326
Epoch 9/500
69s - loss: 0.6252 - acc: 0.7787 - val_loss: 1.0649 - val_acc: 0.6368
Epoch 10/500
69s - loss: 0.6008 - acc: 0.7924 - val_loss: 0.9933 - val_acc: 0.6547
Epoch 11/500
69s - loss: 0.5906 - acc: 0.7942 - val_loss: 0.9914 - val_acc: 0.6726
Epoch 12/500
69s - loss: 0.5782 - acc: 0.7937 - val_loss: 1.0201 - val_acc: 0.6558
Epoch 13/500
69s - loss: 0.5831 - acc: 0.7942 - val_loss: 1.0403 - val_acc: 0.6568
Epoch 14/500
69s - loss: 0.5892 - acc: 0.7900 - val_loss: 1.0281 - val_acc: 0.6695
Epoch 15/500
69s - loss: 0.5954 - acc: 0.7916 - val_loss: 1.0706 - val_acc: 0.6495
Epoch 16/500
69s - loss: 0.5722 - acc: 0.7979 - val_loss: 0.9679 - val_acc: 0.6768
Epoch 17/500
69s - loss: 0.5570 - acc: 0.8047 - val_loss: 1.0316 - val_acc: 0.6621
Epoch 18/500
69s - loss: 0.5357 - acc: 0.8139 - val_loss: 0.9720 - val_acc: 0.6758
Epoch 19/500
69s - loss: 0.5410 - acc: 0.8108 - val_loss: 0.9613 - val_acc: 0.6705
Epoch 20/500
69s - loss: 0.5395 - acc: 0.8103 - val_loss: 0.9371 - val_acc: 0.6800
Epoch 21/500
69s - loss: 0.5394 - acc: 0.8082 - val_loss: 0.9708 - val_acc: 0.6758
Epoch 22/500
69s - loss: 0.5403 - acc: 0.8158 - val_loss: 0.9579 - val_acc: 0.6726
Epoch 23/500
69s - loss: 0.5454 - acc: 0.8047 - val_loss: 0.9757 - val_acc: 0.6758
Epoch 24/500
69s - loss: 0.5416 - acc: 0.8089 - val_loss: 0.9470 - val_acc: 0.6842
Epoch 25/500
69s - loss: 0.5437 - acc: 0.8045 - val_loss: 0.9986 - val_acc: 0.6695
Epoch 26/500
69s - loss: 0.5482 - acc: 0.8074 - val_loss: 0.9804 - val_acc: 0.6747
Epoch 27/500
69s - loss: 0.5483 - acc: 0.8016 - val_loss: 1.0109 - val_acc: 0.6600
Epoch 28/500
69s - loss: 0.5375 - acc: 0.8137 - val_loss: 0.9739 - val_acc: 0.6758
Epoch 29/500
69s - loss: 0.5269 - acc: 0.8197 - val_loss: 0.9804 - val_acc: 0.6705
Epoch 30/500
69s - loss: 0.5204 - acc: 0.8195 - val_loss: 0.9892 - val_acc: 0.6653
Epoch 31/500
69s - loss: 0.5119 - acc: 0.8211 - val_loss: 0.9413 - val_acc: 0.6832
Epoch 32/500
69s - loss: 0.5323 - acc: 0.8105 - val_loss: 0.9866 - val_acc: 0.6705
Epoch 33/500
69s - loss: 0.5293 - acc: 0.8163 - val_loss: 0.9781 - val_acc: 0.6747
Epoch 34/500
69s - loss: 0.5344 - acc: 0.8126 - val_loss: 0.9759 - val_acc: 0.6737
Epoch 35/500
69s - loss: 0.5260 - acc: 0.8192 - val_loss: 0.9699 - val_acc: 0.6737
Epoch 36/500
69s - loss: 0.5433 - acc: 0.8058 - val_loss: 0.9781 - val_acc: 0.6726
Epoch 37/500
69s - loss: 0.5374 - acc: 0.8192 - val_loss: 0.9892 - val_acc: 0.6684
Epoch 38/500
69s - loss: 0.5309 - acc: 0.8079 - val_loss: 0.9849 - val_acc: 0.6695
Epoch 39/500
69s - loss: 0.5356 - acc: 0.8158 - val_loss: 0.9799 - val_acc: 0.6705
Epoch 40/500
69s - loss: 0.5495 - acc: 0.8045 - val_loss: 0.9800 - val_acc: 0.6726
Epoch 41/500
69s - loss: 0.5374 - acc: 0.8116 - val_loss: 0.9720 - val_acc: 0.6726
Epoch 42/500
69s - loss: 0.5448 - acc: 0.8092 - val_loss: 0.9858 - val_acc: 0.6726
Epoch 43/500
69s - loss: 0.5447 - acc: 0.8079 - val_loss: 0.9852 - val_acc: 0.6716
Epoch 44/500
69s - loss: 0.5391 - acc: 0.8061 - val_loss: 0.9876 - val_acc: 0.6684
Epoch 45/500
69s - loss: 0.5376 - acc: 0.8155 - val_loss: 0.9861 - val_acc: 0.6726
Epoch 46/500
69s - loss: 0.5567 - acc: 0.8047 - val_loss: 0.9824 - val_acc: 0.6737
Epoch 47/500
69s - loss: 0.5329 - acc: 0.8139 - val_loss: 0.9759 - val_acc: 0.6726
Epoch 48/500
69s - loss: 0.5465 - acc: 0.8082 - val_loss: 0.9817 - val_acc: 0.6726
Epoch 49/500
69s - loss: 0.5554 - acc: 0.8000 - val_loss: 0.9816 - val_acc: 0.6716
Epoch 50/500
69s - loss: 0.5317 - acc: 0.8124 - val_loss: 0.9782 - val_acc: 0.6747
Epoch 51/500
69s - loss: 0.5438 - acc: 0.8034 - val_loss: 0.9811 - val_acc: 0.6737
Epoch 52/500
69s - loss: 0.5570 - acc: 0.8037 - val_loss: 0.9778 - val_acc: 0.6747
Epoch 53/500
69s - loss: 0.5422 - acc: 0.8137 - val_loss: 0.9779 - val_acc: 0.6726
Epoch 54/500
69s - loss: 0.5416 - acc: 0.8066 - val_loss: 0.9721 - val_acc: 0.6747
Epoch 55/500
69s - loss: 0.5364 - acc: 0.8132 - val_loss: 0.9806 - val_acc: 0.6726
Epoch 56/500
69s - loss: 0.5513 - acc: 0.8024 - val_loss: 0.9783 - val_acc: 0.6726
Epoch 57/500
69s - loss: 0.5385 - acc: 0.8097 - val_loss: 0.9763 - val_acc: 0.6747
Epoch 58/500
69s - loss: 0.5454 - acc: 0.8105 - val_loss: 0.9734 - val_acc: 0.6747
Epoch 59/500
69s - loss: 0.5457 - acc: 0.8163 - val_loss: 0.9793 - val_acc: 0.6726
Epoch 60/500
69s - loss: 0.5337 - acc: 0.8100 - val_loss: 0.9831 - val_acc: 0.6737
Epoch 61/500
69s - loss: 0.5593 - acc: 0.8063 - val_loss: 0.9829 - val_acc: 0.6737
Epoch 62/500
69s - loss: 0.5534 - acc: 0.8087 - val_loss: 0.9846 - val_acc: 0.6716
Epoch 63/500
69s - loss: 0.5507 - acc: 0.8066 - val_loss: 0.9847 - val_acc: 0.6705
Epoch 64/500
69s - loss: 0.5503 - acc: 0.8089 - val_loss: 0.9712 - val_acc: 0.6737
Epoch 65/500
69s - loss: 0.5463 - acc: 0.8063 - val_loss: 0.9788 - val_acc: 0.6726
Epoch 66/500
69s - loss: 0.5412 - acc: 0.8087 - val_loss: 0.9780 - val_acc: 0.6737
Epoch 67/500
69s - loss: 0.5398 - acc: 0.8084 - val_loss: 0.9759 - val_acc: 0.6726
Epoch 68/500
69s - loss: 0.5427 - acc: 0.8084 - val_loss: 0.9726 - val_acc: 0.6737
Epoch 69/500
69s - loss: 0.5477 - acc: 0.8082 - val_loss: 0.9787 - val_acc: 0.6737
Epoch 70/500
69s - loss: 0.5430 - acc: 0.8134 - val_loss: 0.9782 - val_acc: 0.6726
Epoch 71/500
69s - loss: 0.5373 - acc: 0.8208 - val_loss: 0.9754 - val_acc: 0.6726
Epoch 72/500
69s - loss: 0.5573 - acc: 0.8108 - val_loss: 0.9753 - val_acc: 0.6758
Epoch 73/500
69s - loss: 0.5631 - acc: 0.7974 - val_loss: 0.9764 - val_acc: 0.6747
Epoch 74/500
69s - loss: 0.5487 - acc: 0.8066 - val_loss: 0.9758 - val_acc: 0.6705
Epoch 75/500
69s - loss: 0.5549 - acc: 0.8063 - val_loss: 0.9728 - val_acc: 0.6747
Epoch 76/500
69s - loss: 0.5389 - acc: 0.8137 - val_loss: 0.9748 - val_acc: 0.6747
Epoch 77/500
69s - loss: 0.5409 - acc: 0.8150 - val_loss: 0.9803 - val_acc: 0.6737
Epoch 78/500
69s - loss: 0.5311 - acc: 0.8103 - val_loss: 0.9723 - val_acc: 0.6758
Epoch 79/500
69s - loss: 0.5416 - acc: 0.8108 - val_loss: 0.9732 - val_acc: 0.6726
Epoch 80/500
69s - loss: 0.5299 - acc: 0.8147 - val_loss: 0.9696 - val_acc: 0.6737
Epoch 81/500
69s - loss: 0.5414 - acc: 0.8116 - val_loss: 0.9780 - val_acc: 0.6726
Epoch 82/500
69s - loss: 0.5491 - acc: 0.8108 - val_loss: 0.9762 - val_acc: 0.6747
Epoch 83/500
69s - loss: 0.5370 - acc: 0.8103 - val_loss: 0.9781 - val_acc: 0.6737
Epoch 84/500
69s - loss: 0.5487 - acc: 0.8089 - val_loss: 0.9757 - val_acc: 0.6726
Epoch 85/500
69s - loss: 0.5393 - acc: 0.8053 - val_loss: 0.9743 - val_acc: 0.6747
Epoch 86/500
69s - loss: 0.5435 - acc: 0.8092 - val_loss: 0.9831 - val_acc: 0.6737
Epoch 87/500
69s - loss: 0.5425 - acc: 0.8066 - val_loss: 0.9805 - val_acc: 0.6716
Epoch 88/500
69s - loss: 0.5351 - acc: 0.8116 - val_loss: 0.9766 - val_acc: 0.6716
Epoch 89/500
69s - loss: 0.5461 - acc: 0.8074 - val_loss: 0.9739 - val_acc: 0.6747
Epoch 90/500
69s - loss: 0.5463 - acc: 0.8068 - val_loss: 0.9756 - val_acc: 0.6726
Epoch 91/500
69s - loss: 0.5483 - acc: 0.8039 - val_loss: 0.9740 - val_acc: 0.6737
Epoch 92/500
69s - loss: 0.5234 - acc: 0.8229 - val_loss: 0.9765 - val_acc: 0.6747
Training loss for fold 0 is 0.6651088372029756 with percent 75.28947367166218
Testing loss for fold 0 is 0.9370701233964217 with percent 68.00000001254834
 
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
69s - loss: 0.6284 - acc: 0.7918 - val_loss: 0.6605 - val_acc: 0.7505
Epoch 2/500
69s - loss: 0.6404 - acc: 0.7779 - val_loss: 0.6646 - val_acc: 0.7484
Epoch 3/500
69s - loss: 0.6305 - acc: 0.7808 - val_loss: 0.6619 - val_acc: 0.7474
Epoch 4/500
69s - loss: 0.6211 - acc: 0.7847 - val_loss: 0.6669 - val_acc: 0.7495
Epoch 5/500
69s - loss: 0.6271 - acc: 0.7847 - val_loss: 0.6662 - val_acc: 0.7474
Epoch 6/500
69s - loss: 0.6323 - acc: 0.7789 - val_loss: 0.6596 - val_acc: 0.7495
Epoch 7/500
69s - loss: 0.6115 - acc: 0.7916 - val_loss: 0.6636 - val_acc: 0.7463
Epoch 8/500
69s - loss: 0.6239 - acc: 0.7808 - val_loss: 0.6661 - val_acc: 0.7474
Epoch 9/500
69s - loss: 0.6264 - acc: 0.7876 - val_loss: 0.6677 - val_acc: 0.7474
Epoch 10/500
69s - loss: 0.6364 - acc: 0.7879 - val_loss: 0.6656 - val_acc: 0.7463
Epoch 11/500
69s - loss: 0.6286 - acc: 0.7866 - val_loss: 0.6700 - val_acc: 0.7463
Epoch 12/500
69s - loss: 0.6228 - acc: 0.7921 - val_loss: 0.6781 - val_acc: 0.7474
Epoch 13/500
69s - loss: 0.6167 - acc: 0.7934 - val_loss: 0.6760 - val_acc: 0.7463
Epoch 14/500
69s - loss: 0.6322 - acc: 0.7826 - val_loss: 0.6708 - val_acc: 0.7453
Epoch 15/500
69s - loss: 0.6173 - acc: 0.7905 - val_loss: 0.6826 - val_acc: 0.7442
Epoch 16/500
69s - loss: 0.6233 - acc: 0.7868 - val_loss: 0.6720 - val_acc: 0.7453
Epoch 17/500
69s - loss: 0.6412 - acc: 0.7871 - val_loss: 0.6718 - val_acc: 0.7463
Epoch 18/500
69s - loss: 0.6285 - acc: 0.7911 - val_loss: 0.6689 - val_acc: 0.7474
Epoch 19/500
69s - loss: 0.6271 - acc: 0.7884 - val_loss: 0.6769 - val_acc: 0.7453
Epoch 20/500
69s - loss: 0.5970 - acc: 0.7961 - val_loss: 0.6699 - val_acc: 0.7463
Epoch 21/500
69s - loss: 0.6252 - acc: 0.7905 - val_loss: 0.6741 - val_acc: 0.7463
Epoch 22/500
69s - loss: 0.6213 - acc: 0.7837 - val_loss: 0.6697 - val_acc: 0.7463
Epoch 23/500
69s - loss: 0.6288 - acc: 0.7905 - val_loss: 0.6768 - val_acc: 0.7463
Epoch 24/500
69s - loss: 0.6348 - acc: 0.7839 - val_loss: 0.6790 - val_acc: 0.7463
Epoch 25/500
69s - loss: 0.6355 - acc: 0.7834 - val_loss: 0.6789 - val_acc: 0.7442
Epoch 26/500
69s - loss: 0.6230 - acc: 0.7979 - val_loss: 0.6693 - val_acc: 0.7442
Epoch 27/500
69s - loss: 0.6220 - acc: 0.7874 - val_loss: 0.6771 - val_acc: 0.7453
Epoch 28/500
69s - loss: 0.6138 - acc: 0.7847 - val_loss: 0.6832 - val_acc: 0.7442
Epoch 29/500
69s - loss: 0.6237 - acc: 0.7905 - val_loss: 0.6745 - val_acc: 0.7453
Epoch 30/500
69s - loss: 0.6328 - acc: 0.7918 - val_loss: 0.6720 - val_acc: 0.7463
Epoch 31/500
68s - loss: 0.6203 - acc: 0.7868 - val_loss: 0.6825 - val_acc: 0.7432
Epoch 32/500
69s - loss: 0.6305 - acc: 0.7808 - val_loss: 0.6739 - val_acc: 0.7442
Epoch 33/500
68s - loss: 0.6275 - acc: 0.7876 - val_loss: 0.6754 - val_acc: 0.7453
Epoch 34/500
69s - loss: 0.6079 - acc: 0.7876 - val_loss: 0.6722 - val_acc: 0.7463
Epoch 35/500
69s - loss: 0.6056 - acc: 0.7947 - val_loss: 0.6772 - val_acc: 0.7474
Epoch 36/500
68s - loss: 0.6234 - acc: 0.7955 - val_loss: 0.6721 - val_acc: 0.7484
Epoch 37/500
69s - loss: 0.6229 - acc: 0.7903 - val_loss: 0.6699 - val_acc: 0.7463
Epoch 38/500
69s - loss: 0.6207 - acc: 0.7895 - val_loss: 0.6744 - val_acc: 0.7453
Epoch 39/500
69s - loss: 0.6120 - acc: 0.7900 - val_loss: 0.6778 - val_acc: 0.7463
Epoch 40/500
68s - loss: 0.6274 - acc: 0.7879 - val_loss: 0.6788 - val_acc: 0.7484
Epoch 41/500
69s - loss: 0.6284 - acc: 0.7911 - val_loss: 0.6828 - val_acc: 0.7432
Epoch 42/500
68s - loss: 0.6161 - acc: 0.7884 - val_loss: 0.6749 - val_acc: 0.7463
Epoch 43/500
68s - loss: 0.6141 - acc: 0.7926 - val_loss: 0.6818 - val_acc: 0.7442
Epoch 44/500
68s - loss: 0.6214 - acc: 0.7942 - val_loss: 0.6781 - val_acc: 0.7453
Epoch 45/500
69s - loss: 0.6301 - acc: 0.7826 - val_loss: 0.6812 - val_acc: 0.7442
Epoch 46/500
68s - loss: 0.6147 - acc: 0.7903 - val_loss: 0.6671 - val_acc: 0.7474
Epoch 47/500
