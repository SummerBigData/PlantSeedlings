Using TensorFlow backend.
/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
 
Found train data with correct size
 
 
Found test data with correct size
 
/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.
  warn("The default mode, 'constant', will be changed to 'reflect' in "
Augmentation data size (4750, 102) (794, 102)
/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)
(3800, 65, 65, 3) (3800,)
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_1 (InputLayer)             (None, 65, 65, 3)     0                                            
____________________________________________________________________________________________________
conv2d_1 (Conv2D)                (None, 63, 63, 16)    448         input_1[0][0]                    
____________________________________________________________________________________________________
batch_normalization_1 (BatchNorm (None, 63, 63, 16)    64          conv2d_1[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_1 (LeakyReLU)        (None, 63, 63, 16)    0           batch_normalization_1[0][0]      
____________________________________________________________________________________________________
conv2d_2 (Conv2D)                (None, 62, 62, 16)    1040        leaky_re_lu_1[0][0]              
____________________________________________________________________________________________________
batch_normalization_2 (BatchNorm (None, 62, 62, 16)    64          conv2d_2[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_2 (LeakyReLU)        (None, 62, 62, 16)    0           batch_normalization_2[0][0]      
____________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)   (None, 31, 31, 16)    0           leaky_re_lu_2[0][0]              
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 31, 31, 16)    0           max_pooling2d_1[0][0]            
____________________________________________________________________________________________________
conv2d_3 (Conv2D)                (None, 29, 29, 32)    4640        dropout_1[0][0]                  
____________________________________________________________________________________________________
batch_normalization_3 (BatchNorm (None, 29, 29, 32)    128         conv2d_3[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_3 (LeakyReLU)        (None, 29, 29, 32)    0           batch_normalization_3[0][0]      
____________________________________________________________________________________________________
conv2d_4 (Conv2D)                (None, 28, 28, 32)    4128        leaky_re_lu_3[0][0]              
____________________________________________________________________________________________________
batch_normalization_4 (BatchNorm (None, 28, 28, 32)    128         conv2d_4[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_4 (LeakyReLU)        (None, 28, 28, 32)    0           batch_normalization_4[0][0]      
____________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)   (None, 14, 14, 32)    0           leaky_re_lu_4[0][0]              
____________________________________________________________________________________________________
dropout_2 (Dropout)              (None, 14, 14, 32)    0           max_pooling2d_2[0][0]            
____________________________________________________________________________________________________
conv2d_5 (Conv2D)                (None, 12, 12, 64)    18496       dropout_2[0][0]                  
____________________________________________________________________________________________________
batch_normalization_5 (BatchNorm (None, 12, 12, 64)    256         conv2d_5[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_5 (LeakyReLU)        (None, 12, 12, 64)    0           batch_normalization_5[0][0]      
____________________________________________________________________________________________________
conv2d_6 (Conv2D)                (None, 11, 11, 64)    16448       leaky_re_lu_5[0][0]              
____________________________________________________________________________________________________
batch_normalization_6 (BatchNorm (None, 11, 11, 64)    256         conv2d_6[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_6 (LeakyReLU)        (None, 11, 11, 64)    0           batch_normalization_6[0][0]      
____________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)   (None, 5, 5, 64)      0           leaky_re_lu_6[0][0]              
____________________________________________________________________________________________________
dropout_3 (Dropout)              (None, 5, 5, 64)      0           max_pooling2d_3[0][0]            
____________________________________________________________________________________________________
flatten_1 (Flatten)              (None, 1600)          0           dropout_3[0][0]                  
____________________________________________________________________________________________________
batch_normalization_7 (BatchNorm (None, 1600)          6400        flatten_1[0][0]                  
____________________________________________________________________________________________________
input_2 (InputLayer)             (None, 102)           0                                            
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 1702)          0           batch_normalization_7[0][0]      
                                                                   input_2[0][0]                    
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 128)           217984      concatenate_1[0][0]              
____________________________________________________________________________________________________
leaky_re_lu_7 (LeakyReLU)        (None, 128)           0           dense_1[0][0]                    
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 64)            8256        leaky_re_lu_7[0][0]              
____________________________________________________________________________________________________
dropout_4 (Dropout)              (None, 64)            0           dense_2[0][0]                    
____________________________________________________________________________________________________
dense_3 (Dense)                  (None, 32)            2080        dropout_4[0][0]                  
____________________________________________________________________________________________________
dropout_5 (Dropout)              (None, 32)            0           dense_3[0][0]                    
____________________________________________________________________________________________________
dense_4 (Dense)                  (None, 12)            396         dropout_5[0][0]                  
====================================================================================================
Total params: 281,212
Trainable params: 277,564
Non-trainable params: 3,648
____________________________________________________________________________________________________
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
2018-08-13 23:25:48.747347: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-08-13 23:25:48.748129: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
171s - loss: 1.9600 - acc: 0.3353 - val_loss: 3.2477 - val_acc: 0.2126
Epoch 2/500
167s - loss: 1.5704 - acc: 0.4855 - val_loss: 3.2380 - val_acc: 0.3505
Epoch 3/500
166s - loss: 1.3909 - acc: 0.5589 - val_loss: 3.8465 - val_acc: 0.2105
Epoch 4/500
167s - loss: 1.3258 - acc: 0.5853 - val_loss: 1.2880 - val_acc: 0.6453
Epoch 5/500
167s - loss: 1.2537 - acc: 0.6158 - val_loss: 2.7677 - val_acc: 0.3042
Epoch 6/500
167s - loss: 1.1954 - acc: 0.6329 - val_loss: 2.4878 - val_acc: 0.4084
Epoch 7/500
166s - loss: 1.1134 - acc: 0.6732 - val_loss: 1.0115 - val_acc: 0.7179
Epoch 8/500
166s - loss: 1.0946 - acc: 0.6824 - val_loss: 2.2570 - val_acc: 0.4789
Epoch 9/500
167s - loss: 1.0560 - acc: 0.6968 - val_loss: 5.7351 - val_acc: 0.1516
Epoch 10/500
166s - loss: 1.0748 - acc: 0.6824 - val_loss: 2.0741 - val_acc: 0.5400
Epoch 11/500
166s - loss: 0.9607 - acc: 0.7166 - val_loss: 1.3531 - val_acc: 0.6758
Epoch 12/500
166s - loss: 0.9637 - acc: 0.7261 - val_loss: 1.4284 - val_acc: 0.6242
Epoch 13/500
166s - loss: 1.0031 - acc: 0.7197 - val_loss: 1.1308 - val_acc: 0.7368
Epoch 14/500
166s - loss: 0.9393 - acc: 0.7339 - val_loss: 0.7533 - val_acc: 0.7789
Epoch 15/500
167s - loss: 0.9441 - acc: 0.7487 - val_loss: 1.9011 - val_acc: 0.6526
Epoch 16/500
165s - loss: 0.9014 - acc: 0.7545 - val_loss: 0.9211 - val_acc: 0.7516
Epoch 17/500
166s - loss: 0.8652 - acc: 0.7668 - val_loss: 3.4588 - val_acc: 0.3768
Epoch 18/500
166s - loss: 0.8904 - acc: 0.7587 - val_loss: 2.4703 - val_acc: 0.5147
Epoch 19/500
161s - loss: 0.8567 - acc: 0.7603 - val_loss: 1.0433 - val_acc: 0.7684
Epoch 20/500
161s - loss: 0.9351 - acc: 0.7426 - val_loss: 3.0453 - val_acc: 0.4968
Epoch 21/500
166s - loss: 0.8531 - acc: 0.7758 - val_loss: 2.0371 - val_acc: 0.6168
Epoch 22/500
165s - loss: 0.9405 - acc: 0.7497 - val_loss: 1.5781 - val_acc: 0.6484
Epoch 23/500

Epoch 00022: reducing learning rate to 0.010000000149.
165s - loss: 0.9373 - acc: 0.7450 - val_loss: 2.9726 - val_acc: 0.4432
Epoch 24/500
167s - loss: 0.8644 - acc: 0.7655 - val_loss: 0.9114 - val_acc: 0.7695
Epoch 25/500
163s - loss: 0.6027 - acc: 0.8218 - val_loss: 0.8236 - val_acc: 0.7926
Epoch 26/500
164s - loss: 0.5404 - acc: 0.8408 - val_loss: 0.7544 - val_acc: 0.8200
Epoch 27/500
161s - loss: 0.4950 - acc: 0.8442 - val_loss: 0.7246 - val_acc: 0.8232
Epoch 28/500
164s - loss: 0.4521 - acc: 0.8576 - val_loss: 0.6664 - val_acc: 0.8316
Epoch 29/500
163s - loss: 0.4175 - acc: 0.8624 - val_loss: 0.6576 - val_acc: 0.8389
Epoch 30/500
164s - loss: 0.4249 - acc: 0.8658 - val_loss: 0.6637 - val_acc: 0.8368
Epoch 31/500
165s - loss: 0.4326 - acc: 0.8705 - val_loss: 0.6452 - val_acc: 0.8537
Epoch 32/500
165s - loss: 0.4104 - acc: 0.8697 - val_loss: 0.6420 - val_acc: 0.8484
Epoch 33/500
166s - loss: 0.4012 - acc: 0.8705 - val_loss: 0.6464 - val_acc: 0.8547
Epoch 34/500
166s - loss: 0.3800 - acc: 0.8766 - val_loss: 0.6346 - val_acc: 0.8600
Epoch 35/500
166s - loss: 0.3730 - acc: 0.8792 - val_loss: 0.6438 - val_acc: 0.8505
Epoch 36/500
166s - loss: 0.3732 - acc: 0.8774 - val_loss: 0.7042 - val_acc: 0.8442
Epoch 37/500
165s - loss: 0.4062 - acc: 0.8718 - val_loss: 0.6472 - val_acc: 0.8579
Epoch 38/500
165s - loss: 0.3623 - acc: 0.8813 - val_loss: 0.5936 - val_acc: 0.8642
Epoch 39/500
166s - loss: 0.3594 - acc: 0.8889 - val_loss: 0.6001 - val_acc: 0.8568
Epoch 40/500
166s - loss: 0.3552 - acc: 0.8847 - val_loss: 0.6089 - val_acc: 0.8495
Epoch 41/500
166s - loss: 0.3421 - acc: 0.8868 - val_loss: 0.5811 - val_acc: 0.8589
Epoch 42/500
166s - loss: 0.3483 - acc: 0.8847 - val_loss: 0.5997 - val_acc: 0.8684
Epoch 43/500
166s - loss: 0.3460 - acc: 0.8863 - val_loss: 0.6658 - val_acc: 0.8421
Epoch 44/500
165s - loss: 0.3246 - acc: 0.8921 - val_loss: 0.5983 - val_acc: 0.8663
Epoch 45/500
165s - loss: 0.3054 - acc: 0.8955 - val_loss: 0.6216 - val_acc: 0.8642
Epoch 46/500
166s - loss: 0.3421 - acc: 0.8916 - val_loss: 0.6569 - val_acc: 0.8463
Epoch 47/500
165s - loss: 0.3060 - acc: 0.8950 - val_loss: 0.5614 - val_acc: 0.8716
Epoch 48/500
166s - loss: 0.2915 - acc: 0.9039 - val_loss: 0.5831 - val_acc: 0.8695
Epoch 49/500
166s - loss: 0.2986 - acc: 0.9034 - val_loss: 0.6070 - val_acc: 0.8632
Epoch 50/500
166s - loss: 0.3071 - acc: 0.8989 - val_loss: 0.6014 - val_acc: 0.8684
Epoch 51/500
166s - loss: 0.3060 - acc: 0.8961 - val_loss: 0.6068 - val_acc: 0.8653
Epoch 52/500
166s - loss: 0.3154 - acc: 0.8961 - val_loss: 0.6043 - val_acc: 0.8663
Epoch 53/500
166s - loss: 0.3123 - acc: 0.8995 - val_loss: 0.6043 - val_acc: 0.8653
Epoch 54/500
162s - loss: 0.2892 - acc: 0.8989 - val_loss: 0.5704 - val_acc: 0.8768
Epoch 55/500
164s - loss: 0.3000 - acc: 0.8958 - val_loss: 0.5872 - val_acc: 0.8789
Epoch 56/500
164s - loss: 0.2674 - acc: 0.9053 - val_loss: 0.6063 - val_acc: 0.8705
Epoch 57/500
164s - loss: 0.2942 - acc: 0.8992 - val_loss: 0.5670 - val_acc: 0.8768
Epoch 58/500
164s - loss: 0.2529 - acc: 0.9100 - val_loss: 0.6025 - val_acc: 0.8705
Epoch 59/500
164s - loss: 0.2715 - acc: 0.9005 - val_loss: 0.5957 - val_acc: 0.8705
Epoch 60/500
165s - loss: 0.2652 - acc: 0.9055 - val_loss: 0.5731 - val_acc: 0.8821
Epoch 61/500
165s - loss: 0.2446 - acc: 0.9113 - val_loss: 0.5879 - val_acc: 0.8832
Epoch 62/500
166s - loss: 0.2640 - acc: 0.9029 - val_loss: 0.6344 - val_acc: 0.8611
Epoch 63/500
166s - loss: 0.2575 - acc: 0.9129 - val_loss: 0.6134 - val_acc: 0.8737
Epoch 64/500
165s - loss: 0.2437 - acc: 0.9061 - val_loss: 0.6305 - val_acc: 0.8653
Epoch 65/500
167s - loss: 0.2638 - acc: 0.9137 - val_loss: 0.6814 - val_acc: 0.8411
Epoch 66/500
165s - loss: 0.2467 - acc: 0.9142 - val_loss: 0.5909 - val_acc: 0.8779
Epoch 67/500
165s - loss: 0.2284 - acc: 0.9163 - val_loss: 0.7046 - val_acc: 0.8547
Epoch 68/500
166s - loss: 0.2730 - acc: 0.9147 - val_loss: 0.6203 - val_acc: 0.8726
Epoch 69/500
167s - loss: 0.2673 - acc: 0.9150 - val_loss: 0.6424 - val_acc: 0.8705
Epoch 70/500

Epoch 00069: reducing learning rate to 0.000999999977648.
165s - loss: 0.2511 - acc: 0.9187 - val_loss: 0.5876 - val_acc: 0.8789
Epoch 71/500
164s - loss: 0.2060 - acc: 0.9297 - val_loss: 0.6173 - val_acc: 0.8789
Epoch 72/500
165s - loss: 0.2265 - acc: 0.9263 - val_loss: 0.6286 - val_acc: 0.8779
Epoch 73/500
165s - loss: 0.2394 - acc: 0.9229 - val_loss: 0.6213 - val_acc: 0.8779
Epoch 74/500
165s - loss: 0.2191 - acc: 0.9311 - val_loss: 0.6161 - val_acc: 0.8811
Epoch 75/500
165s - loss: 0.2257 - acc: 0.9242 - val_loss: 0.6140 - val_acc: 0.8821
Epoch 76/500
165s - loss: 0.2079 - acc: 0.9261 - val_loss: 0.6076 - val_acc: 0.8811
Epoch 77/500
165s - loss: 0.2223 - acc: 0.9245 - val_loss: 0.6167 - val_acc: 0.8842
Epoch 78/500
165s - loss: 0.2313 - acc: 0.9226 - val_loss: 0.6172 - val_acc: 0.8811
Epoch 79/500
165s - loss: 0.2463 - acc: 0.9245 - val_loss: 0.6077 - val_acc: 0.8832
Epoch 80/500
165s - loss: 0.2130 - acc: 0.9268 - val_loss: 0.6323 - val_acc: 0.8863
Epoch 81/500
165s - loss: 0.2135 - acc: 0.9250 - val_loss: 0.6235 - val_acc: 0.8853
Epoch 82/500
165s - loss: 0.2146 - acc: 0.9292 - val_loss: 0.6154 - val_acc: 0.8874
Epoch 83/500
165s - loss: 0.2298 - acc: 0.9289 - val_loss: 0.6186 - val_acc: 0.8811
Epoch 84/500
165s - loss: 0.2229 - acc: 0.9208 - val_loss: 0.6217 - val_acc: 0.8874
Epoch 85/500
165s - loss: 0.2231 - acc: 0.9255 - val_loss: 0.6151 - val_acc: 0.8842
Epoch 86/500
165s - loss: 0.2106 - acc: 0.9232 - val_loss: 0.6042 - val_acc: 0.8905
Epoch 87/500
165s - loss: 0.2155 - acc: 0.9268 - val_loss: 0.6112 - val_acc: 0.8842
Epoch 88/500
165s - loss: 0.2104 - acc: 0.9242 - val_loss: 0.6116 - val_acc: 0.8853
Epoch 89/500
165s - loss: 0.2268 - acc: 0.9258 - val_loss: 0.6129 - val_acc: 0.8884
Epoch 90/500
165s - loss: 0.2471 - acc: 0.9242 - val_loss: 0.6317 - val_acc: 0.8874
Epoch 91/500
165s - loss: 0.2041 - acc: 0.9279 - val_loss: 0.6252 - val_acc: 0.8842
Epoch 92/500
165s - loss: 0.1961 - acc: 0.9316 - val_loss: 0.6216 - val_acc: 0.8884
Epoch 93/500
165s - loss: 0.2316 - acc: 0.9255 - val_loss: 0.6244 - val_acc: 0.8821
Epoch 94/500
165s - loss: 0.2159 - acc: 0.9287 - val_loss: 0.6168 - val_acc: 0.8832
Epoch 95/500

Epoch 00094: reducing learning rate to 9.99999931082e-05.
165s - loss: 0.2155 - acc: 0.9303 - val_loss: 0.6135 - val_acc: 0.8863
Epoch 96/500
165s - loss: 0.2158 - acc: 0.9279 - val_loss: 0.6138 - val_acc: 0.8842
Epoch 97/500
165s - loss: 0.2079 - acc: 0.9287 - val_loss: 0.6149 - val_acc: 0.8863
Epoch 98/500
166s - loss: 0.2080 - acc: 0.9363 - val_loss: 0.6173 - val_acc: 0.8863
Epoch 99/500
165s - loss: 0.2181 - acc: 0.9268 - val_loss: 0.6219 - val_acc: 0.8863
Epoch 100/500
165s - loss: 0.1987 - acc: 0.9305 - val_loss: 0.6206 - val_acc: 0.8853
Epoch 101/500
165s - loss: 0.2162 - acc: 0.9295 - val_loss: 0.6187 - val_acc: 0.8853
Epoch 102/500
165s - loss: 0.2180 - acc: 0.9289 - val_loss: 0.6179 - val_acc: 0.8874
Epoch 103/500

Epoch 00102: reducing learning rate to 9.99999901978e-06.
166s - loss: 0.1986 - acc: 0.9292 - val_loss: 0.6174 - val_acc: 0.8874
Epoch 104/500
165s - loss: 0.2306 - acc: 0.9276 - val_loss: 0.6181 - val_acc: 0.8884
Epoch 105/500
165s - loss: 0.2302 - acc: 0.9253 - val_loss: 0.6231 - val_acc: 0.8863
Epoch 106/500
165s - loss: 0.2058 - acc: 0.9284 - val_loss: 0.6218 - val_acc: 0.8853
Epoch 107/500
166s - loss: 0.2179 - acc: 0.9224 - val_loss: 0.6171 - val_acc: 0.8884
Epoch 108/500
166s - loss: 0.2157 - acc: 0.9287 - val_loss: 0.6155 - val_acc: 0.8874
Training loss for fold 0 is 0.10434825514247151 with percent 95.49999998745166
Testing loss for fold 0 is 0.6042279331307662 with percent 89.05263156639901
 
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_3 (InputLayer)             (None, 65, 65, 3)     0                                            
____________________________________________________________________________________________________
conv2d_7 (Conv2D)                (None, 63, 63, 16)    448         input_3[0][0]                    
____________________________________________________________________________________________________
batch_normalization_8 (BatchNorm (None, 63, 63, 16)    64          conv2d_7[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_8 (LeakyReLU)        (None, 63, 63, 16)    0           batch_normalization_8[0][0]      
____________________________________________________________________________________________________
conv2d_8 (Conv2D)                (None, 62, 62, 16)    1040        leaky_re_lu_8[0][0]              
____________________________________________________________________________________________________
batch_normalization_9 (BatchNorm (None, 62, 62, 16)    64          conv2d_8[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_9 (LeakyReLU)        (None, 62, 62, 16)    0           batch_normalization_9[0][0]      
____________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)   (None, 31, 31, 16)    0           leaky_re_lu_9[0][0]              
____________________________________________________________________________________________________
dropout_6 (Dropout)              (None, 31, 31, 16)    0           max_pooling2d_4[0][0]            
____________________________________________________________________________________________________
conv2d_9 (Conv2D)                (None, 29, 29, 32)    4640        dropout_6[0][0]                  
____________________________________________________________________________________________________
batch_normalization_10 (BatchNor (None, 29, 29, 32)    128         conv2d_9[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_10 (LeakyReLU)       (None, 29, 29, 32)    0           batch_normalization_10[0][0]     
____________________________________________________________________________________________________
conv2d_10 (Conv2D)               (None, 28, 28, 32)    4128        leaky_re_lu_10[0][0]             
____________________________________________________________________________________________________
batch_normalization_11 (BatchNor (None, 28, 28, 32)    128         conv2d_10[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_11 (LeakyReLU)       (None, 28, 28, 32)    0           batch_normalization_11[0][0]     
____________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)   (None, 14, 14, 32)    0           leaky_re_lu_11[0][0]             
____________________________________________________________________________________________________
dropout_7 (Dropout)              (None, 14, 14, 32)    0           max_pooling2d_5[0][0]            
____________________________________________________________________________________________________
conv2d_11 (Conv2D)               (None, 12, 12, 64)    18496       dropout_7[0][0]                  
____________________________________________________________________________________________________
batch_normalization_12 (BatchNor (None, 12, 12, 64)    256         conv2d_11[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_12 (LeakyReLU)       (None, 12, 12, 64)    0           batch_normalization_12[0][0]     
____________________________________________________________________________________________________
conv2d_12 (Conv2D)               (None, 11, 11, 64)    16448       leaky_re_lu_12[0][0]             
____________________________________________________________________________________________________
batch_normalization_13 (BatchNor (None, 11, 11, 64)    256         conv2d_12[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_13 (LeakyReLU)       (None, 11, 11, 64)    0           batch_normalization_13[0][0]     
____________________________________________________________________________________________________
max_pooling2d_6 (MaxPooling2D)   (None, 5, 5, 64)      0           leaky_re_lu_13[0][0]             
____________________________________________________________________________________________________
dropout_8 (Dropout)              (None, 5, 5, 64)      0           max_pooling2d_6[0][0]            
____________________________________________________________________________________________________
flatten_2 (Flatten)              (None, 1600)          0           dropout_8[0][0]                  
____________________________________________________________________________________________________
batch_normalization_14 (BatchNor (None, 1600)          6400        flatten_2[0][0]                  
____________________________________________________________________________________________________
input_4 (InputLayer)             (None, 102)           0                                            
____________________________________________________________________________________________________
concatenate_2 (Concatenate)      (None, 1702)          0           batch_normalization_14[0][0]     
                                                                   input_4[0][0]                    
____________________________________________________________________________________________________
dense_5 (Dense)                  (None, 128)           217984      concatenate_2[0][0]              
____________________________________________________________________________________________________
leaky_re_lu_14 (LeakyReLU)       (None, 128)           0           dense_5[0][0]                    
____________________________________________________________________________________________________
dense_6 (Dense)                  (None, 64)            8256        leaky_re_lu_14[0][0]             
____________________________________________________________________________________________________
dropout_9 (Dropout)              (None, 64)            0           dense_6[0][0]                    
____________________________________________________________________________________________________
dense_7 (Dense)                  (None, 32)            2080        dropout_9[0][0]                  
____________________________________________________________________________________________________
dropout_10 (Dropout)             (None, 32)            0           dense_7[0][0]                    
____________________________________________________________________________________________________
dense_8 (Dense)                  (None, 12)            396         dropout_10[0][0]                 
====================================================================================================
Total params: 281,212
Trainable params: 277,564
Non-trainable params: 3,648
____________________________________________________________________________________________________
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
170s - loss: 1.9263 - acc: 0.3553 - val_loss: 4.1511 - val_acc: 0.2042
Epoch 2/500
167s - loss: 1.5301 - acc: 0.5026 - val_loss: 2.5112 - val_acc: 0.2558
Epoch 3/500
166s - loss: 1.3971 - acc: 0.5513 - val_loss: 1.7347 - val_acc: 0.4558
Epoch 4/500
167s - loss: 1.3220 - acc: 0.5913 - val_loss: 1.2980 - val_acc: 0.6232
Epoch 5/500
167s - loss: 1.2429 - acc: 0.6158 - val_loss: 1.9796 - val_acc: 0.4568
Epoch 6/500
166s - loss: 1.2322 - acc: 0.6326 - val_loss: 1.8824 - val_acc: 0.5158
Epoch 7/500
167s - loss: 1.2156 - acc: 0.6418 - val_loss: 1.4677 - val_acc: 0.5979
Epoch 8/500
167s - loss: 1.1410 - acc: 0.6634 - val_loss: 1.0906 - val_acc: 0.6768
Epoch 9/500
166s - loss: 1.1167 - acc: 0.6747 - val_loss: 1.1183 - val_acc: 0.7032
Epoch 10/500
167s - loss: 1.0724 - acc: 0.6953 - val_loss: 1.1376 - val_acc: 0.6621
Epoch 11/500
166s - loss: 1.0709 - acc: 0.6929 - val_loss: 1.6851 - val_acc: 0.6505
Epoch 12/500
167s - loss: 1.0642 - acc: 0.7026 - val_loss: 1.5558 - val_acc: 0.6653
Epoch 13/500
166s - loss: 0.9820 - acc: 0.7287 - val_loss: 1.5433 - val_acc: 0.6242
Epoch 14/500
167s - loss: 0.9994 - acc: 0.7284 - val_loss: 1.0256 - val_acc: 0.7611
Epoch 15/500
166s - loss: 0.9660 - acc: 0.7208 - val_loss: 0.9363 - val_acc: 0.7505
Epoch 16/500
167s - loss: 0.8968 - acc: 0.7566 - val_loss: 2.0401 - val_acc: 0.5032
Epoch 17/500
166s - loss: 0.8935 - acc: 0.7487 - val_loss: 1.1596 - val_acc: 0.7400
Epoch 18/500
166s - loss: 0.8787 - acc: 0.7595 - val_loss: 1.9376 - val_acc: 0.4895
Epoch 19/500
167s - loss: 0.9440 - acc: 0.7458 - val_loss: 1.4645 - val_acc: 0.6947
Epoch 20/500
167s - loss: 0.9070 - acc: 0.7529 - val_loss: 1.0109 - val_acc: 0.7442
Epoch 21/500
167s - loss: 0.8762 - acc: 0.7513 - val_loss: 2.5449 - val_acc: 0.4421
Epoch 22/500
167s - loss: 0.8653 - acc: 0.7626 - val_loss: 2.3207 - val_acc: 0.6116
Epoch 23/500

Epoch 00022: reducing learning rate to 0.010000000149.
167s - loss: 0.8826 - acc: 0.7639 - val_loss: 1.3290 - val_acc: 0.6368
Epoch 24/500
166s - loss: 0.7889 - acc: 0.7895 - val_loss: 0.6715 - val_acc: 0.8158
Epoch 25/500
166s - loss: 0.5702 - acc: 0.8363 - val_loss: 0.5827 - val_acc: 0.8400
Epoch 26/500
166s - loss: 0.5340 - acc: 0.8424 - val_loss: 0.5864 - val_acc: 0.8263
Epoch 27/500
166s - loss: 0.5072 - acc: 0.8508 - val_loss: 0.6122 - val_acc: 0.8242
Epoch 28/500
166s - loss: 0.4922 - acc: 0.8503 - val_loss: 0.6560 - val_acc: 0.8274
Epoch 29/500
166s - loss: 0.4559 - acc: 0.8589 - val_loss: 0.5872 - val_acc: 0.8368
Epoch 30/500
166s - loss: 0.4443 - acc: 0.8595 - val_loss: 0.5702 - val_acc: 0.8368
Epoch 31/500
166s - loss: 0.4584 - acc: 0.8629 - val_loss: 0.5768 - val_acc: 0.8421
Epoch 32/500
166s - loss: 0.4268 - acc: 0.8611 - val_loss: 0.5420 - val_acc: 0.8474
Epoch 33/500
167s - loss: 0.4065 - acc: 0.8724 - val_loss: 0.5743 - val_acc: 0.8358
Epoch 34/500
166s - loss: 0.4048 - acc: 0.8729 - val_loss: 0.5947 - val_acc: 0.8337
Epoch 35/500
166s - loss: 0.3865 - acc: 0.8761 - val_loss: 0.5620 - val_acc: 0.8453
Epoch 36/500
166s - loss: 0.3888 - acc: 0.8697 - val_loss: 0.5604 - val_acc: 0.8474
Epoch 37/500
166s - loss: 0.3691 - acc: 0.8758 - val_loss: 0.5694 - val_acc: 0.8411
Epoch 38/500
166s - loss: 0.3712 - acc: 0.8787 - val_loss: 0.5801 - val_acc: 0.8368
Epoch 39/500
166s - loss: 0.3493 - acc: 0.8842 - val_loss: 0.6505 - val_acc: 0.8421
Epoch 40/500
166s - loss: 0.3837 - acc: 0.8782 - val_loss: 0.5767 - val_acc: 0.8537
Epoch 41/500
166s - loss: 0.3666 - acc: 0.8829 - val_loss: 0.5878 - val_acc: 0.8516
Epoch 42/500
166s - loss: 0.3605 - acc: 0.8853 - val_loss: 0.5825 - val_acc: 0.8484
Epoch 43/500
166s - loss: 0.3454 - acc: 0.8889 - val_loss: 0.5871 - val_acc: 0.8537
Epoch 44/500
167s - loss: 0.3294 - acc: 0.8897 - val_loss: 0.6291 - val_acc: 0.8537
Epoch 45/500
166s - loss: 0.3443 - acc: 0.8871 - val_loss: 0.6657 - val_acc: 0.8347
Epoch 46/500
167s - loss: 0.3249 - acc: 0.8992 - val_loss: 0.5793 - val_acc: 0.8516
Epoch 47/500
166s - loss: 0.3375 - acc: 0.8997 - val_loss: 0.6132 - val_acc: 0.8484
Epoch 48/500
166s - loss: 0.3353 - acc: 0.8987 - val_loss: 0.5540 - val_acc: 0.8579
Epoch 49/500
166s - loss: 0.3274 - acc: 0.8963 - val_loss: 0.5843 - val_acc: 0.8589
Epoch 50/500
166s - loss: 0.3165 - acc: 0.8955 - val_loss: 0.6178 - val_acc: 0.8611
Epoch 51/500
166s - loss: 0.2940 - acc: 0.9032 - val_loss: 0.5802 - val_acc: 0.8684
Epoch 52/500
166s - loss: 0.3087 - acc: 0.8976 - val_loss: 0.5915 - val_acc: 0.8537
Epoch 53/500
166s - loss: 0.3202 - acc: 0.8968 - val_loss: 0.6535 - val_acc: 0.8484
Epoch 54/500
166s - loss: 0.3159 - acc: 0.9011 - val_loss: 0.5644 - val_acc: 0.8674
Epoch 55/500
166s - loss: 0.2970 - acc: 0.9079 - val_loss: 0.6644 - val_acc: 0.8495
Epoch 56/500
166s - loss: 0.2868 - acc: 0.9053 - val_loss: 0.5906 - val_acc: 0.8663
Epoch 57/500
166s - loss: 0.3041 - acc: 0.9011 - val_loss: 0.5817 - val_acc: 0.8516
Epoch 58/500
166s - loss: 0.2993 - acc: 0.9071 - val_loss: 0.5855 - val_acc: 0.8737
Epoch 59/500
166s - loss: 0.2668 - acc: 0.9071 - val_loss: 0.5941 - val_acc: 0.8632
Epoch 60/500
166s - loss: 0.2863 - acc: 0.9082 - val_loss: 0.6291 - val_acc: 0.8558
Epoch 61/500
166s - loss: 0.2966 - acc: 0.9124 - val_loss: 0.7578 - val_acc: 0.8168
Epoch 62/500
166s - loss: 0.2651 - acc: 0.9095 - val_loss: 0.6189 - val_acc: 0.8621
Epoch 63/500
166s - loss: 0.2516 - acc: 0.9176 - val_loss: 0.6497 - val_acc: 0.8653
Epoch 64/500
166s - loss: 0.2695 - acc: 0.9166 - val_loss: 0.6059 - val_acc: 0.8568
Epoch 65/500
166s - loss: 0.2745 - acc: 0.9116 - val_loss: 0.6071 - val_acc: 0.8621
Epoch 66/500
166s - loss: 0.2622 - acc: 0.9163 - val_loss: 0.6498 - val_acc: 0.8611
Epoch 67/500

Epoch 00066: reducing learning rate to 0.000999999977648.
166s - loss: 0.2323 - acc: 0.9221 - val_loss: 0.6367 - val_acc: 0.8579
Epoch 68/500
166s - loss: 0.2699 - acc: 0.9176 - val_loss: 0.5885 - val_acc: 0.8695
Epoch 69/500
166s - loss: 0.2689 - acc: 0.9158 - val_loss: 0.5976 - val_acc: 0.8705
Epoch 70/500
166s - loss: 0.2446 - acc: 0.9226 - val_loss: 0.6145 - val_acc: 0.8653
Epoch 71/500
166s - loss: 0.2623 - acc: 0.9153 - val_loss: 0.6060 - val_acc: 0.8737
Epoch 72/500
166s - loss: 0.2326 - acc: 0.9200 - val_loss: 0.6075 - val_acc: 0.8695
Epoch 73/500
166s - loss: 0.2443 - acc: 0.9200 - val_loss: 0.6207 - val_acc: 0.8684
Epoch 74/500
166s - loss: 0.2494 - acc: 0.9182 - val_loss: 0.5983 - val_acc: 0.8705
Epoch 75/500

Epoch 00074: reducing learning rate to 9.99999931082e-05.
166s - loss: 0.2166 - acc: 0.9271 - val_loss: 0.6051 - val_acc: 0.8695
Epoch 76/500
166s - loss: 0.2535 - acc: 0.9213 - val_loss: 0.6043 - val_acc: 0.8695
Epoch 77/500
166s - loss: 0.2349 - acc: 0.9232 - val_loss: 0.5992 - val_acc: 0.8705
Epoch 78/500
166s - loss: 0.2244 - acc: 0.9250 - val_loss: 0.6027 - val_acc: 0.8705
Epoch 79/500
166s - loss: 0.2484 - acc: 0.9189 - val_loss: 0.6055 - val_acc: 0.8705
Epoch 80/500
166s - loss: 0.2375 - acc: 0.9232 - val_loss: 0.6065 - val_acc: 0.8716
Epoch 81/500
166s - loss: 0.2830 - acc: 0.9161 - val_loss: 0.6030 - val_acc: 0.8726
Epoch 82/500
166s - loss: 0.2395 - acc: 0.9232 - val_loss: 0.5964 - val_acc: 0.8716
Epoch 83/500

Epoch 00082: reducing learning rate to 9.99999901978e-06.
166s - loss: 0.2456 - acc: 0.9232 - val_loss: 0.6048 - val_acc: 0.8726
Epoch 84/500
166s - loss: 0.2365 - acc: 0.9229 - val_loss: 0.6004 - val_acc: 0.8716
Epoch 85/500
166s - loss: 0.2351 - acc: 0.9218 - val_loss: 0.6015 - val_acc: 0.8695
Epoch 86/500
166s - loss: 0.2318 - acc: 0.9274 - val_loss: 0.6035 - val_acc: 0.8695
Epoch 87/500
166s - loss: 0.2370 - acc: 0.9205 - val_loss: 0.6023 - val_acc: 0.8716
Epoch 88/500
166s - loss: 0.2631 - acc: 0.9105 - val_loss: 0.5996 - val_acc: 0.8716
Epoch 89/500
166s - loss: 0.2663 - acc: 0.9189 - val_loss: 0.6048 - val_acc: 0.8716
Epoch 90/500
166s - loss: 0.2186 - acc: 0.9271 - val_loss: 0.6009 - val_acc: 0.8716
Epoch 91/500

Epoch 00090: reducing learning rate to 1e-06.
166s - loss: 0.2322 - acc: 0.9261 - val_loss: 0.6047 - val_acc: 0.8716
Epoch 92/500
166s - loss: 0.2469 - acc: 0.9168 - val_loss: 0.6019 - val_acc: 0.8695
Epoch 93/500
166s - loss: 0.2440 - acc: 0.9224 - val_loss: 0.6041 - val_acc: 0.8716
Training loss for fold 1 is 0.1234942289441824 with percent 94.97368419797797
Testing loss for fold 1 is 0.6060008970059847 with percent 87.3684210024382
 
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_5 (InputLayer)             (None, 65, 65, 3)     0                                            
____________________________________________________________________________________________________
conv2d_13 (Conv2D)               (None, 63, 63, 16)    448         input_5[0][0]                    
____________________________________________________________________________________________________
batch_normalization_15 (BatchNor (None, 63, 63, 16)    64          conv2d_13[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_15 (LeakyReLU)       (None, 63, 63, 16)    0           batch_normalization_15[0][0]     
____________________________________________________________________________________________________
conv2d_14 (Conv2D)               (None, 62, 62, 16)    1040        leaky_re_lu_15[0][0]             
____________________________________________________________________________________________________
batch_normalization_16 (BatchNor (None, 62, 62, 16)    64          conv2d_14[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_16 (LeakyReLU)       (None, 62, 62, 16)    0           batch_normalization_16[0][0]     
____________________________________________________________________________________________________
max_pooling2d_7 (MaxPooling2D)   (None, 31, 31, 16)    0           leaky_re_lu_16[0][0]             
____________________________________________________________________________________________________
dropout_11 (Dropout)             (None, 31, 31, 16)    0           max_pooling2d_7[0][0]            
____________________________________________________________________________________________________
conv2d_15 (Conv2D)               (None, 29, 29, 32)    4640        dropout_11[0][0]                 
____________________________________________________________________________________________________
batch_normalization_17 (BatchNor (None, 29, 29, 32)    128         conv2d_15[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_17 (LeakyReLU)       (None, 29, 29, 32)    0           batch_normalization_17[0][0]     
____________________________________________________________________________________________________
conv2d_16 (Conv2D)               (None, 28, 28, 32)    4128        leaky_re_lu_17[0][0]             
____________________________________________________________________________________________________
batch_normalization_18 (BatchNor (None, 28, 28, 32)    128         conv2d_16[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_18 (LeakyReLU)       (None, 28, 28, 32)    0           batch_normalization_18[0][0]     
____________________________________________________________________________________________________
max_pooling2d_8 (MaxPooling2D)   (None, 14, 14, 32)    0           leaky_re_lu_18[0][0]             
____________________________________________________________________________________________________
dropout_12 (Dropout)             (None, 14, 14, 32)    0           max_pooling2d_8[0][0]            
____________________________________________________________________________________________________
conv2d_17 (Conv2D)               (None, 12, 12, 64)    18496       dropout_12[0][0]                 
____________________________________________________________________________________________________
batch_normalization_19 (BatchNor (None, 12, 12, 64)    256         conv2d_17[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_19 (LeakyReLU)       (None, 12, 12, 64)    0           batch_normalization_19[0][0]     
____________________________________________________________________________________________________
conv2d_18 (Conv2D)               (None, 11, 11, 64)    16448       leaky_re_lu_19[0][0]             
____________________________________________________________________________________________________
batch_normalization_20 (BatchNor (None, 11, 11, 64)    256         conv2d_18[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_20 (LeakyReLU)       (None, 11, 11, 64)    0           batch_normalization_20[0][0]     
____________________________________________________________________________________________________
max_pooling2d_9 (MaxPooling2D)   (None, 5, 5, 64)      0           leaky_re_lu_20[0][0]             
____________________________________________________________________________________________________
dropout_13 (Dropout)             (None, 5, 5, 64)      0           max_pooling2d_9[0][0]            
____________________________________________________________________________________________________
flatten_3 (Flatten)              (None, 1600)          0           dropout_13[0][0]                 
____________________________________________________________________________________________________
batch_normalization_21 (BatchNor (None, 1600)          6400        flatten_3[0][0]                  
____________________________________________________________________________________________________
input_6 (InputLayer)             (None, 102)           0                                            
____________________________________________________________________________________________________
concatenate_3 (Concatenate)      (None, 1702)          0           batch_normalization_21[0][0]     
                                                                   input_6[0][0]                    
____________________________________________________________________________________________________
dense_9 (Dense)                  (None, 128)           217984      concatenate_3[0][0]              
____________________________________________________________________________________________________
leaky_re_lu_21 (LeakyReLU)       (None, 128)           0           dense_9[0][0]                    
____________________________________________________________________________________________________
dense_10 (Dense)                 (None, 64)            8256        leaky_re_lu_21[0][0]             
____________________________________________________________________________________________________
dropout_14 (Dropout)             (None, 64)            0           dense_10[0][0]                   
____________________________________________________________________________________________________
dense_11 (Dense)                 (None, 32)            2080        dropout_14[0][0]                 
____________________________________________________________________________________________________
dropout_15 (Dropout)             (None, 32)            0           dense_11[0][0]                   
____________________________________________________________________________________________________
dense_12 (Dense)                 (None, 12)            396         dropout_15[0][0]                 
====================================================================================================
Total params: 281,212
Trainable params: 277,564
Non-trainable params: 3,648
____________________________________________________________________________________________________
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
171s - loss: 2.1123 - acc: 0.2787 - val_loss: 5.2517 - val_acc: 0.1463
Epoch 2/500
167s - loss: 1.6705 - acc: 0.4563 - val_loss: 2.2032 - val_acc: 0.3621
Epoch 3/500
167s - loss: 1.4549 - acc: 0.5350 - val_loss: 2.3233 - val_acc: 0.4042
Epoch 4/500
167s - loss: 1.3625 - acc: 0.5729 - val_loss: 1.7361 - val_acc: 0.4863
Epoch 5/500
167s - loss: 1.3527 - acc: 0.5900 - val_loss: 1.6914 - val_acc: 0.5105
Epoch 6/500
167s - loss: 1.2924 - acc: 0.6079 - val_loss: 1.2634 - val_acc: 0.6663
Epoch 7/500
166s - loss: 1.2499 - acc: 0.6126 - val_loss: 2.2478 - val_acc: 0.3811
Epoch 8/500
167s - loss: 1.1719 - acc: 0.6458 - val_loss: 1.4637 - val_acc: 0.5747
Epoch 9/500
167s - loss: 1.1281 - acc: 0.6618 - val_loss: 2.2710 - val_acc: 0.3474
Epoch 10/500
166s - loss: 1.1410 - acc: 0.6732 - val_loss: 1.3942 - val_acc: 0.6063
Epoch 11/500
167s - loss: 1.1043 - acc: 0.6837 - val_loss: 3.9135 - val_acc: 0.2947
Epoch 12/500
167s - loss: 1.0599 - acc: 0.6979 - val_loss: 1.9603 - val_acc: 0.5000
Epoch 13/500
167s - loss: 1.0169 - acc: 0.7082 - val_loss: 2.9408 - val_acc: 0.4400
Epoch 14/500
166s - loss: 1.0580 - acc: 0.7211 - val_loss: 1.7840 - val_acc: 0.5126
Epoch 15/500
166s - loss: 1.0597 - acc: 0.7053 - val_loss: 1.0389 - val_acc: 0.7232
Epoch 16/500
167s - loss: 0.9835 - acc: 0.7279 - val_loss: 2.4083 - val_acc: 0.4779
Epoch 17/500
167s - loss: 1.0114 - acc: 0.7179 - val_loss: 1.4543 - val_acc: 0.6600
Epoch 18/500
167s - loss: 0.9714 - acc: 0.7342 - val_loss: 1.7692 - val_acc: 0.6779
Epoch 19/500
167s - loss: 1.0124 - acc: 0.7326 - val_loss: 2.1628 - val_acc: 0.4453
Epoch 20/500
167s - loss: 0.9782 - acc: 0.7403 - val_loss: 3.1851 - val_acc: 0.5011
Epoch 21/500
167s - loss: 1.0297 - acc: 0.7339 - val_loss: 1.5282 - val_acc: 0.6863
Epoch 22/500
167s - loss: 1.0246 - acc: 0.7297 - val_loss: 1.4012 - val_acc: 0.6821
Epoch 23/500
167s - loss: 0.9611 - acc: 0.7453 - val_loss: 1.8159 - val_acc: 0.5989
Epoch 24/500

Epoch 00023: reducing learning rate to 0.010000000149.
167s - loss: 0.9759 - acc: 0.7484 - val_loss: 2.9116 - val_acc: 0.3063
Epoch 25/500
167s - loss: 0.9586 - acc: 0.7639 - val_loss: 1.0108 - val_acc: 0.7516
Epoch 26/500
167s - loss: 0.7036 - acc: 0.8029 - val_loss: 0.9741 - val_acc: 0.7716
Epoch 27/500
166s - loss: 0.6008 - acc: 0.8197 - val_loss: 0.9149 - val_acc: 0.7589
Epoch 28/500
166s - loss: 0.6192 - acc: 0.8253 - val_loss: 0.8318 - val_acc: 0.7811
Epoch 29/500
167s - loss: 0.5453 - acc: 0.8418 - val_loss: 0.8799 - val_acc: 0.8021
Epoch 30/500
167s - loss: 0.5139 - acc: 0.8426 - val_loss: 0.9483 - val_acc: 0.7895
Epoch 31/500
167s - loss: 0.5322 - acc: 0.8424 - val_loss: 0.7811 - val_acc: 0.7979
Epoch 32/500
167s - loss: 0.5179 - acc: 0.8487 - val_loss: 0.8144 - val_acc: 0.7958
Epoch 33/500
166s - loss: 0.5051 - acc: 0.8532 - val_loss: 0.8378 - val_acc: 0.8000
Epoch 34/500
167s - loss: 0.5184 - acc: 0.8445 - val_loss: 0.7103 - val_acc: 0.8095
Epoch 35/500
167s - loss: 0.4736 - acc: 0.8492 - val_loss: 0.7044 - val_acc: 0.8042
Epoch 36/500
167s - loss: 0.4722 - acc: 0.8589 - val_loss: 0.7185 - val_acc: 0.8200
Epoch 37/500
167s - loss: 0.4420 - acc: 0.8642 - val_loss: 0.6808 - val_acc: 0.8211
Epoch 38/500
166s - loss: 0.4773 - acc: 0.8618 - val_loss: 0.7472 - val_acc: 0.8189
Epoch 39/500
167s - loss: 0.4416 - acc: 0.8684 - val_loss: 0.7397 - val_acc: 0.8211
Epoch 40/500
166s - loss: 0.4187 - acc: 0.8729 - val_loss: 0.7952 - val_acc: 0.8179
Epoch 41/500
167s - loss: 0.4452 - acc: 0.8687 - val_loss: 0.8718 - val_acc: 0.8074
Epoch 42/500
167s - loss: 0.4614 - acc: 0.8658 - val_loss: 0.7468 - val_acc: 0.8242
Epoch 43/500
166s - loss: 0.4417 - acc: 0.8637 - val_loss: 0.7277 - val_acc: 0.8263
Epoch 44/500
167s - loss: 0.4304 - acc: 0.8732 - val_loss: 0.9467 - val_acc: 0.7705
Epoch 45/500
166s - loss: 0.3993 - acc: 0.8834 - val_loss: 0.8733 - val_acc: 0.8011
Epoch 46/500
167s - loss: 0.4102 - acc: 0.8787 - val_loss: 0.7017 - val_acc: 0.8232
Epoch 47/500
166s - loss: 0.4299 - acc: 0.8768 - val_loss: 0.9983 - val_acc: 0.7632
Epoch 48/500
166s - loss: 0.3933 - acc: 0.8824 - val_loss: 0.8818 - val_acc: 0.8242
Epoch 49/500
166s - loss: 0.3989 - acc: 0.8768 - val_loss: 0.8502 - val_acc: 0.7884
Epoch 50/500
167s - loss: 0.4063 - acc: 0.8868 - val_loss: 0.8144 - val_acc: 0.8295
Epoch 51/500
166s - loss: 0.3949 - acc: 0.8800 - val_loss: 0.8107 - val_acc: 0.8200
Epoch 52/500
166s - loss: 0.4043 - acc: 0.8708 - val_loss: 0.6702 - val_acc: 0.8337
Epoch 53/500
166s - loss: 0.3749 - acc: 0.8855 - val_loss: 0.7825 - val_acc: 0.8232
Epoch 54/500
166s - loss: 0.3844 - acc: 0.8805 - val_loss: 0.7579 - val_acc: 0.8368
Epoch 55/500
166s - loss: 0.3495 - acc: 0.8895 - val_loss: 0.7219 - val_acc: 0.8295
Epoch 56/500
166s - loss: 0.3745 - acc: 0.8882 - val_loss: 0.8054 - val_acc: 0.8147
Epoch 57/500
166s - loss: 0.3589 - acc: 0.8808 - val_loss: 0.7576 - val_acc: 0.8200
Epoch 58/500
166s - loss: 0.3746 - acc: 0.8845 - val_loss: 0.6881 - val_acc: 0.8211
Epoch 59/500
167s - loss: 0.3391 - acc: 0.8950 - val_loss: 0.7244 - val_acc: 0.8368
Epoch 60/500
166s - loss: 0.3303 - acc: 0.8932 - val_loss: 0.6491 - val_acc: 0.8379
Epoch 61/500
166s - loss: 0.3385 - acc: 0.8976 - val_loss: 0.7817 - val_acc: 0.8232
Epoch 62/500
166s - loss: 0.3446 - acc: 0.8884 - val_loss: 0.6831 - val_acc: 0.8379
Epoch 63/500
166s - loss: 0.3476 - acc: 0.8889 - val_loss: 0.6834 - val_acc: 0.8274
Epoch 64/500
166s - loss: 0.3477 - acc: 0.8955 - val_loss: 0.8035 - val_acc: 0.8305
Epoch 65/500
166s - loss: 0.3509 - acc: 0.8926 - val_loss: 0.7741 - val_acc: 0.8274
Epoch 66/500
167s - loss: 0.3169 - acc: 0.9005 - val_loss: 0.7704 - val_acc: 0.8379
Epoch 67/500
166s - loss: 0.3284 - acc: 0.8976 - val_loss: 0.8390 - val_acc: 0.8211
Epoch 68/500
167s - loss: 0.3491 - acc: 0.8992 - val_loss: 0.7555 - val_acc: 0.8347
Epoch 69/500

Epoch 00068: reducing learning rate to 0.000999999977648.
167s - loss: 0.3109 - acc: 0.9021 - val_loss: 0.8854 - val_acc: 0.8232
Epoch 70/500
167s - loss: 0.3162 - acc: 0.9029 - val_loss: 0.7073 - val_acc: 0.8453
Epoch 71/500
167s - loss: 0.3120 - acc: 0.9045 - val_loss: 0.7134 - val_acc: 0.8453
Epoch 72/500
167s - loss: 0.3025 - acc: 0.9074 - val_loss: 0.7134 - val_acc: 0.8411
Epoch 73/500
167s - loss: 0.3062 - acc: 0.9026 - val_loss: 0.7133 - val_acc: 0.8379
Epoch 74/500
167s - loss: 0.3030 - acc: 0.9042 - val_loss: 0.6813 - val_acc: 0.8421
Epoch 75/500
167s - loss: 0.3037 - acc: 0.9066 - val_loss: 0.7064 - val_acc: 0.8421
Epoch 76/500
167s - loss: 0.3098 - acc: 0.9047 - val_loss: 0.7038 - val_acc: 0.8421
Epoch 77/500
167s - loss: 0.2957 - acc: 0.9082 - val_loss: 0.6839 - val_acc: 0.8432
Epoch 78/500
167s - loss: 0.3052 - acc: 0.9084 - val_loss: 0.6847 - val_acc: 0.8421
Epoch 79/500

Epoch 00078: reducing learning rate to 9.99999931082e-05.
167s - loss: 0.2829 - acc: 0.9126 - val_loss: 0.7114 - val_acc: 0.8442
Epoch 80/500
167s - loss: 0.2918 - acc: 0.9097 - val_loss: 0.7132 - val_acc: 0.8442
Epoch 81/500
167s - loss: 0.3036 - acc: 0.9045 - val_loss: 0.7085 - val_acc: 0.8432
Epoch 82/500
166s - loss: 0.3016 - acc: 0.9082 - val_loss: 0.7058 - val_acc: 0.8453
Epoch 83/500
166s - loss: 0.2862 - acc: 0.9097 - val_loss: 0.7171 - val_acc: 0.8453
Epoch 84/500
167s - loss: 0.2748 - acc: 0.9113 - val_loss: 0.7177 - val_acc: 0.8442
Epoch 85/500
166s - loss: 0.2925 - acc: 0.9021 - val_loss: 0.7128 - val_acc: 0.8442
Epoch 86/500
166s - loss: 0.2690 - acc: 0.9137 - val_loss: 0.7082 - val_acc: 0.8442
Epoch 87/500

Epoch 00086: reducing learning rate to 9.99999901978e-06.
166s - loss: 0.2807 - acc: 0.9116 - val_loss: 0.7092 - val_acc: 0.8442
Epoch 88/500
166s - loss: 0.2809 - acc: 0.9084 - val_loss: 0.7242 - val_acc: 0.8432
Epoch 89/500
167s - loss: 0.3016 - acc: 0.9082 - val_loss: 0.7230 - val_acc: 0.8421
Epoch 90/500
166s - loss: 0.2960 - acc: 0.9108 - val_loss: 0.7209 - val_acc: 0.8442
Epoch 91/500
166s - loss: 0.3046 - acc: 0.9095 - val_loss: 0.7118 - val_acc: 0.8421
Epoch 92/500
166s - loss: 0.2935 - acc: 0.9084 - val_loss: 0.7131 - val_acc: 0.8421
Epoch 93/500
167s - loss: 0.3018 - acc: 0.9084 - val_loss: 0.7189 - val_acc: 0.8432
Epoch 94/500
166s - loss: 0.2969 - acc: 0.9089 - val_loss: 0.7116 - val_acc: 0.8411
Epoch 95/500

Epoch 00094: reducing learning rate to 1e-06.
166s - loss: 0.2942 - acc: 0.9066 - val_loss: 0.7203 - val_acc: 0.8432
Epoch 96/500
166s - loss: 0.2889 - acc: 0.9121 - val_loss: 0.7156 - val_acc: 0.8432
Epoch 97/500
167s - loss: 0.2871 - acc: 0.9055 - val_loss: 0.7220 - val_acc: 0.8421
Epoch 98/500
166s - loss: 0.2990 - acc: 0.9116 - val_loss: 0.7237 - val_acc: 0.8421
Epoch 99/500
166s - loss: 0.2855 - acc: 0.9066 - val_loss: 0.7164 - val_acc: 0.8442
Epoch 100/500
167s - loss: 0.2918 - acc: 0.9105 - val_loss: 0.7168 - val_acc: 0.8421
Epoch 101/500
166s - loss: 0.2805 - acc: 0.9111 - val_loss: 0.7254 - val_acc: 0.8421
Epoch 102/500
167s - loss: 0.2731 - acc: 0.9097 - val_loss: 0.7171 - val_acc: 0.8421
Epoch 103/500
166s - loss: 0.2895 - acc: 0.9116 - val_loss: 0.7146 - val_acc: 0.8442
Epoch 104/500
166s - loss: 0.2806 - acc: 0.9092 - val_loss: 0.7229 - val_acc: 0.8421
Epoch 105/500
166s - loss: 0.2811 - acc: 0.9076 - val_loss: 0.7222 - val_acc: 0.8453
Epoch 106/500
166s - loss: 0.3251 - acc: 0.9034 - val_loss: 0.7193 - val_acc: 0.8442
Epoch 107/500
167s - loss: 0.3021 - acc: 0.9045 - val_loss: 0.7097 - val_acc: 0.8442
Epoch 108/500
166s - loss: 0.2999 - acc: 0.9071 - val_loss: 0.7123 - val_acc: 0.8442
Epoch 109/500
166s - loss: 0.2843 - acc: 0.9063 - val_loss: 0.7213 - val_acc: 0.8432
Epoch 110/500
166s - loss: 0.3026 - acc: 0.9047 - val_loss: 0.7236 - val_acc: 0.8421
Epoch 111/500
166s - loss: 0.2938 - acc: 0.9068 - val_loss: 0.7091 - val_acc: 0.8453
Epoch 112/500
167s - loss: 0.2991 - acc: 0.9053 - val_loss: 0.7201 - val_acc: 0.8432
Epoch 113/500
166s - loss: 0.2892 - acc: 0.9058 - val_loss: 0.7231 - val_acc: 0.8411
Epoch 114/500
166s - loss: 0.2976 - acc: 0.9071 - val_loss: 0.7215 - val_acc: 0.8400
Epoch 115/500
166s - loss: 0.2839 - acc: 0.9105 - val_loss: 0.7135 - val_acc: 0.8421
Epoch 116/500
167s - loss: 0.3115 - acc: 0.9026 - val_loss: 0.7122 - val_acc: 0.8463
Epoch 117/500
167s - loss: 0.3051 - acc: 0.9071 - val_loss: 0.7200 - val_acc: 0.8442
Epoch 118/500
166s - loss: 0.3082 - acc: 0.9103 - val_loss: 0.7064 - val_acc: 0.8442
Epoch 119/500
166s - loss: 0.2936 - acc: 0.9089 - val_loss: 0.7177 - val_acc: 0.8411
Epoch 120/500
166s - loss: 0.2873 - acc: 0.9092 - val_loss: 0.7185 - val_acc: 0.8411
Epoch 121/500
166s - loss: 0.3131 - acc: 0.9058 - val_loss: 0.7089 - val_acc: 0.8421
Training loss for fold 2 is 0.1689524223969171 with percent 93.63157893482007
Testing loss for fold 2 is 0.7122274990771946 with percent 84.63157890972339
 
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_7 (InputLayer)             (None, 65, 65, 3)     0                                            
____________________________________________________________________________________________________
conv2d_19 (Conv2D)               (None, 63, 63, 16)    448         input_7[0][0]                    
____________________________________________________________________________________________________
batch_normalization_22 (BatchNor (None, 63, 63, 16)    64          conv2d_19[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_22 (LeakyReLU)       (None, 63, 63, 16)    0           batch_normalization_22[0][0]     
____________________________________________________________________________________________________
conv2d_20 (Conv2D)               (None, 62, 62, 16)    1040        leaky_re_lu_22[0][0]             
____________________________________________________________________________________________________
batch_normalization_23 (BatchNor (None, 62, 62, 16)    64          conv2d_20[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_23 (LeakyReLU)       (None, 62, 62, 16)    0           batch_normalization_23[0][0]     
____________________________________________________________________________________________________
max_pooling2d_10 (MaxPooling2D)  (None, 31, 31, 16)    0           leaky_re_lu_23[0][0]             
____________________________________________________________________________________________________
dropout_16 (Dropout)             (None, 31, 31, 16)    0           max_pooling2d_10[0][0]           
____________________________________________________________________________________________________
conv2d_21 (Conv2D)               (None, 29, 29, 32)    4640        dropout_16[0][0]                 
____________________________________________________________________________________________________
batch_normalization_24 (BatchNor (None, 29, 29, 32)    128         conv2d_21[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_24 (LeakyReLU)       (None, 29, 29, 32)    0           batch_normalization_24[0][0]     
____________________________________________________________________________________________________
conv2d_22 (Conv2D)               (None, 28, 28, 32)    4128        leaky_re_lu_24[0][0]             
____________________________________________________________________________________________________
batch_normalization_25 (BatchNor (None, 28, 28, 32)    128         conv2d_22[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_25 (LeakyReLU)       (None, 28, 28, 32)    0           batch_normalization_25[0][0]     
____________________________________________________________________________________________________
max_pooling2d_11 (MaxPooling2D)  (None, 14, 14, 32)    0           leaky_re_lu_25[0][0]             
____________________________________________________________________________________________________
dropout_17 (Dropout)             (None, 14, 14, 32)    0           max_pooling2d_11[0][0]           
____________________________________________________________________________________________________
conv2d_23 (Conv2D)               (None, 12, 12, 64)    18496       dropout_17[0][0]                 
____________________________________________________________________________________________________
batch_normalization_26 (BatchNor (None, 12, 12, 64)    256         conv2d_23[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_26 (LeakyReLU)       (None, 12, 12, 64)    0           batch_normalization_26[0][0]     
____________________________________________________________________________________________________
conv2d_24 (Conv2D)               (None, 11, 11, 64)    16448       leaky_re_lu_26[0][0]             
____________________________________________________________________________________________________
batch_normalization_27 (BatchNor (None, 11, 11, 64)    256         conv2d_24[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_27 (LeakyReLU)       (None, 11, 11, 64)    0           batch_normalization_27[0][0]     
____________________________________________________________________________________________________
max_pooling2d_12 (MaxPooling2D)  (None, 5, 5, 64)      0           leaky_re_lu_27[0][0]             
____________________________________________________________________________________________________
dropout_18 (Dropout)             (None, 5, 5, 64)      0           max_pooling2d_12[0][0]           
____________________________________________________________________________________________________
flatten_4 (Flatten)              (None, 1600)          0           dropout_18[0][0]                 
____________________________________________________________________________________________________
batch_normalization_28 (BatchNor (None, 1600)          6400        flatten_4[0][0]                  
____________________________________________________________________________________________________
input_8 (InputLayer)             (None, 102)           0                                            
____________________________________________________________________________________________________
concatenate_4 (Concatenate)      (None, 1702)          0           batch_normalization_28[0][0]     
                                                                   input_8[0][0]                    
____________________________________________________________________________________________________
dense_13 (Dense)                 (None, 128)           217984      concatenate_4[0][0]              
____________________________________________________________________________________________________
leaky_re_lu_28 (LeakyReLU)       (None, 128)           0           dense_13[0][0]                   
____________________________________________________________________________________________________
dense_14 (Dense)                 (None, 64)            8256        leaky_re_lu_28[0][0]             
____________________________________________________________________________________________________
dropout_19 (Dropout)             (None, 64)            0           dense_14[0][0]                   
____________________________________________________________________________________________________
dense_15 (Dense)                 (None, 32)            2080        dropout_19[0][0]                 
____________________________________________________________________________________________________
dropout_20 (Dropout)             (None, 32)            0           dense_15[0][0]                   
____________________________________________________________________________________________________
dense_16 (Dense)                 (None, 12)            396         dropout_20[0][0]                 
====================================================================================================
Total params: 281,212
Trainable params: 277,564
Non-trainable params: 3,648
____________________________________________________________________________________________________
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
171s - loss: 2.0332 - acc: 0.3074 - val_loss: 1.8165 - val_acc: 0.4147
Epoch 2/500
167s - loss: 1.5050 - acc: 0.5116 - val_loss: 2.8989 - val_acc: 0.2411
Epoch 3/500
167s - loss: 1.3215 - acc: 0.5908 - val_loss: 2.4614 - val_acc: 0.2895
Epoch 4/500
167s - loss: 1.2374 - acc: 0.6232 - val_loss: 4.6298 - val_acc: 0.4011
Epoch 5/500
167s - loss: 1.2315 - acc: 0.6305 - val_loss: 1.5355 - val_acc: 0.5621
Epoch 6/500
167s - loss: 1.1383 - acc: 0.6518 - val_loss: 3.4366 - val_acc: 0.2895
Epoch 7/500
167s - loss: 1.1178 - acc: 0.6697 - val_loss: 2.4125 - val_acc: 0.4453
Epoch 8/500
167s - loss: 1.0820 - acc: 0.6855 - val_loss: 2.0214 - val_acc: 0.3789
Epoch 9/500
167s - loss: 1.0312 - acc: 0.7066 - val_loss: 1.5091 - val_acc: 0.6105
Epoch 10/500
167s - loss: 0.9905 - acc: 0.7055 - val_loss: 1.1747 - val_acc: 0.6400
Epoch 11/500
167s - loss: 0.9784 - acc: 0.7171 - val_loss: 1.6469 - val_acc: 0.5537
Epoch 12/500
167s - loss: 0.9657 - acc: 0.7279 - val_loss: 1.1553 - val_acc: 0.7063
Epoch 13/500
167s - loss: 0.9676 - acc: 0.7374 - val_loss: 0.7848 - val_acc: 0.7347
Epoch 14/500
168s - loss: 0.9226 - acc: 0.7524 - val_loss: 1.7463 - val_acc: 0.5063
Epoch 15/500
167s - loss: 0.9180 - acc: 0.7408 - val_loss: 2.0102 - val_acc: 0.4758
Epoch 16/500
167s - loss: 0.9150 - acc: 0.7595 - val_loss: 1.3000 - val_acc: 0.7516
Epoch 17/500
167s - loss: 0.9704 - acc: 0.7595 - val_loss: 1.7766 - val_acc: 0.6232
Epoch 18/500
167s - loss: 0.9085 - acc: 0.7542 - val_loss: 0.8217 - val_acc: 0.7853
Epoch 19/500
167s - loss: 0.8287 - acc: 0.7758 - val_loss: 1.8190 - val_acc: 0.6411
Epoch 20/500
167s - loss: 0.8808 - acc: 0.7705 - val_loss: 1.4979 - val_acc: 0.6200
Epoch 21/500
167s - loss: 0.9461 - acc: 0.7608 - val_loss: 1.0048 - val_acc: 0.7695
Epoch 22/500
167s - loss: 0.8476 - acc: 0.7729 - val_loss: 1.3066 - val_acc: 0.7147
Epoch 23/500
167s - loss: 0.9156 - acc: 0.7674 - val_loss: 1.9196 - val_acc: 0.5621
Epoch 24/500
167s - loss: 0.8498 - acc: 0.7811 - val_loss: 0.9415 - val_acc: 0.7863
Epoch 25/500
167s - loss: 0.9186 - acc: 0.7916 - val_loss: 1.7291 - val_acc: 0.6189
Epoch 26/500
167s - loss: 0.8567 - acc: 0.7850 - val_loss: 3.2713 - val_acc: 0.4326
Epoch 27/500
167s - loss: 0.9780 - acc: 0.7518 - val_loss: 1.4511 - val_acc: 0.7484
Epoch 28/500
167s - loss: 0.8341 - acc: 0.8013 - val_loss: 2.0215 - val_acc: 0.6242
Epoch 29/500
167s - loss: 0.9242 - acc: 0.7932 - val_loss: 0.9793 - val_acc: 0.7705
Epoch 30/500
167s - loss: 0.9305 - acc: 0.7739 - val_loss: 1.9855 - val_acc: 0.6905
Epoch 31/500
167s - loss: 0.8963 - acc: 0.7874 - val_loss: 1.4384 - val_acc: 0.6621
Epoch 32/500
167s - loss: 0.9116 - acc: 0.7879 - val_loss: 1.2029 - val_acc: 0.7663
Epoch 33/500

Epoch 00032: reducing learning rate to 0.010000000149.
167s - loss: 0.8302 - acc: 0.8021 - val_loss: 1.2757 - val_acc: 0.7537
Epoch 34/500
167s - loss: 0.7540 - acc: 0.8189 - val_loss: 0.7261 - val_acc: 0.8347
Epoch 35/500
167s - loss: 0.6129 - acc: 0.8358 - val_loss: 0.7541 - val_acc: 0.8347
Epoch 36/500
167s - loss: 0.5598 - acc: 0.8508 - val_loss: 0.6591 - val_acc: 0.8495
Epoch 37/500
167s - loss: 0.5288 - acc: 0.8566 - val_loss: 0.5950 - val_acc: 0.8600
Epoch 38/500
168s - loss: 0.4955 - acc: 0.8658 - val_loss: 0.5689 - val_acc: 0.8632
Epoch 39/500
167s - loss: 0.4875 - acc: 0.8634 - val_loss: 0.5852 - val_acc: 0.8674
Epoch 40/500
167s - loss: 0.4652 - acc: 0.8653 - val_loss: 0.6097 - val_acc: 0.8589
Epoch 41/500
167s - loss: 0.4716 - acc: 0.8724 - val_loss: 0.5369 - val_acc: 0.8768
Epoch 42/500
167s - loss: 0.4033 - acc: 0.8821 - val_loss: 0.5921 - val_acc: 0.8663
Epoch 43/500
167s - loss: 0.4336 - acc: 0.8774 - val_loss: 0.6227 - val_acc: 0.8726
Epoch 44/500
167s - loss: 0.4514 - acc: 0.8739 - val_loss: 0.5428 - val_acc: 0.8726
Epoch 45/500
167s - loss: 0.4262 - acc: 0.8792 - val_loss: 0.5107 - val_acc: 0.8789
Epoch 46/500
167s - loss: 0.3946 - acc: 0.8853 - val_loss: 0.5368 - val_acc: 0.8726
Epoch 47/500
167s - loss: 0.3895 - acc: 0.8842 - val_loss: 0.5753 - val_acc: 0.8674
Epoch 48/500
167s - loss: 0.4022 - acc: 0.8863 - val_loss: 0.5321 - val_acc: 0.8747
Epoch 49/500
167s - loss: 0.4072 - acc: 0.8818 - val_loss: 0.6263 - val_acc: 0.8611
Epoch 50/500
167s - loss: 0.3781 - acc: 0.8913 - val_loss: 0.6017 - val_acc: 0.8684
Epoch 51/500
167s - loss: 0.3851 - acc: 0.8874 - val_loss: 0.5587 - val_acc: 0.8695
Epoch 52/500
167s - loss: 0.3601 - acc: 0.8921 - val_loss: 0.5499 - val_acc: 0.8758
Epoch 53/500
167s - loss: 0.3762 - acc: 0.8913 - val_loss: 0.5491 - val_acc: 0.8789
Epoch 54/500
167s - loss: 0.3579 - acc: 0.8916 - val_loss: 0.5287 - val_acc: 0.8800
Epoch 55/500
167s - loss: 0.3580 - acc: 0.8900 - val_loss: 0.5413 - val_acc: 0.8832
Epoch 56/500
167s - loss: 0.3628 - acc: 0.8926 - val_loss: 0.5751 - val_acc: 0.8663
Epoch 57/500
168s - loss: 0.3727 - acc: 0.8900 - val_loss: 0.5030 - val_acc: 0.8842
Epoch 58/500
167s - loss: 0.3372 - acc: 0.8913 - val_loss: 0.5471 - val_acc: 0.8768
Epoch 59/500
167s - loss: 0.3406 - acc: 0.8942 - val_loss: 0.5287 - val_acc: 0.8737
Epoch 60/500
167s - loss: 0.3366 - acc: 0.8950 - val_loss: 0.5449 - val_acc: 0.8832
Epoch 61/500
167s - loss: 0.3126 - acc: 0.9016 - val_loss: 0.5826 - val_acc: 0.8758
Epoch 62/500
167s - loss: 0.3195 - acc: 0.8982 - val_loss: 0.5573 - val_acc: 0.8705
Epoch 63/500
167s - loss: 0.3236 - acc: 0.9039 - val_loss: 0.5148 - val_acc: 0.8800
Epoch 64/500
167s - loss: 0.3808 - acc: 0.8903 - val_loss: 0.5444 - val_acc: 0.8779
Epoch 65/500
167s - loss: 0.3103 - acc: 0.9050 - val_loss: 0.5190 - val_acc: 0.8821
Epoch 66/500
167s - loss: 0.3508 - acc: 0.9003 - val_loss: 0.5106 - val_acc: 0.8874
Epoch 67/500
167s - loss: 0.3062 - acc: 0.9032 - val_loss: 0.5224 - val_acc: 0.8821
Epoch 68/500
167s - loss: 0.3254 - acc: 0.9032 - val_loss: 0.5732 - val_acc: 0.8768
Epoch 69/500
167s - loss: 0.3147 - acc: 0.9034 - val_loss: 0.5213 - val_acc: 0.8821
Epoch 70/500
167s - loss: 0.3317 - acc: 0.9029 - val_loss: 0.4636 - val_acc: 0.8937
Epoch 71/500
167s - loss: 0.2837 - acc: 0.9063 - val_loss: 0.5046 - val_acc: 0.8895
Epoch 72/500
167s - loss: 0.2996 - acc: 0.9084 - val_loss: 0.5130 - val_acc: 0.8853
Epoch 73/500
167s - loss: 0.3094 - acc: 0.9018 - val_loss: 0.4807 - val_acc: 0.8958
Epoch 74/500
167s - loss: 0.3043 - acc: 0.9021 - val_loss: 0.4917 - val_acc: 0.8884
Epoch 75/500
167s - loss: 0.2685 - acc: 0.9163 - val_loss: 0.5188 - val_acc: 0.8874
Epoch 76/500
167s - loss: 0.3061 - acc: 0.9082 - val_loss: 0.4983 - val_acc: 0.8926
Epoch 77/500
167s - loss: 0.3263 - acc: 0.9045 - val_loss: 0.4909 - val_acc: 0.8905
Epoch 78/500
167s - loss: 0.2844 - acc: 0.9113 - val_loss: 0.5202 - val_acc: 0.8884
Epoch 79/500
167s - loss: 0.2643 - acc: 0.9108 - val_loss: 0.5292 - val_acc: 0.8832
Epoch 80/500
167s - loss: 0.2785 - acc: 0.9097 - val_loss: 0.4840 - val_acc: 0.8968
Epoch 81/500
167s - loss: 0.2560 - acc: 0.9179 - val_loss: 0.4977 - val_acc: 0.8958
Epoch 82/500
167s - loss: 0.2725 - acc: 0.9145 - val_loss: 0.4950 - val_acc: 0.8895
Epoch 83/500
167s - loss: 0.2701 - acc: 0.9132 - val_loss: 0.5335 - val_acc: 0.8874
Epoch 84/500
167s - loss: 0.2682 - acc: 0.9121 - val_loss: 0.4954 - val_acc: 0.8884
Epoch 85/500
167s - loss: 0.2804 - acc: 0.9118 - val_loss: 0.4947 - val_acc: 0.8842
Epoch 86/500
167s - loss: 0.2839 - acc: 0.9139 - val_loss: 0.5220 - val_acc: 0.8895
Epoch 87/500
167s - loss: 0.2729 - acc: 0.9124 - val_loss: 0.5330 - val_acc: 0.8821
Epoch 88/500
167s - loss: 0.2817 - acc: 0.9063 - val_loss: 0.4832 - val_acc: 0.8958
Epoch 89/500

Epoch 00088: reducing learning rate to 0.000999999977648.
167s - loss: 0.2593 - acc: 0.9145 - val_loss: 0.5020 - val_acc: 0.8884
Epoch 90/500
167s - loss: 0.2334 - acc: 0.9224 - val_loss: 0.4766 - val_acc: 0.8916
Epoch 91/500
167s - loss: 0.2620 - acc: 0.9171 - val_loss: 0.4722 - val_acc: 0.8937
Epoch 92/500
167s - loss: 0.2783 - acc: 0.9168 - val_loss: 0.4719 - val_acc: 0.8916
Epoch 93/500
168s - loss: 0.2578 - acc: 0.9192 - val_loss: 0.4681 - val_acc: 0.8979
Epoch 94/500
167s - loss: 0.2527 - acc: 0.9245 - val_loss: 0.4659 - val_acc: 0.8916
Epoch 95/500
167s - loss: 0.2409 - acc: 0.9226 - val_loss: 0.4735 - val_acc: 0.8926
Epoch 96/500
167s - loss: 0.2567 - acc: 0.9229 - val_loss: 0.4749 - val_acc: 0.8916
Epoch 97/500
167s - loss: 0.2551 - acc: 0.9234 - val_loss: 0.4761 - val_acc: 0.8926
Epoch 98/500
167s - loss: 0.2280 - acc: 0.9258 - val_loss: 0.4802 - val_acc: 0.8926
Epoch 99/500
167s - loss: 0.2449 - acc: 0.9245 - val_loss: 0.4739 - val_acc: 0.8947
Epoch 100/500
167s - loss: 0.2678 - acc: 0.9229 - val_loss: 0.4728 - val_acc: 0.8937
Epoch 101/500
167s - loss: 0.2413 - acc: 0.9237 - val_loss: 0.4758 - val_acc: 0.8937
Epoch 102/500

Epoch 00101: reducing learning rate to 9.99999931082e-05.
167s - loss: 0.2546 - acc: 0.9203 - val_loss: 0.4818 - val_acc: 0.8947
Epoch 103/500
167s - loss: 0.2366 - acc: 0.9282 - val_loss: 0.4798 - val_acc: 0.8947
Epoch 104/500
167s - loss: 0.2415 - acc: 0.9271 - val_loss: 0.4783 - val_acc: 0.8937
Epoch 105/500
167s - loss: 0.2510 - acc: 0.9168 - val_loss: 0.4776 - val_acc: 0.8937
Epoch 106/500
167s - loss: 0.2440 - acc: 0.9237 - val_loss: 0.4794 - val_acc: 0.8958
Epoch 107/500
167s - loss: 0.2397 - acc: 0.9276 - val_loss: 0.4802 - val_acc: 0.8926
Epoch 108/500
167s - loss: 0.2266 - acc: 0.9239 - val_loss: 0.4780 - val_acc: 0.8937
Epoch 109/500
167s - loss: 0.2581 - acc: 0.9208 - val_loss: 0.4808 - val_acc: 0.8947
Epoch 110/500

Epoch 00109: reducing learning rate to 9.99999901978e-06.
167s - loss: 0.2432 - acc: 0.9239 - val_loss: 0.4793 - val_acc: 0.8926
Epoch 111/500
167s - loss: 0.2371 - acc: 0.9208 - val_loss: 0.4798 - val_acc: 0.8937
Epoch 112/500
168s - loss: 0.2291 - acc: 0.9234 - val_loss: 0.4786 - val_acc: 0.8958
Epoch 113/500
167s - loss: 0.2575 - acc: 0.9229 - val_loss: 0.4773 - val_acc: 0.8937
Epoch 114/500
167s - loss: 0.2503 - acc: 0.9255 - val_loss: 0.4797 - val_acc: 0.8937
Epoch 115/500
167s - loss: 0.2492 - acc: 0.9226 - val_loss: 0.4803 - val_acc: 0.8937
Epoch 116/500
167s - loss: 0.2482 - acc: 0.9245 - val_loss: 0.4811 - val_acc: 0.8937
Epoch 117/500
167s - loss: 0.2462 - acc: 0.9266 - val_loss: 0.4831 - val_acc: 0.8937
Epoch 118/500

Epoch 00117: reducing learning rate to 1e-06.
167s - loss: 0.2589 - acc: 0.9211 - val_loss: 0.4785 - val_acc: 0.8937
Epoch 119/500
167s - loss: 0.2308 - acc: 0.9242 - val_loss: 0.4776 - val_acc: 0.8937
Epoch 120/500
167s - loss: 0.2356 - acc: 0.9266 - val_loss: 0.4780 - val_acc: 0.8937
Epoch 121/500
167s - loss: 0.2423 - acc: 0.9266 - val_loss: 0.4781 - val_acc: 0.8937
Epoch 122/500
167s - loss: 0.2266 - acc: 0.9268 - val_loss: 0.4802 - val_acc: 0.8937
Epoch 123/500
167s - loss: 0.2640 - acc: 0.9229 - val_loss: 0.4818 - val_acc: 0.8937
Epoch 124/500
167s - loss: 0.2411 - acc: 0.9239 - val_loss: 0.4801 - val_acc: 0.8926
Epoch 125/500
167s - loss: 0.2550 - acc: 0.9182 - val_loss: 0.4790 - val_acc: 0.8926
Epoch 126/500
167s - loss: 0.2501 - acc: 0.9237 - val_loss: 0.4825 - val_acc: 0.8958
Epoch 127/500
167s - loss: 0.2251 - acc: 0.9258 - val_loss: 0.4808 - val_acc: 0.8937
Epoch 128/500
167s - loss: 0.2525 - acc: 0.9221 - val_loss: 0.4778 - val_acc: 0.8937
Epoch 129/500
167s - loss: 0.2312 - acc: 0.9237 - val_loss: 0.4819 - val_acc: 0.8926
Epoch 130/500
167s - loss: 0.2528 - acc: 0.9258 - val_loss: 0.4795 - val_acc: 0.8937
Epoch 131/500
167s - loss: 0.2623 - acc: 0.9218 - val_loss: 0.4799 - val_acc: 0.8937
Training loss for fold 3 is 0.13717375284747074 with percent 94.97368422307466
Testing loss for fold 3 is 0.4681173677193491 with percent 89.78947370930722
 
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_9 (InputLayer)             (None, 65, 65, 3)     0                                            
____________________________________________________________________________________________________
conv2d_25 (Conv2D)               (None, 63, 63, 16)    448         input_9[0][0]                    
____________________________________________________________________________________________________
batch_normalization_29 (BatchNor (None, 63, 63, 16)    64          conv2d_25[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_29 (LeakyReLU)       (None, 63, 63, 16)    0           batch_normalization_29[0][0]     
____________________________________________________________________________________________________
conv2d_26 (Conv2D)               (None, 62, 62, 16)    1040        leaky_re_lu_29[0][0]             
____________________________________________________________________________________________________
batch_normalization_30 (BatchNor (None, 62, 62, 16)    64          conv2d_26[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_30 (LeakyReLU)       (None, 62, 62, 16)    0           batch_normalization_30[0][0]     
____________________________________________________________________________________________________
max_pooling2d_13 (MaxPooling2D)  (None, 31, 31, 16)    0           leaky_re_lu_30[0][0]             
____________________________________________________________________________________________________
dropout_21 (Dropout)             (None, 31, 31, 16)    0           max_pooling2d_13[0][0]           
____________________________________________________________________________________________________
conv2d_27 (Conv2D)               (None, 29, 29, 32)    4640        dropout_21[0][0]                 
____________________________________________________________________________________________________
batch_normalization_31 (BatchNor (None, 29, 29, 32)    128         conv2d_27[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_31 (LeakyReLU)       (None, 29, 29, 32)    0           batch_normalization_31[0][0]     
____________________________________________________________________________________________________
conv2d_28 (Conv2D)               (None, 28, 28, 32)    4128        leaky_re_lu_31[0][0]             
____________________________________________________________________________________________________
batch_normalization_32 (BatchNor (None, 28, 28, 32)    128         conv2d_28[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_32 (LeakyReLU)       (None, 28, 28, 32)    0           batch_normalization_32[0][0]     
____________________________________________________________________________________________________
max_pooling2d_14 (MaxPooling2D)  (None, 14, 14, 32)    0           leaky_re_lu_32[0][0]             
____________________________________________________________________________________________________
dropout_22 (Dropout)             (None, 14, 14, 32)    0           max_pooling2d_14[0][0]           
____________________________________________________________________________________________________
conv2d_29 (Conv2D)               (None, 12, 12, 64)    18496       dropout_22[0][0]                 
____________________________________________________________________________________________________
batch_normalization_33 (BatchNor (None, 12, 12, 64)    256         conv2d_29[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_33 (LeakyReLU)       (None, 12, 12, 64)    0           batch_normalization_33[0][0]     
____________________________________________________________________________________________________
conv2d_30 (Conv2D)               (None, 11, 11, 64)    16448       leaky_re_lu_33[0][0]             
____________________________________________________________________________________________________
batch_normalization_34 (BatchNor (None, 11, 11, 64)    256         conv2d_30[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_34 (LeakyReLU)       (None, 11, 11, 64)    0           batch_normalization_34[0][0]     
____________________________________________________________________________________________________
max_pooling2d_15 (MaxPooling2D)  (None, 5, 5, 64)      0           leaky_re_lu_34[0][0]             
____________________________________________________________________________________________________
dropout_23 (Dropout)             (None, 5, 5, 64)      0           max_pooling2d_15[0][0]           
____________________________________________________________________________________________________
flatten_5 (Flatten)              (None, 1600)          0           dropout_23[0][0]                 
____________________________________________________________________________________________________
batch_normalization_35 (BatchNor (None, 1600)          6400        flatten_5[0][0]                  
____________________________________________________________________________________________________
input_10 (InputLayer)            (None, 102)           0                                            
____________________________________________________________________________________________________
concatenate_5 (Concatenate)      (None, 1702)          0           batch_normalization_35[0][0]     
                                                                   input_10[0][0]                   
____________________________________________________________________________________________________
dense_17 (Dense)                 (None, 128)           217984      concatenate_5[0][0]              
____________________________________________________________________________________________________
leaky_re_lu_35 (LeakyReLU)       (None, 128)           0           dense_17[0][0]                   
____________________________________________________________________________________________________
dense_18 (Dense)                 (None, 64)            8256        leaky_re_lu_35[0][0]             
____________________________________________________________________________________________________
dropout_24 (Dropout)             (None, 64)            0           dense_18[0][0]                   
____________________________________________________________________________________________________
dense_19 (Dense)                 (None, 32)            2080        dropout_24[0][0]                 
____________________________________________________________________________________________________
dropout_25 (Dropout)             (None, 32)            0           dense_19[0][0]                   
____________________________________________________________________________________________________
dense_20 (Dense)                 (None, 12)            396         dropout_25[0][0]                 
====================================================================================================
Total params: 281,212
Trainable params: 277,564
Non-trainable params: 3,648
____________________________________________________________________________________________________
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
172s - loss: 2.0301 - acc: 0.3179 - val_loss: 2.9152 - val_acc: 0.1579
Epoch 2/500
167s - loss: 1.5827 - acc: 0.4747 - val_loss: 2.1081 - val_acc: 0.4189
Epoch 3/500
167s - loss: 1.4069 - acc: 0.5479 - val_loss: 6.4169 - val_acc: 0.2568
Epoch 4/500
167s - loss: 1.2972 - acc: 0.6034 - val_loss: 2.2569 - val_acc: 0.2905
Epoch 5/500
167s - loss: 1.2515 - acc: 0.6150 - val_loss: 3.2140 - val_acc: 0.3274
Epoch 6/500
167s - loss: 1.1671 - acc: 0.6437 - val_loss: 1.4708 - val_acc: 0.5905
Epoch 7/500
167s - loss: 1.1643 - acc: 0.6439 - val_loss: 1.6781 - val_acc: 0.4632
Epoch 8/500
167s - loss: 1.1432 - acc: 0.6632 - val_loss: 0.9327 - val_acc: 0.7032
Epoch 9/500
167s - loss: 1.0268 - acc: 0.7089 - val_loss: 1.5853 - val_acc: 0.6179
Epoch 10/500
167s - loss: 1.0394 - acc: 0.6984 - val_loss: 1.3594 - val_acc: 0.6326
Epoch 11/500
167s - loss: 1.0228 - acc: 0.7142 - val_loss: 3.0161 - val_acc: 0.4305
Epoch 12/500
167s - loss: 0.9746 - acc: 0.7247 - val_loss: 4.4350 - val_acc: 0.2663
Epoch 13/500
167s - loss: 1.0187 - acc: 0.7192 - val_loss: 1.1186 - val_acc: 0.7253
Epoch 14/500
167s - loss: 0.9401 - acc: 0.7326 - val_loss: 1.3215 - val_acc: 0.6705
Epoch 15/500
167s - loss: 0.9183 - acc: 0.7489 - val_loss: 1.4588 - val_acc: 0.6726
Epoch 16/500
166s - loss: 0.9400 - acc: 0.7403 - val_loss: 0.9727 - val_acc: 0.7242
Epoch 17/500
167s - loss: 0.8669 - acc: 0.7574 - val_loss: 1.1023 - val_acc: 0.7126
Epoch 18/500
167s - loss: 0.8978 - acc: 0.7542 - val_loss: 1.4912 - val_acc: 0.6832
Epoch 19/500
168s - loss: 0.8682 - acc: 0.7708 - val_loss: 1.1065 - val_acc: 0.7316
Epoch 20/500
167s - loss: 0.8886 - acc: 0.7674 - val_loss: 0.9786 - val_acc: 0.7379
Epoch 21/500
167s - loss: 0.8742 - acc: 0.7763 - val_loss: 1.5929 - val_acc: 0.6789
Epoch 22/500
167s - loss: 0.8963 - acc: 0.7608 - val_loss: 1.2047 - val_acc: 0.7442
Epoch 23/500
167s - loss: 0.9398 - acc: 0.7645 - val_loss: 1.0249 - val_acc: 0.7905
Epoch 24/500
167s - loss: 0.9288 - acc: 0.7687 - val_loss: 0.9261 - val_acc: 0.7853
Epoch 25/500
168s - loss: 0.8399 - acc: 0.7887 - val_loss: 1.1261 - val_acc: 0.7421
Epoch 26/500
167s - loss: 0.8855 - acc: 0.7729 - val_loss: 1.1106 - val_acc: 0.7705
Epoch 27/500
167s - loss: 0.8484 - acc: 0.7855 - val_loss: 1.3728 - val_acc: 0.7326
Epoch 28/500
167s - loss: 0.9566 - acc: 0.7655 - val_loss: 2.1445 - val_acc: 0.6200
Epoch 29/500
167s - loss: 0.8610 - acc: 0.7871 - val_loss: 1.6117 - val_acc: 0.7611
Epoch 30/500
167s - loss: 0.8961 - acc: 0.7747 - val_loss: 2.0273 - val_acc: 0.6905
Epoch 31/500
168s - loss: 0.9313 - acc: 0.7982 - val_loss: 1.2135 - val_acc: 0.7705
Epoch 32/500

Epoch 00031: reducing learning rate to 0.010000000149.
168s - loss: 1.0145 - acc: 0.7797 - val_loss: 2.3399 - val_acc: 0.6137
Epoch 33/500
167s - loss: 0.7739 - acc: 0.8074 - val_loss: 0.8031 - val_acc: 0.7958
Epoch 34/500
168s - loss: 0.6017 - acc: 0.8366 - val_loss: 0.6462 - val_acc: 0.8295
Epoch 35/500
168s - loss: 0.5672 - acc: 0.8489 - val_loss: 0.6656 - val_acc: 0.8274
Epoch 36/500
167s - loss: 0.4713 - acc: 0.8632 - val_loss: 0.5715 - val_acc: 0.8368
Epoch 37/500
167s - loss: 0.4843 - acc: 0.8647 - val_loss: 0.7280 - val_acc: 0.8253
Epoch 38/500
167s - loss: 0.4566 - acc: 0.8695 - val_loss: 0.5189 - val_acc: 0.8579
Epoch 39/500
168s - loss: 0.4521 - acc: 0.8726 - val_loss: 0.5703 - val_acc: 0.8589
Epoch 40/500
168s - loss: 0.4453 - acc: 0.8724 - val_loss: 0.5738 - val_acc: 0.8568
Epoch 41/500
168s - loss: 0.3759 - acc: 0.8884 - val_loss: 0.5567 - val_acc: 0.8600
Epoch 42/500
167s - loss: 0.4528 - acc: 0.8766 - val_loss: 0.6521 - val_acc: 0.8516
Epoch 43/500
169s - loss: 0.4160 - acc: 0.8829 - val_loss: 0.6123 - val_acc: 0.8632
Epoch 44/500
168s - loss: 0.4045 - acc: 0.8824 - val_loss: 0.6099 - val_acc: 0.8589
Epoch 45/500
167s - loss: 0.3695 - acc: 0.8900 - val_loss: 0.5029 - val_acc: 0.8663
Epoch 46/500
168s - loss: 0.3749 - acc: 0.8871 - val_loss: 0.5619 - val_acc: 0.8642
Epoch 47/500
167s - loss: 0.3681 - acc: 0.8916 - val_loss: 0.5543 - val_acc: 0.8589
Epoch 48/500
167s - loss: 0.3676 - acc: 0.8950 - val_loss: 0.5305 - val_acc: 0.8695
Epoch 49/500
167s - loss: 0.3802 - acc: 0.8979 - val_loss: 0.5871 - val_acc: 0.8474
Epoch 50/500
167s - loss: 0.3649 - acc: 0.8866 - val_loss: 0.5305 - val_acc: 0.8621
Epoch 51/500
168s - loss: 0.3272 - acc: 0.8979 - val_loss: 0.5579 - val_acc: 0.8695
Epoch 52/500
167s - loss: 0.3553 - acc: 0.8961 - val_loss: 0.5199 - val_acc: 0.8716
Epoch 53/500
167s - loss: 0.3098 - acc: 0.9053 - val_loss: 0.5488 - val_acc: 0.8705
Epoch 54/500
168s - loss: 0.3481 - acc: 0.8971 - val_loss: 0.5358 - val_acc: 0.8632
Epoch 55/500
167s - loss: 0.3297 - acc: 0.9013 - val_loss: 0.5390 - val_acc: 0.8705
Epoch 56/500
167s - loss: 0.3189 - acc: 0.9034 - val_loss: 0.4789 - val_acc: 0.8811
Epoch 57/500
167s - loss: 0.3393 - acc: 0.9005 - val_loss: 0.5260 - val_acc: 0.8674
Epoch 58/500
167s - loss: 0.3212 - acc: 0.9005 - val_loss: 0.5503 - val_acc: 0.8663
Epoch 59/500
167s - loss: 0.3104 - acc: 0.9026 - val_loss: 0.5476 - val_acc: 0.8663
Epoch 60/500
167s - loss: 0.3300 - acc: 0.9034 - val_loss: 0.4897 - val_acc: 0.8705
Epoch 61/500
168s - loss: 0.2932 - acc: 0.9039 - val_loss: 0.5105 - val_acc: 0.8768
Epoch 62/500
166s - loss: 0.3242 - acc: 0.9016 - val_loss: 0.5241 - val_acc: 0.8737
Epoch 63/500
166s - loss: 0.2927 - acc: 0.9111 - val_loss: 0.5331 - val_acc: 0.8779
Epoch 64/500
167s - loss: 0.3015 - acc: 0.9061 - val_loss: 0.5357 - val_acc: 0.8747
Epoch 65/500

Epoch 00064: reducing learning rate to 0.000999999977648.
166s - loss: 0.2827 - acc: 0.9092 - val_loss: 0.4757 - val_acc: 0.8737
Epoch 66/500
167s - loss: 0.2847 - acc: 0.9121 - val_loss: 0.4554 - val_acc: 0.8811
Epoch 67/500
167s - loss: 0.2781 - acc: 0.9147 - val_loss: 0.4502 - val_acc: 0.8789
Epoch 68/500
167s - loss: 0.2949 - acc: 0.9095 - val_loss: 0.4509 - val_acc: 0.8768
Epoch 69/500
167s - loss: 0.2788 - acc: 0.9121 - val_loss: 0.4594 - val_acc: 0.8768
Epoch 70/500
167s - loss: 0.2608 - acc: 0.9150 - val_loss: 0.4641 - val_acc: 0.8747
Epoch 71/500
167s - loss: 0.2586 - acc: 0.9179 - val_loss: 0.4596 - val_acc: 0.8789
Epoch 72/500
167s - loss: 0.2620 - acc: 0.9168 - val_loss: 0.4557 - val_acc: 0.8800
Epoch 73/500

Epoch 00072: reducing learning rate to 9.99999931082e-05.
167s - loss: 0.2851 - acc: 0.9100 - val_loss: 0.4660 - val_acc: 0.8789
Epoch 74/500
167s - loss: 0.2794 - acc: 0.9147 - val_loss: 0.4621 - val_acc: 0.8800
Epoch 75/500
167s - loss: 0.2631 - acc: 0.9137 - val_loss: 0.4608 - val_acc: 0.8779
Epoch 76/500
167s - loss: 0.2784 - acc: 0.9103 - val_loss: 0.4669 - val_acc: 0.8789
Epoch 77/500
167s - loss: 0.2760 - acc: 0.9076 - val_loss: 0.4650 - val_acc: 0.8789
Epoch 78/500
167s - loss: 0.2622 - acc: 0.9113 - val_loss: 0.4698 - val_acc: 0.8768
Epoch 79/500
167s - loss: 0.2685 - acc: 0.9126 - val_loss: 0.4705 - val_acc: 0.8768
Epoch 80/500
167s - loss: 0.2541 - acc: 0.9163 - val_loss: 0.4662 - val_acc: 0.8800
Epoch 81/500

Epoch 00080: reducing learning rate to 9.99999901978e-06.
167s - loss: 0.2668 - acc: 0.9132 - val_loss: 0.4642 - val_acc: 0.8768
Epoch 82/500
167s - loss: 0.2464 - acc: 0.9211 - val_loss: 0.4695 - val_acc: 0.8779
Epoch 83/500
167s - loss: 0.2622 - acc: 0.9166 - val_loss: 0.4654 - val_acc: 0.8800
Epoch 84/500
167s - loss: 0.2642 - acc: 0.9142 - val_loss: 0.4655 - val_acc: 0.8800
Epoch 85/500
167s - loss: 0.2759 - acc: 0.9111 - val_loss: 0.4617 - val_acc: 0.8779
Epoch 86/500
167s - loss: 0.2849 - acc: 0.9139 - val_loss: 0.4687 - val_acc: 0.8789
Epoch 87/500
167s - loss: 0.2637 - acc: 0.9147 - val_loss: 0.4664 - val_acc: 0.8779
Epoch 88/500
167s - loss: 0.2496 - acc: 0.9174 - val_loss: 0.4584 - val_acc: 0.8800
Epoch 89/500

Epoch 00088: reducing learning rate to 1e-06.
167s - loss: 0.2914 - acc: 0.9139 - val_loss: 0.4647 - val_acc: 0.8779
Epoch 90/500
167s - loss: 0.2588 - acc: 0.9137 - val_loss: 0.4652 - val_acc: 0.8789
Epoch 91/500
167s - loss: 0.2603 - acc: 0.9155 - val_loss: 0.4659 - val_acc: 0.8832
Epoch 92/500
168s - loss: 0.2748 - acc: 0.9113 - val_loss: 0.4660 - val_acc: 0.8779
Epoch 93/500
168s - loss: 0.2349 - acc: 0.9211 - val_loss: 0.4670 - val_acc: 0.8779
Epoch 94/500
168s - loss: 0.2736 - acc: 0.9108 - val_loss: 0.4711 - val_acc: 0.8768
Epoch 95/500
168s - loss: 0.2683 - acc: 0.9142 - val_loss: 0.4675 - val_acc: 0.8789
Epoch 96/500
167s - loss: 0.2760 - acc: 0.9147 - val_loss: 0.4687 - val_acc: 0.8768
Epoch 97/500
167s - loss: 0.2650 - acc: 0.9150 - val_loss: 0.4711 - val_acc: 0.8779
Epoch 98/500
167s - loss: 0.2579 - acc: 0.9153 - val_loss: 0.4709 - val_acc: 0.8789
Epoch 99/500
167s - loss: 0.2455 - acc: 0.9179 - val_loss: 0.4691 - val_acc: 0.8779
Epoch 100/500
167s - loss: 0.2619 - acc: 0.9108 - val_loss: 0.4675 - val_acc: 0.8789
Epoch 101/500
167s - loss: 0.2552 - acc: 0.9187 - val_loss: 0.4691 - val_acc: 0.8800
Epoch 102/500
167s - loss: 0.2674 - acc: 0.9158 - val_loss: 0.4681 - val_acc: 0.8800
Epoch 103/500
167s - loss: 0.2476 - acc: 0.9192 - val_loss: 0.4635 - val_acc: 0.8779
Epoch 104/500
167s - loss: 0.2512 - acc: 0.9161 - val_loss: 0.4672 - val_acc: 0.8821
Epoch 105/500
167s - loss: 0.2444 - acc: 0.9189 - val_loss: 0.4623 - val_acc: 0.8800
Epoch 106/500
167s - loss: 0.2481 - acc: 0.9176 - val_loss: 0.4645 - val_acc: 0.8768
Epoch 107/500
167s - loss: 0.2775 - acc: 0.9108 - val_loss: 0.4680 - val_acc: 0.8779
Epoch 108/500
167s - loss: 0.2877 - acc: 0.9166 - val_loss: 0.4683 - val_acc: 0.8789
Epoch 109/500
167s - loss: 0.2869 - acc: 0.9121 - val_loss: 0.4702 - val_acc: 0.8779
Epoch 110/500
167s - loss: 0.2556 - acc: 0.9142 - val_loss: 0.4629 - val_acc: 0.8789
Epoch 111/500
167s - loss: 0.2879 - acc: 0.9111 - val_loss: 0.4635 - val_acc: 0.8779
Epoch 112/500
167s - loss: 0.2926 - acc: 0.9066 - val_loss: 0.4628 - val_acc: 0.8789
Epoch 113/500
167s - loss: 0.2816 - acc: 0.9132 - val_loss: 0.4637 - val_acc: 0.8811
Epoch 114/500
167s - loss: 0.2700 - acc: 0.9116 - val_loss: 0.4707 - val_acc: 0.8789
Epoch 115/500
167s - loss: 0.2601 - acc: 0.9184 - val_loss: 0.4693 - val_acc: 0.8779
Epoch 116/500
167s - loss: 0.2605 - acc: 0.9124 - val_loss: 0.4693 - val_acc: 0.8789
Epoch 117/500
167s - loss: 0.2403 - acc: 0.9200 - val_loss: 0.4677 - val_acc: 0.8811
Epoch 118/500
167s - loss: 0.2527 - acc: 0.9200 - val_loss: 0.4661 - val_acc: 0.8821
Epoch 119/500
167s - loss: 0.2680 - acc: 0.9150 - val_loss: 0.4685 - val_acc: 0.8779
Epoch 120/500
167s - loss: 0.2584 - acc: 0.9126 - val_loss: 0.4660 - val_acc: 0.8789
Epoch 121/500
167s - loss: 0.2697 - acc: 0.9166 - val_loss: 0.4676 - val_acc: 0.8768
Epoch 122/500
167s - loss: 0.2609 - acc: 0.9168 - val_loss: 0.4633 - val_acc: 0.8800
Epoch 123/500
167s - loss: 0.2635 - acc: 0.9116 - val_loss: 0.4658 - val_acc: 0.8789
Epoch 124/500
167s - loss: 0.2827 - acc: 0.9176 - val_loss: 0.4701 - val_acc: 0.8800
Epoch 125/500
167s - loss: 0.2915 - acc: 0.9145 - val_loss: 0.4692 - val_acc: 0.8768
Epoch 126/500
167s - loss: 0.2742 - acc: 0.9121 - val_loss: 0.4572 - val_acc: 0.8789
Epoch 127/500
167s - loss: 0.2654 - acc: 0.9134 - val_loss: 0.4697 - val_acc: 0.8779
Epoch 128/500
167s - loss: 0.2794 - acc: 0.9095 - val_loss: 0.4666 - val_acc: 0.8800
Training loss for fold 4 is 0.1412355206593087 with percent 94.05263156639903
Testing loss for fold 4 is 0.4659004683400455 with percent 88.31578943603917
 
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_11 (InputLayer)            (None, 65, 65, 3)     0                                            
____________________________________________________________________________________________________
conv2d_31 (Conv2D)               (None, 63, 63, 16)    448         input_11[0][0]                   
____________________________________________________________________________________________________
batch_normalization_36 (BatchNor (None, 63, 63, 16)    64          conv2d_31[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_36 (LeakyReLU)       (None, 63, 63, 16)    0           batch_normalization_36[0][0]     
____________________________________________________________________________________________________
conv2d_32 (Conv2D)               (None, 62, 62, 16)    1040        leaky_re_lu_36[0][0]             
____________________________________________________________________________________________________
batch_normalization_37 (BatchNor (None, 62, 62, 16)    64          conv2d_32[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_37 (LeakyReLU)       (None, 62, 62, 16)    0           batch_normalization_37[0][0]     
____________________________________________________________________________________________________
max_pooling2d_16 (MaxPooling2D)  (None, 31, 31, 16)    0           leaky_re_lu_37[0][0]             
____________________________________________________________________________________________________
dropout_26 (Dropout)             (None, 31, 31, 16)    0           max_pooling2d_16[0][0]           
____________________________________________________________________________________________________
conv2d_33 (Conv2D)               (None, 29, 29, 32)    4640        dropout_26[0][0]                 
____________________________________________________________________________________________________
batch_normalization_38 (BatchNor (None, 29, 29, 32)    128         conv2d_33[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_38 (LeakyReLU)       (None, 29, 29, 32)    0           batch_normalization_38[0][0]     
____________________________________________________________________________________________________
conv2d_34 (Conv2D)               (None, 28, 28, 32)    4128        leaky_re_lu_38[0][0]             
____________________________________________________________________________________________________
batch_normalization_39 (BatchNor (None, 28, 28, 32)    128         conv2d_34[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_39 (LeakyReLU)       (None, 28, 28, 32)    0           batch_normalization_39[0][0]     
____________________________________________________________________________________________________
max_pooling2d_17 (MaxPooling2D)  (None, 14, 14, 32)    0           leaky_re_lu_39[0][0]             
____________________________________________________________________________________________________
dropout_27 (Dropout)             (None, 14, 14, 32)    0           max_pooling2d_17[0][0]           
____________________________________________________________________________________________________
conv2d_35 (Conv2D)               (None, 12, 12, 64)    18496       dropout_27[0][0]                 
____________________________________________________________________________________________________
batch_normalization_40 (BatchNor (None, 12, 12, 64)    256         conv2d_35[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_40 (LeakyReLU)       (None, 12, 12, 64)    0           batch_normalization_40[0][0]     
____________________________________________________________________________________________________
conv2d_36 (Conv2D)               (None, 11, 11, 64)    16448       leaky_re_lu_40[0][0]             
____________________________________________________________________________________________________
batch_normalization_41 (BatchNor (None, 11, 11, 64)    256         conv2d_36[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_41 (LeakyReLU)       (None, 11, 11, 64)    0           batch_normalization_41[0][0]     
____________________________________________________________________________________________________
max_pooling2d_18 (MaxPooling2D)  (None, 5, 5, 64)      0           leaky_re_lu_41[0][0]             
____________________________________________________________________________________________________
dropout_28 (Dropout)             (None, 5, 5, 64)      0           max_pooling2d_18[0][0]           
____________________________________________________________________________________________________
flatten_6 (Flatten)              (None, 1600)          0           dropout_28[0][0]                 
____________________________________________________________________________________________________
batch_normalization_42 (BatchNor (None, 1600)          6400        flatten_6[0][0]                  
____________________________________________________________________________________________________
input_12 (InputLayer)            (None, 102)           0                                            
____________________________________________________________________________________________________
concatenate_6 (Concatenate)      (None, 1702)          0           batch_normalization_42[0][0]     
                                                                   input_12[0][0]                   
____________________________________________________________________________________________________
dense_21 (Dense)                 (None, 128)           217984      concatenate_6[0][0]              
____________________________________________________________________________________________________
leaky_re_lu_42 (LeakyReLU)       (None, 128)           0           dense_21[0][0]                   
____________________________________________________________________________________________________
dense_22 (Dense)                 (None, 64)            8256        leaky_re_lu_42[0][0]             
____________________________________________________________________________________________________
dropout_29 (Dropout)             (None, 64)            0           dense_22[0][0]                   
____________________________________________________________________________________________________
dense_23 (Dense)                 (None, 32)            2080        dropout_29[0][0]                 
____________________________________________________________________________________________________
dropout_30 (Dropout)             (None, 32)            0           dense_23[0][0]                   
____________________________________________________________________________________________________
dense_24 (Dense)                 (None, 12)            396         dropout_30[0][0]                 
====================================================================================================
Total params: 281,212
Trainable params: 277,564
Non-trainable params: 3,648
____________________________________________________________________________________________________
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
173s - loss: 2.0168 - acc: 0.3124 - val_loss: 4.6325 - val_acc: 0.1568
Epoch 2/500
169s - loss: 1.5459 - acc: 0.4853 - val_loss: 2.4556 - val_acc: 0.3726
Epoch 3/500
169s - loss: 1.3709 - acc: 0.5605 - val_loss: 2.0664 - val_acc: 0.4326
Epoch 4/500
169s - loss: 1.2623 - acc: 0.6034 - val_loss: 1.3521 - val_acc: 0.6011
Epoch 5/500
169s - loss: 1.1935 - acc: 0.6300 - val_loss: 2.7975 - val_acc: 0.3474
Epoch 6/500
169s - loss: 1.1430 - acc: 0.6621 - val_loss: 1.6793 - val_acc: 0.5337
Epoch 7/500
169s - loss: 1.1120 - acc: 0.6721 - val_loss: 1.3001 - val_acc: 0.5968
Epoch 8/500
169s - loss: 1.0867 - acc: 0.6834 - val_loss: 1.3444 - val_acc: 0.6095
Epoch 9/500
169s - loss: 1.0842 - acc: 0.7000 - val_loss: 5.3564 - val_acc: 0.2232
Epoch 10/500
169s - loss: 1.0032 - acc: 0.7061 - val_loss: 1.6332 - val_acc: 0.6274
Epoch 11/500
168s - loss: 1.0014 - acc: 0.7176 - val_loss: 1.1501 - val_acc: 0.7211
Epoch 12/500
169s - loss: 0.9348 - acc: 0.7361 - val_loss: 2.1781 - val_acc: 0.4274
Epoch 13/500
169s - loss: 0.9318 - acc: 0.7363 - val_loss: 1.9797 - val_acc: 0.5726
Epoch 14/500
170s - loss: 0.8727 - acc: 0.7482 - val_loss: 0.9555 - val_acc: 0.7200
Epoch 15/500
169s - loss: 0.8835 - acc: 0.7629 - val_loss: 1.0247 - val_acc: 0.7179
Epoch 16/500
170s - loss: 0.8783 - acc: 0.7608 - val_loss: 1.9168 - val_acc: 0.6074
Epoch 17/500
170s - loss: 0.8357 - acc: 0.7763 - val_loss: 2.2979 - val_acc: 0.6021
Epoch 18/500
169s - loss: 0.9216 - acc: 0.7613 - val_loss: 2.4956 - val_acc: 0.4368
Epoch 19/500
169s - loss: 0.8913 - acc: 0.7589 - val_loss: 2.4234 - val_acc: 0.5842
Epoch 20/500

Epoch 00019: reducing learning rate to 0.010000000149.
169s - loss: 0.7847 - acc: 0.7850 - val_loss: 2.5473 - val_acc: 0.3958
Epoch 21/500
169s - loss: 0.7400 - acc: 0.7961 - val_loss: 0.7640 - val_acc: 0.8000
Epoch 22/500
169s - loss: 0.5748 - acc: 0.8332 - val_loss: 0.6103 - val_acc: 0.8347
Epoch 23/500
169s - loss: 0.4740 - acc: 0.8529 - val_loss: 0.6166 - val_acc: 0.8295
Epoch 24/500
170s - loss: 0.4584 - acc: 0.8553 - val_loss: 0.6004 - val_acc: 0.8358
Epoch 25/500
170s - loss: 0.4865 - acc: 0.8484 - val_loss: 0.6724 - val_acc: 0.8158
Epoch 26/500
170s - loss: 0.4143 - acc: 0.8600 - val_loss: 0.5590 - val_acc: 0.8421
Epoch 27/500
170s - loss: 0.3857 - acc: 0.8708 - val_loss: 0.5355 - val_acc: 0.8632
Epoch 28/500
168s - loss: 0.3743 - acc: 0.8734 - val_loss: 0.6068 - val_acc: 0.8558
Epoch 29/500
168s - loss: 0.3686 - acc: 0.8811 - val_loss: 0.6186 - val_acc: 0.8505
Epoch 30/500
168s - loss: 0.3892 - acc: 0.8755 - val_loss: 0.6106 - val_acc: 0.8537
Epoch 31/500
168s - loss: 0.3546 - acc: 0.8803 - val_loss: 0.6217 - val_acc: 0.8568
Epoch 32/500
169s - loss: 0.3523 - acc: 0.8811 - val_loss: 0.6226 - val_acc: 0.8526
Epoch 33/500
169s - loss: 0.3602 - acc: 0.8863 - val_loss: 0.6064 - val_acc: 0.8663
Epoch 34/500
169s - loss: 0.3456 - acc: 0.8855 - val_loss: 0.6331 - val_acc: 0.8558
Epoch 35/500
168s - loss: 0.3035 - acc: 0.8934 - val_loss: 0.5738 - val_acc: 0.8663
Epoch 36/500
170s - loss: 0.3354 - acc: 0.8926 - val_loss: 0.6067 - val_acc: 0.8558
Epoch 37/500
169s - loss: 0.3444 - acc: 0.8866 - val_loss: 0.5422 - val_acc: 0.8621
Epoch 38/500
169s - loss: 0.2924 - acc: 0.8995 - val_loss: 0.5535 - val_acc: 0.8684
Epoch 39/500
169s - loss: 0.3109 - acc: 0.8968 - val_loss: 0.6002 - val_acc: 0.8632
Epoch 40/500
169s - loss: 0.3233 - acc: 0.8953 - val_loss: 0.5831 - val_acc: 0.8705
Epoch 41/500
170s - loss: 0.2933 - acc: 0.8992 - val_loss: 0.5480 - val_acc: 0.8695
Epoch 42/500
169s - loss: 0.2877 - acc: 0.9013 - val_loss: 0.5503 - val_acc: 0.8663
Epoch 43/500
169s - loss: 0.3020 - acc: 0.8995 - val_loss: 0.5971 - val_acc: 0.8653
Epoch 44/500
170s - loss: 0.2933 - acc: 0.8963 - val_loss: 0.5402 - val_acc: 0.8663
Epoch 45/500
169s - loss: 0.2905 - acc: 0.9042 - val_loss: 0.6047 - val_acc: 0.8684
Epoch 46/500
169s - loss: 0.2805 - acc: 0.9016 - val_loss: 0.5658 - val_acc: 0.8695
Epoch 47/500
168s - loss: 0.3018 - acc: 0.9039 - val_loss: 0.5517 - val_acc: 0.8716
Epoch 48/500
169s - loss: 0.2675 - acc: 0.9047 - val_loss: 0.5801 - val_acc: 0.8695
Epoch 49/500
170s - loss: 0.2828 - acc: 0.9113 - val_loss: 0.6851 - val_acc: 0.8547
Epoch 50/500
170s - loss: 0.2842 - acc: 0.9058 - val_loss: 0.7069 - val_acc: 0.8421
Epoch 51/500
170s - loss: 0.2760 - acc: 0.9089 - val_loss: 0.5482 - val_acc: 0.8737
Epoch 52/500
170s - loss: 0.2849 - acc: 0.9076 - val_loss: 0.6140 - val_acc: 0.8674
Epoch 53/500
170s - loss: 0.2643 - acc: 0.9111 - val_loss: 0.6354 - val_acc: 0.8632
Epoch 54/500
168s - loss: 0.2554 - acc: 0.9137 - val_loss: 0.5971 - val_acc: 0.8684
Epoch 55/500
168s - loss: 0.2530 - acc: 0.9121 - val_loss: 0.5978 - val_acc: 0.8663
Epoch 56/500
169s - loss: 0.2633 - acc: 0.9163 - val_loss: 0.6432 - val_acc: 0.8716
Epoch 57/500
169s - loss: 0.2666 - acc: 0.9118 - val_loss: 0.5677 - val_acc: 0.8705
Epoch 58/500
168s - loss: 0.2574 - acc: 0.9139 - val_loss: 0.6099 - val_acc: 0.8674
Epoch 59/500
168s - loss: 0.2474 - acc: 0.9203 - val_loss: 0.5763 - val_acc: 0.8747
Epoch 60/500
169s - loss: 0.2412 - acc: 0.9111 - val_loss: 0.6396 - val_acc: 0.8621
Epoch 61/500
169s - loss: 0.2587 - acc: 0.9137 - val_loss: 0.5864 - val_acc: 0.8737
Epoch 62/500
168s - loss: 0.2334 - acc: 0.9184 - val_loss: 0.5698 - val_acc: 0.8663
Epoch 63/500
169s - loss: 0.2307 - acc: 0.9221 - val_loss: 0.5952 - val_acc: 0.8716
Epoch 64/500
168s - loss: 0.2537 - acc: 0.9168 - val_loss: 0.6648 - val_acc: 0.8589
Epoch 65/500
168s - loss: 0.2323 - acc: 0.9226 - val_loss: 0.6322 - val_acc: 0.8842
Epoch 66/500
169s - loss: 0.2388 - acc: 0.9168 - val_loss: 0.7856 - val_acc: 0.8505
Epoch 67/500
169s - loss: 0.2357 - acc: 0.9179 - val_loss: 0.5663 - val_acc: 0.8695
Epoch 68/500
169s - loss: 0.2429 - acc: 0.9168 - val_loss: 0.5681 - val_acc: 0.8716
Epoch 69/500
170s - loss: 0.2048 - acc: 0.9237 - val_loss: 0.6549 - val_acc: 0.8684
Epoch 70/500
170s - loss: 0.2094 - acc: 0.9292 - val_loss: 0.6325 - val_acc: 0.8611
Epoch 71/500
169s - loss: 0.2598 - acc: 0.9171 - val_loss: 0.6074 - val_acc: 0.8674
Epoch 72/500
168s - loss: 0.2259 - acc: 0.9182 - val_loss: 0.5818 - val_acc: 0.8695
Epoch 73/500
169s - loss: 0.2267 - acc: 0.9158 - val_loss: 0.6110 - val_acc: 0.8695
Epoch 74/500

Epoch 00073: reducing learning rate to 0.000999999977648.
169s - loss: 0.2111 - acc: 0.9263 - val_loss: 0.5613 - val_acc: 0.8726
Epoch 75/500
168s - loss: 0.2057 - acc: 0.9300 - val_loss: 0.5705 - val_acc: 0.8705
Epoch 76/500
168s - loss: 0.1930 - acc: 0.9334 - val_loss: 0.5952 - val_acc: 0.8705
Epoch 77/500
169s - loss: 0.1992 - acc: 0.9308 - val_loss: 0.5952 - val_acc: 0.8716
Epoch 78/500
169s - loss: 0.2144 - acc: 0.9337 - val_loss: 0.5827 - val_acc: 0.8737
Epoch 79/500
168s - loss: 0.1838 - acc: 0.9347 - val_loss: 0.5904 - val_acc: 0.8695
Epoch 80/500
168s - loss: 0.2056 - acc: 0.9271 - val_loss: 0.6117 - val_acc: 0.8716
Epoch 81/500
169s - loss: 0.2140 - acc: 0.9295 - val_loss: 0.6007 - val_acc: 0.8695
Epoch 82/500

Epoch 00081: reducing learning rate to 9.99999931082e-05.
168s - loss: 0.1929 - acc: 0.9284 - val_loss: 0.6034 - val_acc: 0.8726
Epoch 83/500
169s - loss: 0.1970 - acc: 0.9321 - val_loss: 0.5925 - val_acc: 0.8716
Epoch 84/500
169s - loss: 0.1896 - acc: 0.9345 - val_loss: 0.5957 - val_acc: 0.8705
Epoch 85/500
169s - loss: 0.2119 - acc: 0.9329 - val_loss: 0.5907 - val_acc: 0.8716
Epoch 86/500
168s - loss: 0.1875 - acc: 0.9350 - val_loss: 0.5978 - val_acc: 0.8737
Epoch 87/500
168s - loss: 0.1874 - acc: 0.9400 - val_loss: 0.6022 - val_acc: 0.8737
Epoch 88/500
168s - loss: 0.2058 - acc: 0.9334 - val_loss: 0.6018 - val_acc: 0.8747
Training loss for fold 5 is 0.109516455143396 with percent 95.92105263157895
Testing loss for fold 5 is 0.6321727699041366 with percent 88.42105263157895
 
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_13 (InputLayer)            (None, 65, 65, 3)     0                                            
____________________________________________________________________________________________________
conv2d_37 (Conv2D)               (None, 63, 63, 16)    448         input_13[0][0]                   
____________________________________________________________________________________________________
batch_normalization_43 (BatchNor (None, 63, 63, 16)    64          conv2d_37[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_43 (LeakyReLU)       (None, 63, 63, 16)    0           batch_normalization_43[0][0]     
____________________________________________________________________________________________________
conv2d_38 (Conv2D)               (None, 62, 62, 16)    1040        leaky_re_lu_43[0][0]             
____________________________________________________________________________________________________
batch_normalization_44 (BatchNor (None, 62, 62, 16)    64          conv2d_38[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_44 (LeakyReLU)       (None, 62, 62, 16)    0           batch_normalization_44[0][0]     
____________________________________________________________________________________________________
max_pooling2d_19 (MaxPooling2D)  (None, 31, 31, 16)    0           leaky_re_lu_44[0][0]             
____________________________________________________________________________________________________
dropout_31 (Dropout)             (None, 31, 31, 16)    0           max_pooling2d_19[0][0]           
____________________________________________________________________________________________________
conv2d_39 (Conv2D)               (None, 29, 29, 32)    4640        dropout_31[0][0]                 
____________________________________________________________________________________________________
batch_normalization_45 (BatchNor (None, 29, 29, 32)    128         conv2d_39[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_45 (LeakyReLU)       (None, 29, 29, 32)    0           batch_normalization_45[0][0]     
____________________________________________________________________________________________________
conv2d_40 (Conv2D)               (None, 28, 28, 32)    4128        leaky_re_lu_45[0][0]             
____________________________________________________________________________________________________
batch_normalization_46 (BatchNor (None, 28, 28, 32)    128         conv2d_40[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_46 (LeakyReLU)       (None, 28, 28, 32)    0           batch_normalization_46[0][0]     
____________________________________________________________________________________________________
max_pooling2d_20 (MaxPooling2D)  (None, 14, 14, 32)    0           leaky_re_lu_46[0][0]             
____________________________________________________________________________________________________
dropout_32 (Dropout)             (None, 14, 14, 32)    0           max_pooling2d_20[0][0]           
____________________________________________________________________________________________________
conv2d_41 (Conv2D)               (None, 12, 12, 64)    18496       dropout_32[0][0]                 
____________________________________________________________________________________________________
batch_normalization_47 (BatchNor (None, 12, 12, 64)    256         conv2d_41[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_47 (LeakyReLU)       (None, 12, 12, 64)    0           batch_normalization_47[0][0]     
____________________________________________________________________________________________________
conv2d_42 (Conv2D)               (None, 11, 11, 64)    16448       leaky_re_lu_47[0][0]             
____________________________________________________________________________________________________
batch_normalization_48 (BatchNor (None, 11, 11, 64)    256         conv2d_42[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_48 (LeakyReLU)       (None, 11, 11, 64)    0           batch_normalization_48[0][0]     
____________________________________________________________________________________________________
max_pooling2d_21 (MaxPooling2D)  (None, 5, 5, 64)      0           leaky_re_lu_48[0][0]             
____________________________________________________________________________________________________
dropout_33 (Dropout)             (None, 5, 5, 64)      0           max_pooling2d_21[0][0]           
____________________________________________________________________________________________________
flatten_7 (Flatten)              (None, 1600)          0           dropout_33[0][0]                 
____________________________________________________________________________________________________
batch_normalization_49 (BatchNor (None, 1600)          6400        flatten_7[0][0]                  
____________________________________________________________________________________________________
input_14 (InputLayer)            (None, 102)           0                                            
____________________________________________________________________________________________________
concatenate_7 (Concatenate)      (None, 1702)          0           batch_normalization_49[0][0]     
                                                                   input_14[0][0]                   
____________________________________________________________________________________________________
dense_25 (Dense)                 (None, 128)           217984      concatenate_7[0][0]              
____________________________________________________________________________________________________
leaky_re_lu_49 (LeakyReLU)       (None, 128)           0           dense_25[0][0]                   
____________________________________________________________________________________________________
dense_26 (Dense)                 (None, 64)            8256        leaky_re_lu_49[0][0]             
____________________________________________________________________________________________________
dropout_34 (Dropout)             (None, 64)            0           dense_26[0][0]                   
____________________________________________________________________________________________________
dense_27 (Dense)                 (None, 32)            2080        dropout_34[0][0]                 
____________________________________________________________________________________________________
dropout_35 (Dropout)             (None, 32)            0           dense_27[0][0]                   
____________________________________________________________________________________________________
dense_28 (Dense)                 (None, 12)            396         dropout_35[0][0]                 
====================================================================================================
Total params: 281,212
Trainable params: 277,564
Non-trainable params: 3,648
____________________________________________________________________________________________________
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
174s - loss: 2.0194 - acc: 0.3192 - val_loss: 2.0936 - val_acc: 0.3137
Epoch 2/500
169s - loss: 1.5524 - acc: 0.4916 - val_loss: 1.4525 - val_acc: 0.5200
Epoch 3/500
170s - loss: 1.4042 - acc: 0.5447 - val_loss: 1.0561 - val_acc: 0.6642
Epoch 4/500
169s - loss: 1.2688 - acc: 0.5911 - val_loss: 2.0608 - val_acc: 0.4516
Epoch 5/500
169s - loss: 1.2225 - acc: 0.6368 - val_loss: 1.0932 - val_acc: 0.6832
Epoch 6/500
169s - loss: 1.1831 - acc: 0.6445 - val_loss: 1.5251 - val_acc: 0.5895
Epoch 7/500
169s - loss: 1.1129 - acc: 0.6668 - val_loss: 1.0155 - val_acc: 0.7063
Epoch 8/500
169s - loss: 1.0771 - acc: 0.6792 - val_loss: 1.1466 - val_acc: 0.6463
Epoch 9/500
168s - loss: 1.0711 - acc: 0.6866 - val_loss: 5.2292 - val_acc: 0.1779
Epoch 10/500
168s - loss: 1.0579 - acc: 0.6905 - val_loss: 1.2076 - val_acc: 0.7295
Epoch 11/500
169s - loss: 1.0450 - acc: 0.7013 - val_loss: 3.2437 - val_acc: 0.4463
Epoch 12/500
169s - loss: 1.0328 - acc: 0.7139 - val_loss: 2.2782 - val_acc: 0.4137
Epoch 13/500
170s - loss: 0.9987 - acc: 0.7237 - val_loss: 1.1071 - val_acc: 0.7158
Epoch 14/500
169s - loss: 0.9825 - acc: 0.7284 - val_loss: 1.1958 - val_acc: 0.7021
Epoch 15/500
169s - loss: 0.9556 - acc: 0.7326 - val_loss: 1.5529 - val_acc: 0.6379
Epoch 16/500
169s - loss: 0.8796 - acc: 0.7561 - val_loss: 1.7508 - val_acc: 0.6168
Epoch 17/500
168s - loss: 0.8874 - acc: 0.7711 - val_loss: 1.7917 - val_acc: 0.6463
Epoch 18/500
169s - loss: 0.9150 - acc: 0.7647 - val_loss: 1.9923 - val_acc: 0.5632
Epoch 19/500

Epoch 00018: reducing learning rate to 0.010000000149.
169s - loss: 0.9351 - acc: 0.7621 - val_loss: 3.4855 - val_acc: 0.4432
Epoch 20/500
168s - loss: 0.7840 - acc: 0.7908 - val_loss: 0.6737 - val_acc: 0.8042
Epoch 21/500
169s - loss: 0.6001 - acc: 0.8329 - val_loss: 0.8101 - val_acc: 0.7779
Epoch 22/500
169s - loss: 0.5725 - acc: 0.8400 - val_loss: 0.5943 - val_acc: 0.8253
Epoch 23/500
169s - loss: 0.5278 - acc: 0.8521 - val_loss: 0.5915 - val_acc: 0.8368
Epoch 24/500
169s - loss: 0.4738 - acc: 0.8574 - val_loss: 0.6099 - val_acc: 0.8221
Epoch 25/500
169s - loss: 0.4883 - acc: 0.8545 - val_loss: 0.6481 - val_acc: 0.8326
Epoch 26/500
168s - loss: 0.4516 - acc: 0.8576 - val_loss: 0.5915 - val_acc: 0.8263
Epoch 27/500
168s - loss: 0.4255 - acc: 0.8705 - val_loss: 0.5801 - val_acc: 0.8189
Epoch 28/500
168s - loss: 0.4196 - acc: 0.8703 - val_loss: 0.5169 - val_acc: 0.8474
Epoch 29/500
169s - loss: 0.4040 - acc: 0.8724 - val_loss: 0.5785 - val_acc: 0.8495
Epoch 30/500
169s - loss: 0.3836 - acc: 0.8782 - val_loss: 0.5794 - val_acc: 0.8358
Epoch 31/500
168s - loss: 0.4193 - acc: 0.8753 - val_loss: 0.5784 - val_acc: 0.8421
Epoch 32/500
169s - loss: 0.4147 - acc: 0.8776 - val_loss: 0.5534 - val_acc: 0.8516
Epoch 33/500
169s - loss: 0.3655 - acc: 0.8826 - val_loss: 0.6091 - val_acc: 0.8474
Epoch 34/500
168s - loss: 0.3748 - acc: 0.8834 - val_loss: 0.5910 - val_acc: 0.8389
Epoch 35/500
168s - loss: 0.3585 - acc: 0.8839 - val_loss: 0.5486 - val_acc: 0.8537
Epoch 36/500
169s - loss: 0.3432 - acc: 0.8903 - val_loss: 0.5792 - val_acc: 0.8442
Epoch 37/500
169s - loss: 0.3664 - acc: 0.8876 - val_loss: 0.5289 - val_acc: 0.8579
Epoch 38/500
169s - loss: 0.3551 - acc: 0.8950 - val_loss: 0.5360 - val_acc: 0.8621
Epoch 39/500
169s - loss: 0.3498 - acc: 0.8866 - val_loss: 0.5622 - val_acc: 0.8484
Epoch 40/500
169s - loss: 0.3260 - acc: 0.8958 - val_loss: 0.6177 - val_acc: 0.8516
Epoch 41/500
169s - loss: 0.3293 - acc: 0.8908 - val_loss: 0.5319 - val_acc: 0.8537
Epoch 42/500
169s - loss: 0.3052 - acc: 0.8992 - val_loss: 0.5380 - val_acc: 0.8653
Epoch 43/500
169s - loss: 0.3054 - acc: 0.8966 - val_loss: 0.6550 - val_acc: 0.8474
Epoch 44/500
169s - loss: 0.3035 - acc: 0.9000 - val_loss: 0.6200 - val_acc: 0.8526
Epoch 45/500
169s - loss: 0.3142 - acc: 0.9053 - val_loss: 0.6396 - val_acc: 0.8421
Epoch 46/500
170s - loss: 0.2806 - acc: 0.9079 - val_loss: 0.6029 - val_acc: 0.8453
Epoch 47/500
169s - loss: 0.3112 - acc: 0.9011 - val_loss: 0.5737 - val_acc: 0.8611
Epoch 48/500
169s - loss: 0.2970 - acc: 0.9011 - val_loss: 0.7196 - val_acc: 0.8358
Epoch 49/500
169s - loss: 0.2966 - acc: 0.9050 - val_loss: 0.5454 - val_acc: 0.8653
Epoch 50/500
169s - loss: 0.2844 - acc: 0.9026 - val_loss: 0.6171 - val_acc: 0.8495
Epoch 51/500

Epoch 00050: reducing learning rate to 0.000999999977648.
168s - loss: 0.2722 - acc: 0.9111 - val_loss: 0.6491 - val_acc: 0.8495
Epoch 52/500
168s - loss: 0.2553 - acc: 0.9095 - val_loss: 0.5956 - val_acc: 0.8558
Epoch 53/500
168s - loss: 0.2637 - acc: 0.9097 - val_loss: 0.5845 - val_acc: 0.8621
Epoch 54/500
168s - loss: 0.2790 - acc: 0.9139 - val_loss: 0.5844 - val_acc: 0.8611
Epoch 55/500
168s - loss: 0.3022 - acc: 0.9092 - val_loss: 0.5592 - val_acc: 0.8653
Epoch 56/500
168s - loss: 0.2726 - acc: 0.9147 - val_loss: 0.5693 - val_acc: 0.8589
Epoch 57/500
169s - loss: 0.2904 - acc: 0.9100 - val_loss: 0.5702 - val_acc: 0.8611
Epoch 58/500
169s - loss: 0.2711 - acc: 0.9124 - val_loss: 0.5685 - val_acc: 0.8600
Epoch 59/500

Epoch 00058: reducing learning rate to 9.99999931082e-05.
169s - loss: 0.2678 - acc: 0.9182 - val_loss: 0.5657 - val_acc: 0.8600
Epoch 60/500
168s - loss: 0.2442 - acc: 0.9145 - val_loss: 0.5686 - val_acc: 0.8589
Epoch 61/500
168s - loss: 0.2588 - acc: 0.9139 - val_loss: 0.5681 - val_acc: 0.8600
Epoch 62/500
168s - loss: 0.2438 - acc: 0.9163 - val_loss: 0.5609 - val_acc: 0.8632
Epoch 63/500
168s - loss: 0.2628 - acc: 0.9121 - val_loss: 0.5666 - val_acc: 0.8579
Epoch 64/500
168s - loss: 0.2712 - acc: 0.9116 - val_loss: 0.5665 - val_acc: 0.8589
Epoch 65/500
168s - loss: 0.3031 - acc: 0.9116 - val_loss: 0.5650 - val_acc: 0.8589
Epoch 66/500
168s - loss: 0.2636 - acc: 0.9132 - val_loss: 0.5702 - val_acc: 0.8589
Epoch 67/500

Epoch 00066: reducing learning rate to 9.99999901978e-06.
168s - loss: 0.2524 - acc: 0.9153 - val_loss: 0.5613 - val_acc: 0.8621
Epoch 68/500
169s - loss: 0.2437 - acc: 0.9192 - val_loss: 0.5638 - val_acc: 0.8621
Epoch 69/500
169s - loss: 0.2613 - acc: 0.9121 - val_loss: 0.5637 - val_acc: 0.8611
Epoch 70/500
169s - loss: 0.2440 - acc: 0.9134 - val_loss: 0.5622 - val_acc: 0.8589
Epoch 71/500
168s - loss: 0.2689 - acc: 0.9113 - val_loss: 0.5665 - val_acc: 0.8589
Epoch 72/500
169s - loss: 0.2495 - acc: 0.9153 - val_loss: 0.5662 - val_acc: 0.8568
Epoch 73/500
168s - loss: 0.2800 - acc: 0.9113 - val_loss: 0.5682 - val_acc: 0.8568
Epoch 74/500
169s - loss: 0.2581 - acc: 0.9145 - val_loss: 0.5692 - val_acc: 0.8600
Epoch 75/500

Epoch 00074: reducing learning rate to 1e-06.
169s - loss: 0.2531 - acc: 0.9174 - val_loss: 0.5667 - val_acc: 0.8632
Epoch 76/500
169s - loss: 0.2664 - acc: 0.9116 - val_loss: 0.5669 - val_acc: 0.8642
Epoch 77/500
169s - loss: 0.2426 - acc: 0.9166 - val_loss: 0.5652 - val_acc: 0.8621
Epoch 78/500
168s - loss: 0.2452 - acc: 0.9150 - val_loss: 0.5666 - val_acc: 0.8600
Epoch 79/500
169s - loss: 0.2452 - acc: 0.9145 - val_loss: 0.5630 - val_acc: 0.8611
Epoch 80/500
169s - loss: 0.3004 - acc: 0.9097 - val_loss: 0.5664 - val_acc: 0.8600
Epoch 81/500
168s - loss: 0.2323 - acc: 0.9163 - val_loss: 0.5677 - val_acc: 0.8621
Epoch 82/500
169s - loss: 0.2408 - acc: 0.9208 - val_loss: 0.5654 - val_acc: 0.8611
Epoch 83/500
169s - loss: 0.2578 - acc: 0.9150 - val_loss: 0.5673 - val_acc: 0.8589
Epoch 84/500
168s - loss: 0.2347 - acc: 0.9174 - val_loss: 0.5689 - val_acc: 0.8600
Epoch 85/500
169s - loss: 0.2548 - acc: 0.9139 - val_loss: 0.5672 - val_acc: 0.8600
Epoch 86/500
169s - loss: 0.2419 - acc: 0.9161 - val_loss: 0.5678 - val_acc: 0.8589
Epoch 87/500
168s - loss: 0.2663 - acc: 0.9084 - val_loss: 0.5645 - val_acc: 0.8589
Epoch 88/500
169s - loss: 0.2434 - acc: 0.9184 - val_loss: 0.5628 - val_acc: 0.8621
Epoch 89/500
169s - loss: 0.2571 - acc: 0.9142 - val_loss: 0.5650 - val_acc: 0.8589
Training loss for fold 6 is 0.13320774535207372 with percent 94.13157895991677
Testing loss for fold 6 is 0.5591633179940675 with percent 86.52631581457038
 
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_15 (InputLayer)            (None, 65, 65, 3)     0                                            
____________________________________________________________________________________________________
conv2d_43 (Conv2D)               (None, 63, 63, 16)    448         input_15[0][0]                   
____________________________________________________________________________________________________
batch_normalization_50 (BatchNor (None, 63, 63, 16)    64          conv2d_43[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_50 (LeakyReLU)       (None, 63, 63, 16)    0           batch_normalization_50[0][0]     
____________________________________________________________________________________________________
conv2d_44 (Conv2D)               (None, 62, 62, 16)    1040        leaky_re_lu_50[0][0]             
____________________________________________________________________________________________________
batch_normalization_51 (BatchNor (None, 62, 62, 16)    64          conv2d_44[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_51 (LeakyReLU)       (None, 62, 62, 16)    0           batch_normalization_51[0][0]     
____________________________________________________________________________________________________
max_pooling2d_22 (MaxPooling2D)  (None, 31, 31, 16)    0           leaky_re_lu_51[0][0]             
____________________________________________________________________________________________________
dropout_36 (Dropout)             (None, 31, 31, 16)    0           max_pooling2d_22[0][0]           
____________________________________________________________________________________________________
conv2d_45 (Conv2D)               (None, 29, 29, 32)    4640        dropout_36[0][0]                 
____________________________________________________________________________________________________
batch_normalization_52 (BatchNor (None, 29, 29, 32)    128         conv2d_45[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_52 (LeakyReLU)       (None, 29, 29, 32)    0           batch_normalization_52[0][0]     
____________________________________________________________________________________________________
conv2d_46 (Conv2D)               (None, 28, 28, 32)    4128        leaky_re_lu_52[0][0]             
____________________________________________________________________________________________________
batch_normalization_53 (BatchNor (None, 28, 28, 32)    128         conv2d_46[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_53 (LeakyReLU)       (None, 28, 28, 32)    0           batch_normalization_53[0][0]     
____________________________________________________________________________________________________
max_pooling2d_23 (MaxPooling2D)  (None, 14, 14, 32)    0           leaky_re_lu_53[0][0]             
____________________________________________________________________________________________________
dropout_37 (Dropout)             (None, 14, 14, 32)    0           max_pooling2d_23[0][0]           
____________________________________________________________________________________________________
conv2d_47 (Conv2D)               (None, 12, 12, 64)    18496       dropout_37[0][0]                 
____________________________________________________________________________________________________
batch_normalization_54 (BatchNor (None, 12, 12, 64)    256         conv2d_47[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_54 (LeakyReLU)       (None, 12, 12, 64)    0           batch_normalization_54[0][0]     
____________________________________________________________________________________________________
conv2d_48 (Conv2D)               (None, 11, 11, 64)    16448       leaky_re_lu_54[0][0]             
____________________________________________________________________________________________________
batch_normalization_55 (BatchNor (None, 11, 11, 64)    256         conv2d_48[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_55 (LeakyReLU)       (None, 11, 11, 64)    0           batch_normalization_55[0][0]     
____________________________________________________________________________________________________
max_pooling2d_24 (MaxPooling2D)  (None, 5, 5, 64)      0           leaky_re_lu_55[0][0]             
____________________________________________________________________________________________________
dropout_38 (Dropout)             (None, 5, 5, 64)      0           max_pooling2d_24[0][0]           
____________________________________________________________________________________________________
flatten_8 (Flatten)              (None, 1600)          0           dropout_38[0][0]                 
____________________________________________________________________________________________________
batch_normalization_56 (BatchNor (None, 1600)          6400        flatten_8[0][0]                  
____________________________________________________________________________________________________
input_16 (InputLayer)            (None, 102)           0                                            
____________________________________________________________________________________________________
concatenate_8 (Concatenate)      (None, 1702)          0           batch_normalization_56[0][0]     
                                                                   input_16[0][0]                   
____________________________________________________________________________________________________
dense_29 (Dense)                 (None, 128)           217984      concatenate_8[0][0]              
____________________________________________________________________________________________________
leaky_re_lu_56 (LeakyReLU)       (None, 128)           0           dense_29[0][0]                   
____________________________________________________________________________________________________
dense_30 (Dense)                 (None, 64)            8256        leaky_re_lu_56[0][0]             
____________________________________________________________________________________________________
dropout_39 (Dropout)             (None, 64)            0           dense_30[0][0]                   
____________________________________________________________________________________________________
dense_31 (Dense)                 (None, 32)            2080        dropout_39[0][0]                 
____________________________________________________________________________________________________
dropout_40 (Dropout)             (None, 32)            0           dense_31[0][0]                   
____________________________________________________________________________________________________
dense_32 (Dense)                 (None, 12)            396         dropout_40[0][0]                 
====================================================================================================
Total params: 281,212
Trainable params: 277,564
Non-trainable params: 3,648
____________________________________________________________________________________________________
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
175s - loss: 1.8950 - acc: 0.3613 - val_loss: 5.1843 - val_acc: 0.1379
Epoch 2/500
170s - loss: 1.5223 - acc: 0.4953 - val_loss: 3.1248 - val_acc: 0.3663
Epoch 3/500
170s - loss: 1.3844 - acc: 0.5558 - val_loss: 3.1722 - val_acc: 0.3474
Epoch 4/500
170s - loss: 1.3056 - acc: 0.5884 - val_loss: 3.7681 - val_acc: 0.2695
Epoch 5/500
170s - loss: 1.2232 - acc: 0.6187 - val_loss: 2.2125 - val_acc: 0.3674
Epoch 6/500
171s - loss: 1.1407 - acc: 0.6724 - val_loss: 1.1157 - val_acc: 0.6832
Epoch 7/500
170s - loss: 1.1092 - acc: 0.6755 - val_loss: 2.9640 - val_acc: 0.4168
Epoch 8/500
170s - loss: 1.0426 - acc: 0.6932 - val_loss: 2.0359 - val_acc: 0.5042
Epoch 9/500
170s - loss: 1.0117 - acc: 0.7063 - val_loss: 1.1355 - val_acc: 0.7063
Epoch 10/500
170s - loss: 0.9746 - acc: 0.7247 - val_loss: 1.8404 - val_acc: 0.6358
Epoch 11/500
170s - loss: 0.9402 - acc: 0.7374 - val_loss: 1.1787 - val_acc: 0.7232
Epoch 12/500
170s - loss: 0.9228 - acc: 0.7392 - val_loss: 1.7855 - val_acc: 0.5589
Epoch 13/500
170s - loss: 0.9046 - acc: 0.7482 - val_loss: 1.0909 - val_acc: 0.7242
Epoch 14/500
170s - loss: 0.9191 - acc: 0.7482 - val_loss: 1.1884 - val_acc: 0.7032
Epoch 15/500
170s - loss: 0.9465 - acc: 0.7455 - val_loss: 1.5449 - val_acc: 0.6737
Epoch 16/500
170s - loss: 0.8797 - acc: 0.7724 - val_loss: 5.6699 - val_acc: 0.3053
Epoch 17/500
170s - loss: 0.8883 - acc: 0.7663 - val_loss: 1.2019 - val_acc: 0.7316
Epoch 18/500
170s - loss: 0.9055 - acc: 0.7824 - val_loss: 1.3778 - val_acc: 0.6863
Epoch 19/500
170s - loss: 0.8967 - acc: 0.7824 - val_loss: 1.3014 - val_acc: 0.7705
Epoch 20/500
170s - loss: 0.8879 - acc: 0.7787 - val_loss: 1.0756 - val_acc: 0.7600
Epoch 21/500
170s - loss: 0.8487 - acc: 0.7900 - val_loss: 0.9892 - val_acc: 0.7632
Epoch 22/500
170s - loss: 0.8173 - acc: 0.8037 - val_loss: 0.9822 - val_acc: 0.7968
Epoch 23/500
170s - loss: 0.8971 - acc: 0.7874 - val_loss: 5.1343 - val_acc: 0.4368
Epoch 24/500
170s - loss: 0.8535 - acc: 0.8066 - val_loss: 1.2174 - val_acc: 0.7516
Epoch 25/500
170s - loss: 0.9703 - acc: 0.8003 - val_loss: 6.3142 - val_acc: 0.3495
Epoch 26/500
170s - loss: 0.9398 - acc: 0.7837 - val_loss: 1.7395 - val_acc: 0.6653
Epoch 27/500
170s - loss: 0.8801 - acc: 0.8055 - val_loss: 1.0773 - val_acc: 0.8263
Epoch 28/500
170s - loss: 0.9002 - acc: 0.8042 - val_loss: 4.7389 - val_acc: 0.4642
Epoch 29/500
170s - loss: 0.9654 - acc: 0.7958 - val_loss: 1.6283 - val_acc: 0.6632
Epoch 30/500
170s - loss: 0.9068 - acc: 0.8026 - val_loss: 1.4035 - val_acc: 0.6926
Epoch 31/500
170s - loss: 0.8538 - acc: 0.8000 - val_loss: 2.4792 - val_acc: 0.5705
Epoch 32/500
170s - loss: 0.8932 - acc: 0.7955 - val_loss: 1.1836 - val_acc: 0.7505
Epoch 33/500
170s - loss: 0.9167 - acc: 0.7971 - val_loss: 2.4539 - val_acc: 0.6547
Epoch 34/500
170s - loss: 0.9958 - acc: 0.7997 - val_loss: 1.5335 - val_acc: 0.7305
Epoch 35/500
170s - loss: 0.9958 - acc: 0.7995 - val_loss: 1.4827 - val_acc: 0.7611
Epoch 36/500

Epoch 00035: reducing learning rate to 0.010000000149.
171s - loss: 1.1297 - acc: 0.7695 - val_loss: 1.5502 - val_acc: 0.7137
Epoch 37/500
170s - loss: 1.0547 - acc: 0.7974 - val_loss: 1.2151 - val_acc: 0.7821
Epoch 38/500
170s - loss: 0.7197 - acc: 0.8321 - val_loss: 0.8812 - val_acc: 0.8316
Epoch 39/500
170s - loss: 0.6972 - acc: 0.8411 - val_loss: 0.8427 - val_acc: 0.8284
Epoch 40/500
170s - loss: 0.6669 - acc: 0.8450 - val_loss: 0.8690 - val_acc: 0.8379
Epoch 41/500
170s - loss: 0.6094 - acc: 0.8532 - val_loss: 0.7945 - val_acc: 0.8453
Epoch 42/500
170s - loss: 0.5837 - acc: 0.8555 - val_loss: 0.7994 - val_acc: 0.8474
Epoch 43/500
171s - loss: 0.5731 - acc: 0.8629 - val_loss: 0.7751 - val_acc: 0.8526
Epoch 44/500
170s - loss: 0.5764 - acc: 0.8624 - val_loss: 0.7667 - val_acc: 0.8495
Epoch 45/500
171s - loss: 0.5205 - acc: 0.8663 - val_loss: 0.8109 - val_acc: 0.8411
Epoch 46/500
171s - loss: 0.5012 - acc: 0.8676 - val_loss: 0.7066 - val_acc: 0.8568
Epoch 47/500
171s - loss: 0.5046 - acc: 0.8718 - val_loss: 0.6990 - val_acc: 0.8611
Epoch 48/500
171s - loss: 0.4749 - acc: 0.8800 - val_loss: 0.7104 - val_acc: 0.8642
Epoch 49/500
170s - loss: 0.5190 - acc: 0.8742 - val_loss: 0.7052 - val_acc: 0.8611
Epoch 50/500
170s - loss: 0.5088 - acc: 0.8797 - val_loss: 0.7703 - val_acc: 0.8516
Epoch 51/500
171s - loss: 0.4995 - acc: 0.8747 - val_loss: 0.7224 - val_acc: 0.8632
Epoch 52/500
171s - loss: 0.4691 - acc: 0.8797 - val_loss: 0.6956 - val_acc: 0.8621
Epoch 53/500
172s - loss: 0.4707 - acc: 0.8761 - val_loss: 0.6679 - val_acc: 0.8663
Epoch 54/500
171s - loss: 0.4361 - acc: 0.8847 - val_loss: 0.7084 - val_acc: 0.8611
Epoch 55/500
171s - loss: 0.4458 - acc: 0.8855 - val_loss: 0.7002 - val_acc: 0.8558
Epoch 56/500
170s - loss: 0.4107 - acc: 0.8900 - val_loss: 0.6756 - val_acc: 0.8695
Epoch 57/500
170s - loss: 0.4189 - acc: 0.8845 - val_loss: 0.7145 - val_acc: 0.8642
Epoch 58/500
170s - loss: 0.4307 - acc: 0.8879 - val_loss: 0.6945 - val_acc: 0.8674
Epoch 59/500
170s - loss: 0.4231 - acc: 0.8863 - val_loss: 0.7177 - val_acc: 0.8611
Epoch 60/500
170s - loss: 0.4224 - acc: 0.8903 - val_loss: 0.6676 - val_acc: 0.8705
Epoch 61/500
170s - loss: 0.4363 - acc: 0.8868 - val_loss: 0.6371 - val_acc: 0.8747
Epoch 62/500
170s - loss: 0.3861 - acc: 0.8903 - val_loss: 0.6574 - val_acc: 0.8716
Epoch 63/500
171s - loss: 0.3997 - acc: 0.8916 - val_loss: 0.6818 - val_acc: 0.8663
Epoch 64/500
170s - loss: 0.3821 - acc: 0.8979 - val_loss: 0.6558 - val_acc: 0.8705
Epoch 65/500
171s - loss: 0.3899 - acc: 0.8937 - val_loss: 0.6576 - val_acc: 0.8632
Epoch 66/500
170s - loss: 0.3863 - acc: 0.8900 - val_loss: 0.6459 - val_acc: 0.8632
Epoch 67/500
170s - loss: 0.3511 - acc: 0.9011 - val_loss: 0.6153 - val_acc: 0.8684
Epoch 68/500
170s - loss: 0.3902 - acc: 0.8976 - val_loss: 0.6607 - val_acc: 0.8705
Epoch 69/500
170s - loss: 0.3484 - acc: 0.9034 - val_loss: 0.6754 - val_acc: 0.8695
Epoch 70/500
170s - loss: 0.3722 - acc: 0.8955 - val_loss: 0.6696 - val_acc: 0.8758
Epoch 71/500
170s - loss: 0.3487 - acc: 0.9008 - val_loss: 0.7003 - val_acc: 0.8684
Epoch 72/500
170s - loss: 0.3796 - acc: 0.8976 - val_loss: 0.6746 - val_acc: 0.8695
Epoch 73/500
171s - loss: 0.3854 - acc: 0.8982 - val_loss: 0.6425 - val_acc: 0.8674
Epoch 74/500
170s - loss: 0.3935 - acc: 0.8971 - val_loss: 0.6692 - val_acc: 0.8695
Epoch 75/500
170s - loss: 0.3652 - acc: 0.8958 - val_loss: 0.7017 - val_acc: 0.8642
Epoch 76/500
170s - loss: 0.3763 - acc: 0.8971 - val_loss: 0.6800 - val_acc: 0.8705
Epoch 77/500
170s - loss: 0.3312 - acc: 0.9029 - val_loss: 0.6990 - val_acc: 0.8663
Epoch 78/500
170s - loss: 0.3645 - acc: 0.8987 - val_loss: 0.6791 - val_acc: 0.8737
Epoch 79/500

Epoch 00078: reducing learning rate to 0.000999999977648.
170s - loss: 0.3473 - acc: 0.9018 - val_loss: 0.7268 - val_acc: 0.8642
Epoch 80/500
170s - loss: 0.3635 - acc: 0.8966 - val_loss: 0.6926 - val_acc: 0.8716
Epoch 81/500
170s - loss: 0.3474 - acc: 0.9034 - val_loss: 0.6852 - val_acc: 0.8684
Epoch 82/500
170s - loss: 0.3525 - acc: 0.9018 - val_loss: 0.6816 - val_acc: 0.8716
Epoch 83/500
170s - loss: 0.3045 - acc: 0.9066 - val_loss: 0.6731 - val_acc: 0.8747
Epoch 84/500
170s - loss: 0.3116 - acc: 0.9063 - val_loss: 0.6780 - val_acc: 0.8747
Epoch 85/500
170s - loss: 0.3129 - acc: 0.9058 - val_loss: 0.6821 - val_acc: 0.8758
Epoch 86/500
170s - loss: 0.3382 - acc: 0.9029 - val_loss: 0.6705 - val_acc: 0.8768
Epoch 87/500
170s - loss: 0.3372 - acc: 0.9050 - val_loss: 0.6712 - val_acc: 0.8758
Epoch 88/500
170s - loss: 0.3126 - acc: 0.9082 - val_loss: 0.6709 - val_acc: 0.8737
Epoch 89/500
170s - loss: 0.3328 - acc: 0.9082 - val_loss: 0.6753 - val_acc: 0.8726
Epoch 90/500
170s - loss: 0.3403 - acc: 0.9047 - val_loss: 0.6751 - val_acc: 0.8737
Epoch 91/500
170s - loss: 0.3456 - acc: 0.9058 - val_loss: 0.6725 - val_acc: 0.8726
Epoch 92/500
170s - loss: 0.3206 - acc: 0.9066 - val_loss: 0.6694 - val_acc: 0.8737
Epoch 93/500
170s - loss: 0.3193 - acc: 0.9076 - val_loss: 0.6709 - val_acc: 0.8747
Epoch 94/500
170s - loss: 0.3227 - acc: 0.9068 - val_loss: 0.6653 - val_acc: 0.8768
Epoch 95/500

Epoch 00094: reducing learning rate to 9.99999931082e-05.
170s - loss: 0.3450 - acc: 0.9037 - val_loss: 0.6673 - val_acc: 0.8747
Epoch 96/500
170s - loss: 0.3367 - acc: 0.9029 - val_loss: 0.6709 - val_acc: 0.8726
Epoch 97/500
170s - loss: 0.3243 - acc: 0.9016 - val_loss: 0.6721 - val_acc: 0.8737
Epoch 98/500
170s - loss: 0.3331 - acc: 0.9053 - val_loss: 0.6633 - val_acc: 0.8747
Epoch 99/500
170s - loss: 0.3120 - acc: 0.9116 - val_loss: 0.6669 - val_acc: 0.8747
Epoch 100/500
170s - loss: 0.3086 - acc: 0.9050 - val_loss: 0.6651 - val_acc: 0.8747
Epoch 101/500
171s - loss: 0.3307 - acc: 0.9053 - val_loss: 0.6675 - val_acc: 0.8758
Epoch 102/500
170s - loss: 0.3313 - acc: 0.9061 - val_loss: 0.6671 - val_acc: 0.8758
Epoch 103/500

Epoch 00102: reducing learning rate to 9.99999901978e-06.
170s - loss: 0.3250 - acc: 0.9042 - val_loss: 0.6661 - val_acc: 0.8747
Epoch 104/500
170s - loss: 0.3186 - acc: 0.9092 - val_loss: 0.6647 - val_acc: 0.8768
Epoch 105/500
170s - loss: 0.3221 - acc: 0.9087 - val_loss: 0.6664 - val_acc: 0.8747
Epoch 106/500
170s - loss: 0.3196 - acc: 0.9047 - val_loss: 0.6689 - val_acc: 0.8747
Epoch 107/500
170s - loss: 0.2984 - acc: 0.9092 - val_loss: 0.6692 - val_acc: 0.8747
Epoch 108/500
170s - loss: 0.3253 - acc: 0.9047 - val_loss: 0.6629 - val_acc: 0.8758
Epoch 109/500
170s - loss: 0.3207 - acc: 0.9068 - val_loss: 0.6650 - val_acc: 0.8758
Epoch 110/500
170s - loss: 0.3342 - acc: 0.9055 - val_loss: 0.6688 - val_acc: 0.8747
Epoch 111/500

Epoch 00110: reducing learning rate to 1e-06.
170s - loss: 0.3147 - acc: 0.9063 - val_loss: 0.6703 - val_acc: 0.8726
Epoch 112/500
170s - loss: 0.3407 - acc: 0.9024 - val_loss: 0.6661 - val_acc: 0.8747
Epoch 113/500
170s - loss: 0.3008 - acc: 0.9087 - val_loss: 0.6655 - val_acc: 0.8747
Epoch 114/500
170s - loss: 0.3367 - acc: 0.9045 - val_loss: 0.6665 - val_acc: 0.8747
Epoch 115/500
170s - loss: 0.3096 - acc: 0.9076 - val_loss: 0.6657 - val_acc: 0.8747
Epoch 116/500
171s - loss: 0.3223 - acc: 0.9037 - val_loss: 0.6679 - val_acc: 0.8747
Epoch 117/500
174s - loss: 0.3313 - acc: 0.9066 - val_loss: 0.6646 - val_acc: 0.8737
Epoch 118/500
174s - loss: 0.3469 - acc: 0.9037 - val_loss: 0.6729 - val_acc: 0.8726
Epoch 119/500
170s - loss: 0.3162 - acc: 0.9061 - val_loss: 0.6645 - val_acc: 0.8758
Epoch 120/500
169s - loss: 0.3070 - acc: 0.9087 - val_loss: 0.6682 - val_acc: 0.8758
Epoch 121/500
169s - loss: 0.3205 - acc: 0.9055 - val_loss: 0.6672 - val_acc: 0.8758
Epoch 122/500
170s - loss: 0.3207 - acc: 0.9050 - val_loss: 0.6682 - val_acc: 0.8737
Epoch 123/500
170s - loss: 0.3275 - acc: 0.9082 - val_loss: 0.6705 - val_acc: 0.8737
Epoch 124/500
171s - loss: 0.2975 - acc: 0.9089 - val_loss: 0.6705 - val_acc: 0.8747
Epoch 125/500
171s - loss: 0.3053 - acc: 0.9087 - val_loss: 0.6651 - val_acc: 0.8747
Epoch 126/500
171s - loss: 0.3098 - acc: 0.9042 - val_loss: 0.6633 - val_acc: 0.8747
Epoch 127/500
171s - loss: 0.3305 - acc: 0.9011 - val_loss: 0.6661 - val_acc: 0.8747
Epoch 128/500
171s - loss: 0.3206 - acc: 0.9079 - val_loss: 0.6642 - val_acc: 0.8737
Training loss for fold 7 is 0.2051135317432253 with percent 92.68421051376744
Testing loss for fold 7 is 0.665278757747851 with percent 87.68421051376745
 
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_17 (InputLayer)            (None, 65, 65, 3)     0                                            
____________________________________________________________________________________________________
conv2d_49 (Conv2D)               (None, 63, 63, 16)    448         input_17[0][0]                   
____________________________________________________________________________________________________
batch_normalization_57 (BatchNor (None, 63, 63, 16)    64          conv2d_49[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_57 (LeakyReLU)       (None, 63, 63, 16)    0           batch_normalization_57[0][0]     
____________________________________________________________________________________________________
conv2d_50 (Conv2D)               (None, 62, 62, 16)    1040        leaky_re_lu_57[0][0]             
____________________________________________________________________________________________________
batch_normalization_58 (BatchNor (None, 62, 62, 16)    64          conv2d_50[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_58 (LeakyReLU)       (None, 62, 62, 16)    0           batch_normalization_58[0][0]     
____________________________________________________________________________________________________
max_pooling2d_25 (MaxPooling2D)  (None, 31, 31, 16)    0           leaky_re_lu_58[0][0]             
____________________________________________________________________________________________________
dropout_41 (Dropout)             (None, 31, 31, 16)    0           max_pooling2d_25[0][0]           
____________________________________________________________________________________________________
conv2d_51 (Conv2D)               (None, 29, 29, 32)    4640        dropout_41[0][0]                 
____________________________________________________________________________________________________
batch_normalization_59 (BatchNor (None, 29, 29, 32)    128         conv2d_51[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_59 (LeakyReLU)       (None, 29, 29, 32)    0           batch_normalization_59[0][0]     
____________________________________________________________________________________________________
conv2d_52 (Conv2D)               (None, 28, 28, 32)    4128        leaky_re_lu_59[0][0]             
____________________________________________________________________________________________________
batch_normalization_60 (BatchNor (None, 28, 28, 32)    128         conv2d_52[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_60 (LeakyReLU)       (None, 28, 28, 32)    0           batch_normalization_60[0][0]     
____________________________________________________________________________________________________
max_pooling2d_26 (MaxPooling2D)  (None, 14, 14, 32)    0           leaky_re_lu_60[0][0]             
____________________________________________________________________________________________________
dropout_42 (Dropout)             (None, 14, 14, 32)    0           max_pooling2d_26[0][0]           
____________________________________________________________________________________________________
conv2d_53 (Conv2D)               (None, 12, 12, 64)    18496       dropout_42[0][0]                 
____________________________________________________________________________________________________
batch_normalization_61 (BatchNor (None, 12, 12, 64)    256         conv2d_53[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_61 (LeakyReLU)       (None, 12, 12, 64)    0           batch_normalization_61[0][0]     
____________________________________________________________________________________________________
conv2d_54 (Conv2D)               (None, 11, 11, 64)    16448       leaky_re_lu_61[0][0]             
____________________________________________________________________________________________________
batch_normalization_62 (BatchNor (None, 11, 11, 64)    256         conv2d_54[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_62 (LeakyReLU)       (None, 11, 11, 64)    0           batch_normalization_62[0][0]     
____________________________________________________________________________________________________
max_pooling2d_27 (MaxPooling2D)  (None, 5, 5, 64)      0           leaky_re_lu_62[0][0]             
____________________________________________________________________________________________________
dropout_43 (Dropout)             (None, 5, 5, 64)      0           max_pooling2d_27[0][0]           
____________________________________________________________________________________________________
flatten_9 (Flatten)              (None, 1600)          0           dropout_43[0][0]                 
____________________________________________________________________________________________________
batch_normalization_63 (BatchNor (None, 1600)          6400        flatten_9[0][0]                  
____________________________________________________________________________________________________
input_18 (InputLayer)            (None, 102)           0                                            
____________________________________________________________________________________________________
concatenate_9 (Concatenate)      (None, 1702)          0           batch_normalization_63[0][0]     
                                                                   input_18[0][0]                   
____________________________________________________________________________________________________
dense_33 (Dense)                 (None, 128)           217984      concatenate_9[0][0]              
____________________________________________________________________________________________________
leaky_re_lu_63 (LeakyReLU)       (None, 128)           0           dense_33[0][0]                   
____________________________________________________________________________________________________
dense_34 (Dense)                 (None, 64)            8256        leaky_re_lu_63[0][0]             
____________________________________________________________________________________________________
dropout_44 (Dropout)             (None, 64)            0           dense_34[0][0]                   
____________________________________________________________________________________________________
dense_35 (Dense)                 (None, 32)            2080        dropout_44[0][0]                 
____________________________________________________________________________________________________
dropout_45 (Dropout)             (None, 32)            0           dense_35[0][0]                   
____________________________________________________________________________________________________
dense_36 (Dense)                 (None, 12)            396         dropout_45[0][0]                 
====================================================================================================
Total params: 281,212
Trainable params: 277,564
Non-trainable params: 3,648
____________________________________________________________________________________________________
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
174s - loss: 1.9544 - acc: 0.3258 - val_loss: 1.9807 - val_acc: 0.3126
Epoch 2/500
169s - loss: 1.5298 - acc: 0.4874 - val_loss: 2.2476 - val_acc: 0.3095
Epoch 3/500
169s - loss: 1.4338 - acc: 0.5476 - val_loss: 2.6008 - val_acc: 0.3674
Epoch 4/500
169s - loss: 1.3227 - acc: 0.5853 - val_loss: 1.1408 - val_acc: 0.6537
Epoch 5/500
168s - loss: 1.2573 - acc: 0.6076 - val_loss: 1.3718 - val_acc: 0.6189
Epoch 6/500
168s - loss: 1.2634 - acc: 0.6234 - val_loss: 0.9113 - val_acc: 0.7168
Epoch 7/500
169s - loss: 1.1288 - acc: 0.6642 - val_loss: 1.3069 - val_acc: 0.6811
Epoch 8/500
169s - loss: 1.1065 - acc: 0.6795 - val_loss: 1.1344 - val_acc: 0.6516
Epoch 9/500
169s - loss: 1.0873 - acc: 0.6834 - val_loss: 2.0745 - val_acc: 0.5253
Epoch 10/500
169s - loss: 0.9861 - acc: 0.7153 - val_loss: 1.4445 - val_acc: 0.6316
Epoch 11/500
168s - loss: 0.9883 - acc: 0.7203 - val_loss: 2.3639 - val_acc: 0.4695
Epoch 12/500
169s - loss: 0.9394 - acc: 0.7376 - val_loss: 1.5458 - val_acc: 0.6221
Epoch 13/500
168s - loss: 1.0414 - acc: 0.7245 - val_loss: 4.5917 - val_acc: 0.2832
Epoch 14/500
169s - loss: 0.9756 - acc: 0.7345 - val_loss: 2.2334 - val_acc: 0.4179
Epoch 15/500

Epoch 00014: reducing learning rate to 0.010000000149.
179s - loss: 0.9293 - acc: 0.7416 - val_loss: 1.7075 - val_acc: 0.6032
Epoch 16/500
233s - loss: 0.7308 - acc: 0.7932 - val_loss: 0.6541 - val_acc: 0.8168
Epoch 17/500
211s - loss: 0.5961 - acc: 0.8218 - val_loss: 0.6253 - val_acc: 0.8242
Epoch 18/500
168s - loss: 0.5418 - acc: 0.8397 - val_loss: 0.6069 - val_acc: 0.8253
Epoch 19/500
168s - loss: 0.4849 - acc: 0.8437 - val_loss: 0.6344 - val_acc: 0.8189
Epoch 20/500
169s - loss: 0.5072 - acc: 0.8453 - val_loss: 0.6786 - val_acc: 0.8158
Epoch 21/500
169s - loss: 0.4818 - acc: 0.8418 - val_loss: 0.6292 - val_acc: 0.8221
Epoch 22/500
169s - loss: 0.4888 - acc: 0.8524 - val_loss: 0.9378 - val_acc: 0.7853
Epoch 23/500
169s - loss: 0.4666 - acc: 0.8513 - val_loss: 0.6441 - val_acc: 0.8211
Epoch 24/500
169s - loss: 0.4262 - acc: 0.8616 - val_loss: 0.6690 - val_acc: 0.8253
Epoch 25/500
169s - loss: 0.4367 - acc: 0.8603 - val_loss: 0.6233 - val_acc: 0.8137
Epoch 26/500
169s - loss: 0.4013 - acc: 0.8668 - val_loss: 1.1987 - val_acc: 0.7305
Epoch 27/500
169s - loss: 0.4500 - acc: 0.8553 - val_loss: 0.5819 - val_acc: 0.8316
Epoch 28/500
169s - loss: 0.3997 - acc: 0.8711 - val_loss: 0.6313 - val_acc: 0.8347
Epoch 29/500
169s - loss: 0.4134 - acc: 0.8697 - val_loss: 0.6610 - val_acc: 0.8305
Epoch 30/500
169s - loss: 0.3796 - acc: 0.8718 - val_loss: 0.7161 - val_acc: 0.8158
Epoch 31/500
169s - loss: 0.3611 - acc: 0.8761 - val_loss: 0.6025 - val_acc: 0.8421
Epoch 32/500
168s - loss: 0.4026 - acc: 0.8734 - val_loss: 0.6152 - val_acc: 0.8411
Epoch 33/500
169s - loss: 0.3968 - acc: 0.8753 - val_loss: 0.8449 - val_acc: 0.7989
Epoch 34/500
169s - loss: 0.3605 - acc: 0.8779 - val_loss: 0.6127 - val_acc: 0.8474
Epoch 35/500
169s - loss: 0.3656 - acc: 0.8837 - val_loss: 0.5995 - val_acc: 0.8516
Epoch 36/500
169s - loss: 0.3467 - acc: 0.8850 - val_loss: 0.6884 - val_acc: 0.8305
Epoch 37/500
169s - loss: 0.3648 - acc: 0.8853 - val_loss: 0.6287 - val_acc: 0.8400
Epoch 38/500
169s - loss: 0.3499 - acc: 0.8826 - val_loss: 0.7857 - val_acc: 0.8116
Epoch 39/500
169s - loss: 0.3548 - acc: 0.8842 - val_loss: 0.5908 - val_acc: 0.8568
Epoch 40/500
169s - loss: 0.3230 - acc: 0.8905 - val_loss: 0.7686 - val_acc: 0.8168
Epoch 41/500
169s - loss: 0.3406 - acc: 0.8916 - val_loss: 0.6676 - val_acc: 0.8253
Epoch 42/500
169s - loss: 0.3383 - acc: 0.8892 - val_loss: 0.6167 - val_acc: 0.8474
Epoch 43/500
169s - loss: 0.3104 - acc: 0.8937 - val_loss: 0.7262 - val_acc: 0.8316
Epoch 44/500
168s - loss: 0.3143 - acc: 0.9003 - val_loss: 0.6099 - val_acc: 0.8526
Epoch 45/500
169s - loss: 0.3158 - acc: 0.8982 - val_loss: 0.6468 - val_acc: 0.8474
Epoch 46/500
169s - loss: 0.3013 - acc: 0.8995 - val_loss: 0.7596 - val_acc: 0.8368
Epoch 47/500
169s - loss: 0.3178 - acc: 0.8963 - val_loss: 0.6460 - val_acc: 0.8495
Epoch 48/500

Epoch 00047: reducing learning rate to 0.000999999977648.
169s - loss: 0.2975 - acc: 0.9005 - val_loss: 0.6359 - val_acc: 0.8516
Epoch 49/500
169s - loss: 0.2857 - acc: 0.9042 - val_loss: 0.6189 - val_acc: 0.8537
Epoch 50/500
169s - loss: 0.2908 - acc: 0.9042 - val_loss: 0.6204 - val_acc: 0.8547
Epoch 51/500
169s - loss: 0.2810 - acc: 0.9026 - val_loss: 0.6212 - val_acc: 0.8516
Epoch 52/500
169s - loss: 0.2419 - acc: 0.9174 - val_loss: 0.6285 - val_acc: 0.8505
Epoch 53/500
169s - loss: 0.2685 - acc: 0.9100 - val_loss: 0.6271 - val_acc: 0.8547
Epoch 54/500
169s - loss: 0.2723 - acc: 0.9095 - val_loss: 0.6395 - val_acc: 0.8474
Epoch 55/500
169s - loss: 0.2692 - acc: 0.9076 - val_loss: 0.6337 - val_acc: 0.8495
Epoch 56/500

Epoch 00055: reducing learning rate to 9.99999931082e-05.
169s - loss: 0.2635 - acc: 0.9087 - val_loss: 0.6457 - val_acc: 0.8526
Epoch 57/500
169s - loss: 0.2588 - acc: 0.9071 - val_loss: 0.6412 - val_acc: 0.8484
Epoch 58/500
169s - loss: 0.2405 - acc: 0.9137 - val_loss: 0.6421 - val_acc: 0.8474
Epoch 59/500
169s - loss: 0.2650 - acc: 0.9124 - val_loss: 0.6343 - val_acc: 0.8526
Epoch 60/500
169s - loss: 0.2824 - acc: 0.9113 - val_loss: 0.6321 - val_acc: 0.8505
Epoch 61/500
170s - loss: 0.2613 - acc: 0.9092 - val_loss: 0.6417 - val_acc: 0.8495
Epoch 62/500
169s - loss: 0.2760 - acc: 0.9100 - val_loss: 0.6446 - val_acc: 0.8495
Epoch 63/500
169s - loss: 0.2684 - acc: 0.9076 - val_loss: 0.6343 - val_acc: 0.8495
Epoch 64/500

Epoch 00063: reducing learning rate to 9.99999901978e-06.
169s - loss: 0.2640 - acc: 0.9168 - val_loss: 0.6370 - val_acc: 0.8484
Epoch 65/500
169s - loss: 0.2655 - acc: 0.9137 - val_loss: 0.6382 - val_acc: 0.8463
Epoch 66/500
169s - loss: 0.2694 - acc: 0.9132 - val_loss: 0.6383 - val_acc: 0.8505
Epoch 67/500
169s - loss: 0.2725 - acc: 0.9121 - val_loss: 0.6394 - val_acc: 0.8495
Epoch 68/500
169s - loss: 0.2755 - acc: 0.9121 - val_loss: 0.6418 - val_acc: 0.8484
Epoch 69/500
170s - loss: 0.2588 - acc: 0.9111 - val_loss: 0.6408 - val_acc: 0.8495
Epoch 70/500
169s - loss: 0.2626 - acc: 0.9100 - val_loss: 0.6342 - val_acc: 0.8463
Epoch 71/500
169s - loss: 0.2478 - acc: 0.9142 - val_loss: 0.6399 - val_acc: 0.8442
Epoch 72/500

Epoch 00071: reducing learning rate to 1e-06.
169s - loss: 0.2650 - acc: 0.9111 - val_loss: 0.6379 - val_acc: 0.8463
Epoch 73/500
169s - loss: 0.2756 - acc: 0.9068 - val_loss: 0.6311 - val_acc: 0.8484
Epoch 74/500
169s - loss: 0.2740 - acc: 0.9068 - val_loss: 0.6293 - val_acc: 0.8505
Epoch 75/500
169s - loss: 0.2685 - acc: 0.9100 - val_loss: 0.6422 - val_acc: 0.8495
Epoch 76/500
169s - loss: 0.2630 - acc: 0.9089 - val_loss: 0.6315 - val_acc: 0.8484
Epoch 77/500
169s - loss: 0.2385 - acc: 0.9161 - val_loss: 0.6360 - val_acc: 0.8484
Epoch 78/500
169s - loss: 0.2588 - acc: 0.9118 - val_loss: 0.6339 - val_acc: 0.8505
Epoch 79/500
169s - loss: 0.2505 - acc: 0.9132 - val_loss: 0.6320 - val_acc: 0.8474
Epoch 80/500
169s - loss: 0.2737 - acc: 0.9079 - val_loss: 0.6475 - val_acc: 0.8463
Epoch 81/500
170s - loss: 0.2615 - acc: 0.9121 - val_loss: 0.6405 - val_acc: 0.8505
Epoch 82/500
169s - loss: 0.2655 - acc: 0.9139 - val_loss: 0.6344 - val_acc: 0.8484
Epoch 83/500
169s - loss: 0.2584 - acc: 0.9121 - val_loss: 0.6339 - val_acc: 0.8495
Epoch 84/500
169s - loss: 0.2672 - acc: 0.9053 - val_loss: 0.6325 - val_acc: 0.8505
Epoch 85/500
169s - loss: 0.2652 - acc: 0.9105 - val_loss: 0.6351 - val_acc: 0.8484
Epoch 86/500
169s - loss: 0.2844 - acc: 0.9097 - val_loss: 0.6319 - val_acc: 0.8484
Epoch 87/500
169s - loss: 0.2652 - acc: 0.9118 - val_loss: 0.6313 - val_acc: 0.8495
Epoch 88/500
169s - loss: 0.2750 - acc: 0.9129 - val_loss: 0.6342 - val_acc: 0.8495
Training loss for fold 8 is 0.16979055559556736 with percent 92.7631578821885
Testing loss for fold 8 is 0.5907879511933578 with percent 85.68421057650917
 
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_19 (InputLayer)            (None, 65, 65, 3)     0                                            
____________________________________________________________________________________________________
conv2d_55 (Conv2D)               (None, 63, 63, 16)    448         input_19[0][0]                   
____________________________________________________________________________________________________
batch_normalization_64 (BatchNor (None, 63, 63, 16)    64          conv2d_55[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_64 (LeakyReLU)       (None, 63, 63, 16)    0           batch_normalization_64[0][0]     
____________________________________________________________________________________________________
conv2d_56 (Conv2D)               (None, 62, 62, 16)    1040        leaky_re_lu_64[0][0]             
____________________________________________________________________________________________________
batch_normalization_65 (BatchNor (None, 62, 62, 16)    64          conv2d_56[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_65 (LeakyReLU)       (None, 62, 62, 16)    0           batch_normalization_65[0][0]     
____________________________________________________________________________________________________
max_pooling2d_28 (MaxPooling2D)  (None, 31, 31, 16)    0           leaky_re_lu_65[0][0]             
____________________________________________________________________________________________________
dropout_46 (Dropout)             (None, 31, 31, 16)    0           max_pooling2d_28[0][0]           
____________________________________________________________________________________________________
conv2d_57 (Conv2D)               (None, 29, 29, 32)    4640        dropout_46[0][0]                 
____________________________________________________________________________________________________
batch_normalization_66 (BatchNor (None, 29, 29, 32)    128         conv2d_57[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_66 (LeakyReLU)       (None, 29, 29, 32)    0           batch_normalization_66[0][0]     
____________________________________________________________________________________________________
conv2d_58 (Conv2D)               (None, 28, 28, 32)    4128        leaky_re_lu_66[0][0]             
____________________________________________________________________________________________________
batch_normalization_67 (BatchNor (None, 28, 28, 32)    128         conv2d_58[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_67 (LeakyReLU)       (None, 28, 28, 32)    0           batch_normalization_67[0][0]     
____________________________________________________________________________________________________
max_pooling2d_29 (MaxPooling2D)  (None, 14, 14, 32)    0           leaky_re_lu_67[0][0]             
____________________________________________________________________________________________________
dropout_47 (Dropout)             (None, 14, 14, 32)    0           max_pooling2d_29[0][0]           
____________________________________________________________________________________________________
conv2d_59 (Conv2D)               (None, 12, 12, 64)    18496       dropout_47[0][0]                 
____________________________________________________________________________________________________
batch_normalization_68 (BatchNor (None, 12, 12, 64)    256         conv2d_59[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_68 (LeakyReLU)       (None, 12, 12, 64)    0           batch_normalization_68[0][0]     
____________________________________________________________________________________________________
conv2d_60 (Conv2D)               (None, 11, 11, 64)    16448       leaky_re_lu_68[0][0]             
____________________________________________________________________________________________________
batch_normalization_69 (BatchNor (None, 11, 11, 64)    256         conv2d_60[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_69 (LeakyReLU)       (None, 11, 11, 64)    0           batch_normalization_69[0][0]     
____________________________________________________________________________________________________
max_pooling2d_30 (MaxPooling2D)  (None, 5, 5, 64)      0           leaky_re_lu_69[0][0]             
____________________________________________________________________________________________________
dropout_48 (Dropout)             (None, 5, 5, 64)      0           max_pooling2d_30[0][0]           
____________________________________________________________________________________________________
flatten_10 (Flatten)             (None, 1600)          0           dropout_48[0][0]                 
____________________________________________________________________________________________________
batch_normalization_70 (BatchNor (None, 1600)          6400        flatten_10[0][0]                 
____________________________________________________________________________________________________
input_20 (InputLayer)            (None, 102)           0                                            
____________________________________________________________________________________________________
concatenate_10 (Concatenate)     (None, 1702)          0           batch_normalization_70[0][0]     
                                                                   input_20[0][0]                   
____________________________________________________________________________________________________
dense_37 (Dense)                 (None, 128)           217984      concatenate_10[0][0]             
____________________________________________________________________________________________________
leaky_re_lu_70 (LeakyReLU)       (None, 128)           0           dense_37[0][0]                   
____________________________________________________________________________________________________
dense_38 (Dense)                 (None, 64)            8256        leaky_re_lu_70[0][0]             
____________________________________________________________________________________________________
dropout_49 (Dropout)             (None, 64)            0           dense_38[0][0]                   
____________________________________________________________________________________________________
dense_39 (Dense)                 (None, 32)            2080        dropout_49[0][0]                 
____________________________________________________________________________________________________
dropout_50 (Dropout)             (None, 32)            0           dense_39[0][0]                   
____________________________________________________________________________________________________
dense_40 (Dense)                 (None, 12)            396         dropout_50[0][0]                 
====================================================================================================
Total params: 281,212
Trainable params: 277,564
Non-trainable params: 3,648
____________________________________________________________________________________________________
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
175s - loss: 2.0673 - acc: 0.2913 - val_loss: 1.5952 - val_acc: 0.4495
Epoch 2/500
170s - loss: 1.6149 - acc: 0.4708 - val_loss: 2.0270 - val_acc: 0.3642
Epoch 3/500
170s - loss: 1.4386 - acc: 0.5379 - val_loss: 3.8241 - val_acc: 0.1516
Epoch 4/500
170s - loss: 1.3261 - acc: 0.5739 - val_loss: 1.3247 - val_acc: 0.6221
Epoch 5/500
170s - loss: 1.2592 - acc: 0.6161 - val_loss: 1.4140 - val_acc: 0.5779
Epoch 6/500
171s - loss: 1.1913 - acc: 0.6326 - val_loss: 1.0392 - val_acc: 0.6821
Epoch 7/500
170s - loss: 1.1277 - acc: 0.6624 - val_loss: 1.7320 - val_acc: 0.5347
Epoch 8/500
170s - loss: 1.1277 - acc: 0.6597 - val_loss: 1.3965 - val_acc: 0.6495
Epoch 9/500
170s - loss: 1.0736 - acc: 0.6863 - val_loss: 1.3222 - val_acc: 0.6821
Epoch 10/500
170s - loss: 1.0423 - acc: 0.6963 - val_loss: 1.1893 - val_acc: 0.6821
Epoch 11/500
170s - loss: 1.0300 - acc: 0.7011 - val_loss: 1.1389 - val_acc: 0.7011
Epoch 12/500
170s - loss: 1.0327 - acc: 0.7037 - val_loss: 1.2652 - val_acc: 0.6484
Epoch 13/500
170s - loss: 0.9817 - acc: 0.7229 - val_loss: 1.2173 - val_acc: 0.6632
Epoch 14/500
170s - loss: 0.9711 - acc: 0.7321 - val_loss: 1.3763 - val_acc: 0.6253
Epoch 15/500
170s - loss: 1.0012 - acc: 0.7379 - val_loss: 2.8132 - val_acc: 0.3968
Epoch 16/500
170s - loss: 0.8973 - acc: 0.7526 - val_loss: 0.8387 - val_acc: 0.7000
Epoch 17/500
170s - loss: 0.9203 - acc: 0.7455 - val_loss: 1.4109 - val_acc: 0.6411
Epoch 18/500
170s - loss: 0.9054 - acc: 0.7603 - val_loss: 0.9014 - val_acc: 0.7474
Epoch 19/500
170s - loss: 0.8341 - acc: 0.7766 - val_loss: 1.4805 - val_acc: 0.6726
Epoch 20/500
170s - loss: 0.8964 - acc: 0.7637 - val_loss: 1.2395 - val_acc: 0.6642
Epoch 21/500
170s - loss: 0.9314 - acc: 0.7526 - val_loss: 2.1434 - val_acc: 0.5842
Epoch 22/500
170s - loss: 0.8818 - acc: 0.7642 - val_loss: 1.1517 - val_acc: 0.7211
Epoch 23/500
170s - loss: 0.9021 - acc: 0.7679 - val_loss: 1.6884 - val_acc: 0.5863
Epoch 24/500
170s - loss: 0.9505 - acc: 0.7558 - val_loss: 2.1847 - val_acc: 0.4368
Epoch 25/500
170s - loss: 0.9072 - acc: 0.7655 - val_loss: 1.3709 - val_acc: 0.7084
Epoch 26/500
170s - loss: 0.8803 - acc: 0.7779 - val_loss: 1.5439 - val_acc: 0.6463
Epoch 27/500

Epoch 00026: reducing learning rate to 0.010000000149.
170s - loss: 0.8781 - acc: 0.7782 - val_loss: 1.5175 - val_acc: 0.6916
Epoch 28/500
170s - loss: 0.7207 - acc: 0.8153 - val_loss: 0.7233 - val_acc: 0.8253
Epoch 29/500
178s - loss: 0.5658 - acc: 0.8429 - val_loss: 0.6854 - val_acc: 0.8305
Epoch 30/500
189s - loss: 0.4923 - acc: 0.8516 - val_loss: 0.6884 - val_acc: 0.8284
Epoch 31/500
180s - loss: 0.4464 - acc: 0.8608 - val_loss: 0.6670 - val_acc: 0.8232
Epoch 32/500
191s - loss: 0.4583 - acc: 0.8671 - val_loss: 0.5468 - val_acc: 0.8505
Epoch 33/500
201s - loss: 0.4198 - acc: 0.8721 - val_loss: 0.7411 - val_acc: 0.8305
Epoch 34/500
207s - loss: 0.4164 - acc: 0.8739 - val_loss: 0.5784 - val_acc: 0.8400
Epoch 35/500
203s - loss: 0.4170 - acc: 0.8755 - val_loss: 0.6310 - val_acc: 0.8400
Epoch 36/500
195s - loss: 0.3800 - acc: 0.8779 - val_loss: 0.5939 - val_acc: 0.8547
Epoch 37/500
193s - loss: 0.3964 - acc: 0.8805 - val_loss: 0.6482 - val_acc: 0.8537
Epoch 38/500
193s - loss: 0.3901 - acc: 0.8795 - val_loss: 0.5426 - val_acc: 0.8579
Epoch 39/500
203s - loss: 0.3706 - acc: 0.8797 - val_loss: 0.5468 - val_acc: 0.8621
Epoch 40/500
182s - loss: 0.3584 - acc: 0.8861 - val_loss: 0.6920 - val_acc: 0.8400
Epoch 41/500
170s - loss: 0.3757 - acc: 0.8808 - val_loss: 0.5697 - val_acc: 0.8653
Epoch 42/500
170s - loss: 0.3544 - acc: 0.8839 - val_loss: 0.5084 - val_acc: 0.8653
Epoch 43/500
170s - loss: 0.3402 - acc: 0.8868 - val_loss: 0.5326 - val_acc: 0.8716
Epoch 44/500
170s - loss: 0.3595 - acc: 0.8800 - val_loss: 0.6572 - val_acc: 0.8516
Epoch 45/500
171s - loss: 0.3653 - acc: 0.8816 - val_loss: 0.5925 - val_acc: 0.8505
Epoch 46/500
170s - loss: 0.3291 - acc: 0.8868 - val_loss: 0.5764 - val_acc: 0.8653
Epoch 47/500
171s - loss: 0.3457 - acc: 0.8842 - val_loss: 0.6189 - val_acc: 0.8516
Epoch 48/500
170s - loss: 0.3330 - acc: 0.8855 - val_loss: 0.5603 - val_acc: 0.8695
Epoch 49/500
170s - loss: 0.3251 - acc: 0.8982 - val_loss: 0.5613 - val_acc: 0.8589
Epoch 50/500
170s - loss: 0.3006 - acc: 0.8937 - val_loss: 0.5716 - val_acc: 0.8632
Epoch 51/500
170s - loss: 0.3256 - acc: 0.8884 - val_loss: 0.6333 - val_acc: 0.8642
Epoch 52/500
170s - loss: 0.3209 - acc: 0.8947 - val_loss: 0.5185 - val_acc: 0.8768
Epoch 53/500
170s - loss: 0.2970 - acc: 0.8950 - val_loss: 0.5458 - val_acc: 0.8705
Epoch 54/500
169s - loss: 0.2858 - acc: 0.9018 - val_loss: 0.6008 - val_acc: 0.8621
Epoch 55/500
170s - loss: 0.2929 - acc: 0.9000 - val_loss: 0.6190 - val_acc: 0.8642
Epoch 56/500
169s - loss: 0.2975 - acc: 0.8911 - val_loss: 0.5257 - val_acc: 0.8674
Epoch 57/500
170s - loss: 0.2928 - acc: 0.8987 - val_loss: 0.5667 - val_acc: 0.8611
Epoch 58/500
169s - loss: 0.3434 - acc: 0.8897 - val_loss: 0.6056 - val_acc: 0.8632
Epoch 59/500
169s - loss: 0.3151 - acc: 0.8939 - val_loss: 0.5467 - val_acc: 0.8632
Epoch 60/500
169s - loss: 0.2891 - acc: 0.9026 - val_loss: 0.6243 - val_acc: 0.8632
Epoch 61/500

Epoch 00060: reducing learning rate to 0.000999999977648.
169s - loss: 0.2940 - acc: 0.9005 - val_loss: 0.6240 - val_acc: 0.8621
Epoch 62/500
169s - loss: 0.2779 - acc: 0.9018 - val_loss: 0.5683 - val_acc: 0.8695
Epoch 63/500
169s - loss: 0.2757 - acc: 0.9018 - val_loss: 0.5649 - val_acc: 0.8716
Epoch 64/500
170s - loss: 0.2509 - acc: 0.9047 - val_loss: 0.5588 - val_acc: 0.8684
Epoch 65/500
169s - loss: 0.2832 - acc: 0.9061 - val_loss: 0.5596 - val_acc: 0.8663
Epoch 66/500
170s - loss: 0.2522 - acc: 0.9118 - val_loss: 0.5645 - val_acc: 0.8684
Epoch 67/500
169s - loss: 0.2471 - acc: 0.9137 - val_loss: 0.5660 - val_acc: 0.8684
Epoch 68/500
170s - loss: 0.2660 - acc: 0.9113 - val_loss: 0.5687 - val_acc: 0.8632
Epoch 69/500

Epoch 00068: reducing learning rate to 9.99999931082e-05.
169s - loss: 0.2792 - acc: 0.9039 - val_loss: 0.5690 - val_acc: 0.8674
Epoch 70/500
169s - loss: 0.2693 - acc: 0.9116 - val_loss: 0.5661 - val_acc: 0.8653
Epoch 71/500
170s - loss: 0.2422 - acc: 0.9089 - val_loss: 0.5592 - val_acc: 0.8663
Epoch 72/500
170s - loss: 0.2604 - acc: 0.9071 - val_loss: 0.5648 - val_acc: 0.8663
Epoch 73/500
170s - loss: 0.2674 - acc: 0.9050 - val_loss: 0.5672 - val_acc: 0.8653
Epoch 74/500
170s - loss: 0.2474 - acc: 0.9113 - val_loss: 0.5641 - val_acc: 0.8674
Epoch 75/500
170s - loss: 0.2711 - acc: 0.8953 - val_loss: 0.5582 - val_acc: 0.8674
Epoch 76/500
170s - loss: 0.2530 - acc: 0.9108 - val_loss: 0.5675 - val_acc: 0.8642
Epoch 77/500

Epoch 00076: reducing learning rate to 9.99999901978e-06.
170s - loss: 0.2717 - acc: 0.9008 - val_loss: 0.5613 - val_acc: 0.8663
Epoch 78/500
170s - loss: 0.2751 - acc: 0.9042 - val_loss: 0.5596 - val_acc: 0.8663
Epoch 79/500
170s - loss: 0.2703 - acc: 0.9079 - val_loss: 0.5593 - val_acc: 0.8653
Epoch 80/500
169s - loss: 0.2510 - acc: 0.9137 - val_loss: 0.5635 - val_acc: 0.8653
Epoch 81/500
169s - loss: 0.2568 - acc: 0.9066 - val_loss: 0.5576 - val_acc: 0.8674
Epoch 82/500
170s - loss: 0.2566 - acc: 0.9058 - val_loss: 0.5620 - val_acc: 0.8674
Epoch 83/500
169s - loss: 0.2482 - acc: 0.9126 - val_loss: 0.5698 - val_acc: 0.8663
Epoch 84/500
170s - loss: 0.2752 - acc: 0.9061 - val_loss: 0.5697 - val_acc: 0.8674
Epoch 85/500

Epoch 00084: reducing learning rate to 1e-06.
169s - loss: 0.2668 - acc: 0.9008 - val_loss: 0.5647 - val_acc: 0.8684
Epoch 86/500
170s - loss: 0.2714 - acc: 0.9087 - val_loss: 0.5678 - val_acc: 0.8663
Epoch 87/500
169s - loss: 0.2624 - acc: 0.9034 - val_loss: 0.5634 - val_acc: 0.8663
Epoch 88/500
169s - loss: 0.2546 - acc: 0.9063 - val_loss: 0.5639 - val_acc: 0.8663
Epoch 89/500
170s - loss: 0.2634 - acc: 0.9079 - val_loss: 0.5633 - val_acc: 0.8653
Epoch 90/500
169s - loss: 0.2848 - acc: 0.9042 - val_loss: 0.5582 - val_acc: 0.8663
Epoch 91/500
170s - loss: 0.2719 - acc: 0.9045 - val_loss: 0.5642 - val_acc: 0.8653
Epoch 92/500
169s - loss: 0.2643 - acc: 0.9013 - val_loss: 0.5658 - val_acc: 0.8653
Epoch 93/500
170s - loss: 0.2567 - acc: 0.9079 - val_loss: 0.5624 - val_acc: 0.8653
Epoch 94/500
170s - loss: 0.2772 - acc: 0.9018 - val_loss: 0.5670 - val_acc: 0.8632
Epoch 95/500
170s - loss: 0.2591 - acc: 0.9118 - val_loss: 0.5629 - val_acc: 0.8663
Epoch 96/500
169s - loss: 0.2751 - acc: 0.9021 - val_loss: 0.5672 - val_acc: 0.8653
Epoch 97/500
169s - loss: 0.2716 - acc: 0.9053 - val_loss: 0.5713 - val_acc: 0.8653
Epoch 98/500
170s - loss: 0.2570 - acc: 0.9058 - val_loss: 0.5612 - val_acc: 0.8653
Epoch 99/500
169s - loss: 0.2430 - acc: 0.9111 - val_loss: 0.5655 - val_acc: 0.8653
Epoch 100/500
170s - loss: 0.2721 - acc: 0.9061 - val_loss: 0.5589 - val_acc: 0.8642
Epoch 101/500
169s - loss: 0.2744 - acc: 0.9026 - val_loss: 0.5635 - val_acc: 0.8653
Epoch 102/500
169s - loss: 0.2503 - acc: 0.9071 - val_loss: 0.5641 - val_acc: 0.8684
Epoch 103/500
169s - loss: 0.2669 - acc: 0.9018 - val_loss: 0.5585 - val_acc: 0.8653
Training loss for fold 9 is 0.16070115278818106 with percent 92.84210526315789
Testing loss for fold 9 is 0.5185104955811249 with percent 87.68421058905751
 
Prediction dims (10, 794, 12)
Done training kfolds. Results:
[[0.10434826 0.955      0.60422793 0.89052632]
 [0.12349423 0.94973684 0.6060009  0.87368421]
 [0.16895242 0.93631579 0.7122275  0.84631579]
 [0.13717375 0.94973684 0.46811737 0.89789474]
 [0.14123552 0.94052632 0.46590047 0.88315789]
 [0.10951646 0.95921053 0.63217277 0.88421053]
 [0.13320775 0.94131579 0.55916332 0.86526316]
 [0.20511353 0.92684211 0.66527876 0.87684211]
 [0.16979056 0.92763158 0.59078795 0.85684211]
 [0.16070115 0.92842105 0.5185105  0.87684211]]
