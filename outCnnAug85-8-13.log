Using TensorFlow backend.
/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
 
Did not find train data with correct size. Generating...
 
/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.
  warn("The default mode, 'constant', will be changed to 'reflect' in "
Done. Loading train images
 
 
Did not find test data with correct size. Generating...
 
Done. Loading test images
 
Augmentation data size (4750, 102) (794, 102)
/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)
(3800, 85, 85, 3) (3800,)
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_1 (InputLayer)             (None, 85, 85, 3)     0                                            
____________________________________________________________________________________________________
conv2d_1 (Conv2D)                (None, 83, 83, 16)    448         input_1[0][0]                    
____________________________________________________________________________________________________
batch_normalization_1 (BatchNorm (None, 83, 83, 16)    64          conv2d_1[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_1 (LeakyReLU)        (None, 83, 83, 16)    0           batch_normalization_1[0][0]      
____________________________________________________________________________________________________
conv2d_2 (Conv2D)                (None, 82, 82, 16)    1040        leaky_re_lu_1[0][0]              
____________________________________________________________________________________________________
batch_normalization_2 (BatchNorm (None, 82, 82, 16)    64          conv2d_2[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_2 (LeakyReLU)        (None, 82, 82, 16)    0           batch_normalization_2[0][0]      
____________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)   (None, 41, 41, 16)    0           leaky_re_lu_2[0][0]              
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 41, 41, 16)    0           max_pooling2d_1[0][0]            
____________________________________________________________________________________________________
conv2d_3 (Conv2D)                (None, 39, 39, 32)    4640        dropout_1[0][0]                  
____________________________________________________________________________________________________
batch_normalization_3 (BatchNorm (None, 39, 39, 32)    128         conv2d_3[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_3 (LeakyReLU)        (None, 39, 39, 32)    0           batch_normalization_3[0][0]      
____________________________________________________________________________________________________
conv2d_4 (Conv2D)                (None, 38, 38, 32)    4128        leaky_re_lu_3[0][0]              
____________________________________________________________________________________________________
batch_normalization_4 (BatchNorm (None, 38, 38, 32)    128         conv2d_4[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_4 (LeakyReLU)        (None, 38, 38, 32)    0           batch_normalization_4[0][0]      
____________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)   (None, 19, 19, 32)    0           leaky_re_lu_4[0][0]              
____________________________________________________________________________________________________
dropout_2 (Dropout)              (None, 19, 19, 32)    0           max_pooling2d_2[0][0]            
____________________________________________________________________________________________________
conv2d_5 (Conv2D)                (None, 17, 17, 64)    18496       dropout_2[0][0]                  
____________________________________________________________________________________________________
batch_normalization_5 (BatchNorm (None, 17, 17, 64)    256         conv2d_5[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_5 (LeakyReLU)        (None, 17, 17, 64)    0           batch_normalization_5[0][0]      
____________________________________________________________________________________________________
conv2d_6 (Conv2D)                (None, 16, 16, 64)    16448       leaky_re_lu_5[0][0]              
____________________________________________________________________________________________________
batch_normalization_6 (BatchNorm (None, 16, 16, 64)    256         conv2d_6[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_6 (LeakyReLU)        (None, 16, 16, 64)    0           batch_normalization_6[0][0]      
____________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)   (None, 8, 8, 64)      0           leaky_re_lu_6[0][0]              
____________________________________________________________________________________________________
dropout_3 (Dropout)              (None, 8, 8, 64)      0           max_pooling2d_3[0][0]            
____________________________________________________________________________________________________
flatten_1 (Flatten)              (None, 4096)          0           dropout_3[0][0]                  
____________________________________________________________________________________________________
batch_normalization_7 (BatchNorm (None, 4096)          16384       flatten_1[0][0]                  
____________________________________________________________________________________________________
input_2 (InputLayer)             (None, 102)           0                                            
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 4198)          0           batch_normalization_7[0][0]      
                                                                   input_2[0][0]                    
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 128)           537472      concatenate_1[0][0]              
____________________________________________________________________________________________________
leaky_re_lu_7 (LeakyReLU)        (None, 128)           0           dense_1[0][0]                    
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 64)            8256        leaky_re_lu_7[0][0]              
____________________________________________________________________________________________________
dropout_4 (Dropout)              (None, 64)            0           dense_2[0][0]                    
____________________________________________________________________________________________________
dense_3 (Dense)                  (None, 32)            2080        dropout_4[0][0]                  
____________________________________________________________________________________________________
dropout_5 (Dropout)              (None, 32)            0           dense_3[0][0]                    
____________________________________________________________________________________________________
dense_4 (Dense)                  (None, 12)            396         dropout_5[0][0]                  
====================================================================================================
Total params: 610,684
Trainable params: 602,044
Non-trainable params: 8,640
____________________________________________________________________________________________________
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
2018-08-13 23:38:58.218725: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-08-13 23:38:58.219151: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
291s - loss: 2.0095 - acc: 0.3329 - val_loss: 2.4639 - val_acc: 0.2568
Epoch 2/500
290s - loss: 1.6103 - acc: 0.4674 - val_loss: 2.1925 - val_acc: 0.4600
Epoch 3/500
290s - loss: 1.4584 - acc: 0.5289 - val_loss: 2.0718 - val_acc: 0.4389
Epoch 4/500
291s - loss: 1.3586 - acc: 0.5761 - val_loss: 1.3351 - val_acc: 0.5716
Epoch 5/500
291s - loss: 1.2641 - acc: 0.6192 - val_loss: 1.4158 - val_acc: 0.6284
Epoch 6/500
291s - loss: 1.1911 - acc: 0.6553 - val_loss: 2.6211 - val_acc: 0.3653
Epoch 7/500
290s - loss: 1.1602 - acc: 0.6555 - val_loss: 1.3422 - val_acc: 0.6695
Epoch 8/500
290s - loss: 1.1435 - acc: 0.6653 - val_loss: 1.3417 - val_acc: 0.5916
Epoch 9/500
289s - loss: 1.0868 - acc: 0.6842 - val_loss: 1.1539 - val_acc: 0.7063
Epoch 10/500
290s - loss: 1.1329 - acc: 0.6832 - val_loss: 1.2685 - val_acc: 0.7032
Epoch 11/500
289s - loss: 1.0795 - acc: 0.7021 - val_loss: 1.3369 - val_acc: 0.6232
Epoch 12/500
289s - loss: 1.1285 - acc: 0.6892 - val_loss: 2.0213 - val_acc: 0.4979
Epoch 13/500
289s - loss: 1.1450 - acc: 0.6932 - val_loss: 5.5465 - val_acc: 0.3284
Epoch 14/500
288s - loss: 1.1281 - acc: 0.6995 - val_loss: 3.5546 - val_acc: 0.4411
Epoch 15/500
289s - loss: 1.0359 - acc: 0.7174 - val_loss: 1.9150 - val_acc: 0.5737
Epoch 16/500
289s - loss: 1.0940 - acc: 0.7247 - val_loss: 1.9688 - val_acc: 0.5863
Epoch 17/500
288s - loss: 1.1034 - acc: 0.7045 - val_loss: 3.1200 - val_acc: 0.4537
Epoch 18/500

Epoch 00017: reducing learning rate to 0.010000000149.
289s - loss: 1.1285 - acc: 0.7237 - val_loss: 1.4059 - val_acc: 0.7011
Epoch 19/500
288s - loss: 0.9364 - acc: 0.7324 - val_loss: 0.9229 - val_acc: 0.7674
Epoch 20/500
289s - loss: 0.7964 - acc: 0.7645 - val_loss: 1.0176 - val_acc: 0.7526
Epoch 21/500
289s - loss: 0.7476 - acc: 0.7834 - val_loss: 0.7753 - val_acc: 0.7968
Epoch 22/500
289s - loss: 0.6985 - acc: 0.8003 - val_loss: 0.7664 - val_acc: 0.8084
Epoch 23/500
289s - loss: 0.6618 - acc: 0.8045 - val_loss: 0.7811 - val_acc: 0.8042
Epoch 24/500
289s - loss: 0.6378 - acc: 0.8068 - val_loss: 0.7427 - val_acc: 0.8168
Epoch 25/500
289s - loss: 0.6188 - acc: 0.8121 - val_loss: 0.8170 - val_acc: 0.7989
Epoch 26/500
289s - loss: 0.6053 - acc: 0.8147 - val_loss: 0.7261 - val_acc: 0.8221
Epoch 27/500
288s - loss: 0.5730 - acc: 0.8158 - val_loss: 0.7472 - val_acc: 0.8168
Epoch 28/500
288s - loss: 0.5681 - acc: 0.8379 - val_loss: 0.7677 - val_acc: 0.8200
Epoch 29/500
289s - loss: 0.5636 - acc: 0.8282 - val_loss: 0.7587 - val_acc: 0.8274
Epoch 30/500
289s - loss: 0.5549 - acc: 0.8250 - val_loss: 0.6519 - val_acc: 0.8347
Epoch 31/500
288s - loss: 0.5255 - acc: 0.8382 - val_loss: 0.7227 - val_acc: 0.8284
Epoch 32/500
289s - loss: 0.5663 - acc: 0.8361 - val_loss: 0.6228 - val_acc: 0.8484
Epoch 33/500
289s - loss: 0.5283 - acc: 0.8337 - val_loss: 0.6177 - val_acc: 0.8537
Epoch 34/500
289s - loss: 0.4764 - acc: 0.8445 - val_loss: 0.6637 - val_acc: 0.8411
Epoch 35/500
288s - loss: 0.4902 - acc: 0.8437 - val_loss: 0.6470 - val_acc: 0.8474
Epoch 36/500
288s - loss: 0.4801 - acc: 0.8466 - val_loss: 0.6998 - val_acc: 0.8400
Epoch 37/500
288s - loss: 0.4880 - acc: 0.8484 - val_loss: 0.6357 - val_acc: 0.8463
Epoch 38/500
289s - loss: 0.4465 - acc: 0.8503 - val_loss: 0.5910 - val_acc: 0.8537
Epoch 39/500
289s - loss: 0.4225 - acc: 0.8658 - val_loss: 0.6529 - val_acc: 0.8621
Epoch 40/500
289s - loss: 0.4477 - acc: 0.8576 - val_loss: 0.6044 - val_acc: 0.8568
Epoch 41/500
289s - loss: 0.4070 - acc: 0.8655 - val_loss: 0.5933 - val_acc: 0.8621
Epoch 42/500
289s - loss: 0.4207 - acc: 0.8661 - val_loss: 0.6242 - val_acc: 0.8526
Epoch 43/500
289s - loss: 0.4015 - acc: 0.8705 - val_loss: 0.6463 - val_acc: 0.8537
Epoch 44/500
289s - loss: 0.4105 - acc: 0.8689 - val_loss: 0.6221 - val_acc: 0.8558
Epoch 45/500
288s - loss: 0.3914 - acc: 0.8742 - val_loss: 0.6627 - val_acc: 0.8526
Epoch 46/500
289s - loss: 0.3945 - acc: 0.8745 - val_loss: 0.5713 - val_acc: 0.8684
Epoch 47/500
289s - loss: 0.3942 - acc: 0.8724 - val_loss: 0.5759 - val_acc: 0.8663
Epoch 48/500
289s - loss: 0.3389 - acc: 0.8855 - val_loss: 0.6076 - val_acc: 0.8558
Epoch 49/500
288s - loss: 0.3782 - acc: 0.8755 - val_loss: 0.7293 - val_acc: 0.8495
Epoch 50/500
289s - loss: 0.3704 - acc: 0.8834 - val_loss: 0.6706 - val_acc: 0.8579
Epoch 51/500
289s - loss: 0.3358 - acc: 0.8879 - val_loss: 0.6345 - val_acc: 0.8611
Epoch 52/500
289s - loss: 0.3641 - acc: 0.8816 - val_loss: 0.5714 - val_acc: 0.8716
Epoch 53/500
290s - loss: 0.3797 - acc: 0.8818 - val_loss: 0.6905 - val_acc: 0.8453
Epoch 54/500
289s - loss: 0.3411 - acc: 0.8987 - val_loss: 0.6744 - val_acc: 0.8663
Epoch 55/500
289s - loss: 0.3364 - acc: 0.8950 - val_loss: 0.6456 - val_acc: 0.8621
Epoch 56/500
288s - loss: 0.3397 - acc: 0.8905 - val_loss: 0.6628 - val_acc: 0.8642
Epoch 57/500
289s - loss: 0.3603 - acc: 0.8813 - val_loss: 0.6241 - val_acc: 0.8737
Epoch 58/500
290s - loss: 0.3126 - acc: 0.8979 - val_loss: 0.6487 - val_acc: 0.8621
Epoch 59/500
289s - loss: 0.3069 - acc: 0.8995 - val_loss: 0.6112 - val_acc: 0.8695
Epoch 60/500
289s - loss: 0.2994 - acc: 0.8984 - val_loss: 0.7637 - val_acc: 0.8526
Epoch 61/500
288s - loss: 0.2953 - acc: 0.9013 - val_loss: 0.6779 - val_acc: 0.8653
Epoch 62/500
289s - loss: 0.3035 - acc: 0.9034 - val_loss: 0.6278 - val_acc: 0.8684
Epoch 63/500
289s - loss: 0.3175 - acc: 0.8968 - val_loss: 0.6383 - val_acc: 0.8674
Epoch 64/500
288s - loss: 0.2759 - acc: 0.9039 - val_loss: 0.6905 - val_acc: 0.8663
Epoch 65/500
288s - loss: 0.3058 - acc: 0.9024 - val_loss: 0.6908 - val_acc: 0.8716
Epoch 66/500

Epoch 00065: reducing learning rate to 0.000999999977648.
288s - loss: 0.2954 - acc: 0.9039 - val_loss: 0.6755 - val_acc: 0.8589
Epoch 67/500
289s - loss: 0.2668 - acc: 0.9100 - val_loss: 0.6346 - val_acc: 0.8768
Epoch 68/500
290s - loss: 0.2916 - acc: 0.9068 - val_loss: 0.6339 - val_acc: 0.8737
Epoch 69/500
289s - loss: 0.2895 - acc: 0.9124 - val_loss: 0.6389 - val_acc: 0.8779
Epoch 70/500
290s - loss: 0.2573 - acc: 0.9118 - val_loss: 0.6405 - val_acc: 0.8747
Epoch 71/500
289s - loss: 0.2753 - acc: 0.9139 - val_loss: 0.6353 - val_acc: 0.8758
Epoch 72/500
288s - loss: 0.2652 - acc: 0.9166 - val_loss: 0.6482 - val_acc: 0.8758
Epoch 73/500
289s - loss: 0.2856 - acc: 0.9071 - val_loss: 0.6387 - val_acc: 0.8779
Epoch 74/500
289s - loss: 0.2609 - acc: 0.9155 - val_loss: 0.6420 - val_acc: 0.8842
Epoch 75/500
289s - loss: 0.2732 - acc: 0.9124 - val_loss: 0.6446 - val_acc: 0.8768
Epoch 76/500
288s - loss: 0.2786 - acc: 0.9132 - val_loss: 0.6442 - val_acc: 0.8758
Epoch 77/500
288s - loss: 0.2764 - acc: 0.9132 - val_loss: 0.6457 - val_acc: 0.8779
Epoch 78/500
288s - loss: 0.2646 - acc: 0.9084 - val_loss: 0.6475 - val_acc: 0.8758
Epoch 79/500
288s - loss: 0.2491 - acc: 0.9161 - val_loss: 0.6543 - val_acc: 0.8779
Epoch 80/500
288s - loss: 0.2769 - acc: 0.9142 - val_loss: 0.6495 - val_acc: 0.8779
Epoch 81/500
288s - loss: 0.2675 - acc: 0.9121 - val_loss: 0.6506 - val_acc: 0.8768
Epoch 82/500
288s - loss: 0.2892 - acc: 0.9118 - val_loss: 0.6506 - val_acc: 0.8758
Epoch 83/500

Epoch 00082: reducing learning rate to 9.99999931082e-05.
290s - loss: 0.2614 - acc: 0.9134 - val_loss: 0.6561 - val_acc: 0.8747
Epoch 84/500
289s - loss: 0.2662 - acc: 0.9168 - val_loss: 0.6550 - val_acc: 0.8747
Epoch 85/500
289s - loss: 0.2901 - acc: 0.9129 - val_loss: 0.6515 - val_acc: 0.8758
Epoch 86/500
288s - loss: 0.2807 - acc: 0.9113 - val_loss: 0.6528 - val_acc: 0.8747
Epoch 87/500
288s - loss: 0.2615 - acc: 0.9134 - val_loss: 0.6523 - val_acc: 0.8768
Epoch 88/500
288s - loss: 0.2679 - acc: 0.9158 - val_loss: 0.6481 - val_acc: 0.8758
Epoch 89/500
289s - loss: 0.2499 - acc: 0.9147 - val_loss: 0.6479 - val_acc: 0.8768
Epoch 90/500
289s - loss: 0.2884 - acc: 0.9079 - val_loss: 0.6569 - val_acc: 0.8747
Epoch 91/500

Epoch 00090: reducing learning rate to 9.99999901978e-06.
289s - loss: 0.2500 - acc: 0.9197 - val_loss: 0.6502 - val_acc: 0.8768
Epoch 92/500
289s - loss: 0.2449 - acc: 0.9150 - val_loss: 0.6528 - val_acc: 0.8747
Epoch 93/500
289s - loss: 0.2896 - acc: 0.9068 - val_loss: 0.6562 - val_acc: 0.8758
Epoch 94/500
289s - loss: 0.2704 - acc: 0.9134 - val_loss: 0.6585 - val_acc: 0.8768
Epoch 95/500
289s - loss: 0.2574 - acc: 0.9171 - val_loss: 0.6483 - val_acc: 0.8768
Epoch 96/500
290s - loss: 0.2601 - acc: 0.9121 - val_loss: 0.6489 - val_acc: 0.8747
Epoch 97/500
290s - loss: 0.2423 - acc: 0.9142 - val_loss: 0.6523 - val_acc: 0.8758
Epoch 98/500
289s - loss: 0.2696 - acc: 0.9126 - val_loss: 0.6526 - val_acc: 0.8758
Epoch 99/500

Epoch 00098: reducing learning rate to 1e-06.
289s - loss: 0.2495 - acc: 0.9161 - val_loss: 0.6547 - val_acc: 0.8747
Epoch 100/500
290s - loss: 0.2693 - acc: 0.9126 - val_loss: 0.6533 - val_acc: 0.8758
Epoch 101/500
288s - loss: 0.2615 - acc: 0.9182 - val_loss: 0.6543 - val_acc: 0.8768
Epoch 102/500
289s - loss: 0.2688 - acc: 0.9118 - val_loss: 0.6528 - val_acc: 0.8768
Epoch 103/500
288s - loss: 0.2676 - acc: 0.9189 - val_loss: 0.6531 - val_acc: 0.8747
Epoch 104/500
288s - loss: 0.2568 - acc: 0.9137 - val_loss: 0.6508 - val_acc: 0.8758
Epoch 105/500
288s - loss: 0.2572 - acc: 0.9129 - val_loss: 0.6521 - val_acc: 0.8758
Epoch 106/500
288s - loss: 0.2537 - acc: 0.9211 - val_loss: 0.6524 - val_acc: 0.8758
Epoch 107/500
288s - loss: 0.2618 - acc: 0.9153 - val_loss: 0.6499 - val_acc: 0.8758
Training loss for fold 0 is 0.12107949730964672 with percent 95.94736840850429
Testing loss for fold 0 is 0.6419737953888742 with percent 88.42105258138557
 
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_3 (InputLayer)             (None, 85, 85, 3)     0                                            
____________________________________________________________________________________________________
conv2d_7 (Conv2D)                (None, 83, 83, 16)    448         input_3[0][0]                    
____________________________________________________________________________________________________
batch_normalization_8 (BatchNorm (None, 83, 83, 16)    64          conv2d_7[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_8 (LeakyReLU)        (None, 83, 83, 16)    0           batch_normalization_8[0][0]      
____________________________________________________________________________________________________
conv2d_8 (Conv2D)                (None, 82, 82, 16)    1040        leaky_re_lu_8[0][0]              
____________________________________________________________________________________________________
batch_normalization_9 (BatchNorm (None, 82, 82, 16)    64          conv2d_8[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_9 (LeakyReLU)        (None, 82, 82, 16)    0           batch_normalization_9[0][0]      
____________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)   (None, 41, 41, 16)    0           leaky_re_lu_9[0][0]              
____________________________________________________________________________________________________
dropout_6 (Dropout)              (None, 41, 41, 16)    0           max_pooling2d_4[0][0]            
____________________________________________________________________________________________________
conv2d_9 (Conv2D)                (None, 39, 39, 32)    4640        dropout_6[0][0]                  
____________________________________________________________________________________________________
batch_normalization_10 (BatchNor (None, 39, 39, 32)    128         conv2d_9[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_10 (LeakyReLU)       (None, 39, 39, 32)    0           batch_normalization_10[0][0]     
____________________________________________________________________________________________________
conv2d_10 (Conv2D)               (None, 38, 38, 32)    4128        leaky_re_lu_10[0][0]             
____________________________________________________________________________________________________
batch_normalization_11 (BatchNor (None, 38, 38, 32)    128         conv2d_10[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_11 (LeakyReLU)       (None, 38, 38, 32)    0           batch_normalization_11[0][0]     
____________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)   (None, 19, 19, 32)    0           leaky_re_lu_11[0][0]             
____________________________________________________________________________________________________
dropout_7 (Dropout)              (None, 19, 19, 32)    0           max_pooling2d_5[0][0]            
____________________________________________________________________________________________________
conv2d_11 (Conv2D)               (None, 17, 17, 64)    18496       dropout_7[0][0]                  
____________________________________________________________________________________________________
batch_normalization_12 (BatchNor (None, 17, 17, 64)    256         conv2d_11[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_12 (LeakyReLU)       (None, 17, 17, 64)    0           batch_normalization_12[0][0]     
____________________________________________________________________________________________________
conv2d_12 (Conv2D)               (None, 16, 16, 64)    16448       leaky_re_lu_12[0][0]             
____________________________________________________________________________________________________
batch_normalization_13 (BatchNor (None, 16, 16, 64)    256         conv2d_12[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_13 (LeakyReLU)       (None, 16, 16, 64)    0           batch_normalization_13[0][0]     
____________________________________________________________________________________________________
max_pooling2d_6 (MaxPooling2D)   (None, 8, 8, 64)      0           leaky_re_lu_13[0][0]             
____________________________________________________________________________________________________
dropout_8 (Dropout)              (None, 8, 8, 64)      0           max_pooling2d_6[0][0]            
____________________________________________________________________________________________________
flatten_2 (Flatten)              (None, 4096)          0           dropout_8[0][0]                  
____________________________________________________________________________________________________
batch_normalization_14 (BatchNor (None, 4096)          16384       flatten_2[0][0]                  
____________________________________________________________________________________________________
input_4 (InputLayer)             (None, 102)           0                                            
____________________________________________________________________________________________________
concatenate_2 (Concatenate)      (None, 4198)          0           batch_normalization_14[0][0]     
                                                                   input_4[0][0]                    
____________________________________________________________________________________________________
dense_5 (Dense)                  (None, 128)           537472      concatenate_2[0][0]              
____________________________________________________________________________________________________
leaky_re_lu_14 (LeakyReLU)       (None, 128)           0           dense_5[0][0]                    
____________________________________________________________________________________________________
dense_6 (Dense)                  (None, 64)            8256        leaky_re_lu_14[0][0]             
____________________________________________________________________________________________________
dropout_9 (Dropout)              (None, 64)            0           dense_6[0][0]                    
____________________________________________________________________________________________________
dense_7 (Dense)                  (None, 32)            2080        dropout_9[0][0]                  
____________________________________________________________________________________________________
dropout_10 (Dropout)             (None, 32)            0           dense_7[0][0]                    
____________________________________________________________________________________________________
dense_8 (Dense)                  (None, 12)            396         dropout_10[0][0]                 
====================================================================================================
Total params: 610,684
Trainable params: 602,044
Non-trainable params: 8,640
____________________________________________________________________________________________________
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
292s - loss: 2.0036 - acc: 0.3366 - val_loss: 3.5000 - val_acc: 0.1263
Epoch 2/500
289s - loss: 1.5592 - acc: 0.4861 - val_loss: 2.0260 - val_acc: 0.4137
Epoch 3/500
287s - loss: 1.4465 - acc: 0.5376 - val_loss: 1.9195 - val_acc: 0.5295
Epoch 4/500
287s - loss: 1.3725 - acc: 0.5679 - val_loss: 1.1646 - val_acc: 0.6484
Epoch 5/500
287s - loss: 1.2907 - acc: 0.6211 - val_loss: 3.6522 - val_acc: 0.2800
Epoch 6/500
287s - loss: 1.2482 - acc: 0.6305 - val_loss: 2.7932 - val_acc: 0.3232
Epoch 7/500
287s - loss: 1.2277 - acc: 0.6384 - val_loss: 1.2922 - val_acc: 0.6168
Epoch 8/500
287s - loss: 1.1866 - acc: 0.6545 - val_loss: 1.4265 - val_acc: 0.6147
Epoch 9/500
287s - loss: 1.1853 - acc: 0.6729 - val_loss: 6.3384 - val_acc: 0.1632
Epoch 10/500
287s - loss: 1.1355 - acc: 0.6753 - val_loss: 1.2737 - val_acc: 0.6453
Epoch 11/500
288s - loss: 1.1264 - acc: 0.6905 - val_loss: 1.2598 - val_acc: 0.7011
Epoch 12/500
287s - loss: 1.0180 - acc: 0.7153 - val_loss: 1.4490 - val_acc: 0.6642
Epoch 13/500
287s - loss: 0.9939 - acc: 0.7221 - val_loss: 3.0860 - val_acc: 0.4568
Epoch 14/500
287s - loss: 1.0595 - acc: 0.7361 - val_loss: 1.2773 - val_acc: 0.6737
Epoch 15/500
287s - loss: 1.0848 - acc: 0.7071 - val_loss: 1.0299 - val_acc: 0.7611
Epoch 16/500
287s - loss: 1.0408 - acc: 0.7279 - val_loss: 1.3771 - val_acc: 0.6695
Epoch 17/500
287s - loss: 1.0667 - acc: 0.7329 - val_loss: 2.5304 - val_acc: 0.5589
Epoch 18/500
287s - loss: 0.9245 - acc: 0.7616 - val_loss: 2.0231 - val_acc: 0.6368
Epoch 19/500
287s - loss: 1.0595 - acc: 0.7405 - val_loss: 2.2609 - val_acc: 0.5179
Epoch 20/500
287s - loss: 1.0327 - acc: 0.7468 - val_loss: 1.9366 - val_acc: 0.6032
Epoch 21/500
287s - loss: 1.0516 - acc: 0.7258 - val_loss: 1.5054 - val_acc: 0.6832
Epoch 22/500
287s - loss: 1.0316 - acc: 0.7411 - val_loss: 2.3297 - val_acc: 0.6032
Epoch 23/500
287s - loss: 1.1942 - acc: 0.7163 - val_loss: 6.2141 - val_acc: 0.2663
Epoch 24/500

Epoch 00023: reducing learning rate to 0.010000000149.
288s - loss: 1.1880 - acc: 0.7124 - val_loss: 1.5144 - val_acc: 0.6821
Epoch 25/500
287s - loss: 1.0027 - acc: 0.7529 - val_loss: 1.0351 - val_acc: 0.7653
Epoch 26/500
287s - loss: 0.8059 - acc: 0.7918 - val_loss: 0.9321 - val_acc: 0.7958
Epoch 27/500
287s - loss: 0.7477 - acc: 0.8097 - val_loss: 0.8234 - val_acc: 0.8116
Epoch 28/500
287s - loss: 0.7024 - acc: 0.8137 - val_loss: 0.8449 - val_acc: 0.8074
Epoch 29/500
287s - loss: 0.6316 - acc: 0.8287 - val_loss: 0.9137 - val_acc: 0.7989
Epoch 30/500
288s - loss: 0.5951 - acc: 0.8384 - val_loss: 0.8366 - val_acc: 0.8242
Epoch 31/500
287s - loss: 0.6071 - acc: 0.8374 - val_loss: 1.0154 - val_acc: 0.7832
Epoch 32/500
287s - loss: 0.5895 - acc: 0.8434 - val_loss: 0.8198 - val_acc: 0.8253
Epoch 33/500
287s - loss: 0.5627 - acc: 0.8505 - val_loss: 0.8651 - val_acc: 0.8116
Epoch 34/500
287s - loss: 0.5789 - acc: 0.8505 - val_loss: 0.8439 - val_acc: 0.8284
Epoch 35/500
287s - loss: 0.5508 - acc: 0.8547 - val_loss: 0.7737 - val_acc: 0.8211
Epoch 36/500
288s - loss: 0.5304 - acc: 0.8558 - val_loss: 0.8086 - val_acc: 0.8305
Epoch 37/500
287s - loss: 0.5422 - acc: 0.8566 - val_loss: 0.9069 - val_acc: 0.8116
Epoch 38/500
287s - loss: 0.5140 - acc: 0.8647 - val_loss: 0.8706 - val_acc: 0.8221
Epoch 39/500
287s - loss: 0.5072 - acc: 0.8676 - val_loss: 0.8867 - val_acc: 0.8200
Epoch 40/500
287s - loss: 0.5064 - acc: 0.8687 - val_loss: 1.0710 - val_acc: 0.8042
Epoch 41/500
287s - loss: 0.4795 - acc: 0.8700 - val_loss: 0.8395 - val_acc: 0.8284
Epoch 42/500
287s - loss: 0.4742 - acc: 0.8700 - val_loss: 0.8257 - val_acc: 0.8347
Epoch 43/500
287s - loss: 0.4682 - acc: 0.8755 - val_loss: 0.8402 - val_acc: 0.8305
Epoch 44/500
287s - loss: 0.4950 - acc: 0.8674 - val_loss: 0.8384 - val_acc: 0.8326
Epoch 45/500
287s - loss: 0.4484 - acc: 0.8776 - val_loss: 0.8357 - val_acc: 0.8400
Epoch 46/500
287s - loss: 0.4615 - acc: 0.8813 - val_loss: 0.8459 - val_acc: 0.8389
Epoch 47/500
287s - loss: 0.4247 - acc: 0.8837 - val_loss: 0.8899 - val_acc: 0.8263
Epoch 48/500
287s - loss: 0.4307 - acc: 0.8826 - val_loss: 0.9393 - val_acc: 0.8116
Epoch 49/500
287s - loss: 0.4515 - acc: 0.8755 - val_loss: 0.8803 - val_acc: 0.8295
Epoch 50/500
287s - loss: 0.4299 - acc: 0.8837 - val_loss: 0.9313 - val_acc: 0.8326
Epoch 51/500
287s - loss: 0.4134 - acc: 0.8874 - val_loss: 0.8324 - val_acc: 0.8316
Epoch 52/500
287s - loss: 0.4276 - acc: 0.8803 - val_loss: 0.8319 - val_acc: 0.8358
Epoch 53/500
287s - loss: 0.4297 - acc: 0.8842 - val_loss: 1.0265 - val_acc: 0.7947
Epoch 54/500

Epoch 00053: reducing learning rate to 0.000999999977648.
287s - loss: 0.3989 - acc: 0.8939 - val_loss: 1.0510 - val_acc: 0.8253
Epoch 55/500
287s - loss: 0.3953 - acc: 0.8903 - val_loss: 0.8930 - val_acc: 0.8368
Epoch 56/500
287s - loss: 0.3905 - acc: 0.8950 - val_loss: 0.8569 - val_acc: 0.8421
Epoch 57/500
287s - loss: 0.3814 - acc: 0.8955 - val_loss: 0.8280 - val_acc: 0.8432
Epoch 58/500
288s - loss: 0.4153 - acc: 0.8924 - val_loss: 0.8308 - val_acc: 0.8453
Epoch 59/500
287s - loss: 0.4016 - acc: 0.8892 - val_loss: 0.8169 - val_acc: 0.8400
Epoch 60/500
287s - loss: 0.3910 - acc: 0.8913 - val_loss: 0.8139 - val_acc: 0.8379
Epoch 61/500
287s - loss: 0.3949 - acc: 0.8924 - val_loss: 0.8110 - val_acc: 0.8432
Epoch 62/500
287s - loss: 0.3777 - acc: 0.8971 - val_loss: 0.8167 - val_acc: 0.8453
Epoch 63/500
287s - loss: 0.3751 - acc: 0.8950 - val_loss: 0.8136 - val_acc: 0.8432
Epoch 64/500
287s - loss: 0.3519 - acc: 0.9024 - val_loss: 0.8032 - val_acc: 0.8442
Epoch 65/500
288s - loss: 0.4017 - acc: 0.8889 - val_loss: 0.7927 - val_acc: 0.8463
Epoch 66/500
287s - loss: 0.3660 - acc: 0.8974 - val_loss: 0.8016 - val_acc: 0.8495
Epoch 67/500
287s - loss: 0.3778 - acc: 0.8947 - val_loss: 0.7872 - val_acc: 0.8474
Epoch 68/500
287s - loss: 0.3576 - acc: 0.8926 - val_loss: 0.7956 - val_acc: 0.8484
Epoch 69/500
287s - loss: 0.3729 - acc: 0.8945 - val_loss: 0.8021 - val_acc: 0.8474
Epoch 70/500
287s - loss: 0.3795 - acc: 0.8968 - val_loss: 0.7856 - val_acc: 0.8505
Epoch 71/500
287s - loss: 0.3785 - acc: 0.8884 - val_loss: 0.8217 - val_acc: 0.8505
Epoch 72/500
287s - loss: 0.3964 - acc: 0.8908 - val_loss: 0.8014 - val_acc: 0.8453
Epoch 73/500
287s - loss: 0.3780 - acc: 0.8966 - val_loss: 0.8261 - val_acc: 0.8442
Epoch 74/500
287s - loss: 0.3658 - acc: 0.8971 - val_loss: 0.8017 - val_acc: 0.8526
Epoch 75/500
287s - loss: 0.3626 - acc: 0.8982 - val_loss: 0.8034 - val_acc: 0.8432
Epoch 76/500
287s - loss: 0.3538 - acc: 0.8997 - val_loss: 0.7971 - val_acc: 0.8463
Epoch 77/500
287s - loss: 0.3599 - acc: 0.8950 - val_loss: 0.8158 - val_acc: 0.8442
Epoch 78/500
287s - loss: 0.3639 - acc: 0.8984 - val_loss: 0.7942 - val_acc: 0.8526
Epoch 79/500
287s - loss: 0.3580 - acc: 0.9000 - val_loss: 0.8081 - val_acc: 0.8463
Epoch 80/500
287s - loss: 0.3498 - acc: 0.8997 - val_loss: 0.7847 - val_acc: 0.8526
Epoch 81/500
287s - loss: 0.3751 - acc: 0.8945 - val_loss: 0.7888 - val_acc: 0.8526
Epoch 82/500
287s - loss: 0.3635 - acc: 0.9016 - val_loss: 0.7869 - val_acc: 0.8495
Epoch 83/500
287s - loss: 0.3643 - acc: 0.8971 - val_loss: 0.7789 - val_acc: 0.8537
Epoch 84/500
287s - loss: 0.3483 - acc: 0.8997 - val_loss: 0.7705 - val_acc: 0.8526
Epoch 85/500
287s - loss: 0.3588 - acc: 0.9024 - val_loss: 0.7723 - val_acc: 0.8537
Epoch 86/500
287s - loss: 0.3451 - acc: 0.9032 - val_loss: 0.7979 - val_acc: 0.8505
Epoch 87/500
287s - loss: 0.3399 - acc: 0.9024 - val_loss: 0.7948 - val_acc: 0.8505
Epoch 88/500
287s - loss: 0.3690 - acc: 0.8989 - val_loss: 0.7821 - val_acc: 0.8495
Epoch 89/500
287s - loss: 0.3706 - acc: 0.8942 - val_loss: 0.8097 - val_acc: 0.8474
Epoch 90/500
287s - loss: 0.3564 - acc: 0.9003 - val_loss: 0.8065 - val_acc: 0.8484
Epoch 91/500
287s - loss: 0.3446 - acc: 0.9021 - val_loss: 0.7867 - val_acc: 0.8568
Epoch 92/500
287s - loss: 0.3710 - acc: 0.8997 - val_loss: 0.7871 - val_acc: 0.8505
Epoch 93/500
287s - loss: 0.3440 - acc: 0.9013 - val_loss: 0.7708 - val_acc: 0.8537
Epoch 94/500
287s - loss: 0.3316 - acc: 0.9029 - val_loss: 0.7801 - val_acc: 0.8547
Epoch 95/500
287s - loss: 0.3418 - acc: 0.9008 - val_loss: 0.7812 - val_acc: 0.8537
Epoch 96/500
287s - loss: 0.3652 - acc: 0.9016 - val_loss: 0.7697 - val_acc: 0.8558
Epoch 97/500
287s - loss: 0.3761 - acc: 0.8979 - val_loss: 0.7854 - val_acc: 0.8547
Epoch 98/500
287s - loss: 0.3738 - acc: 0.9047 - val_loss: 0.8017 - val_acc: 0.8463
Epoch 99/500
288s - loss: 0.3490 - acc: 0.9000 - val_loss: 0.7703 - val_acc: 0.8547
Epoch 100/500

Epoch 00099: reducing learning rate to 9.99999931082e-05.
287s - loss: 0.3706 - acc: 0.8997 - val_loss: 0.7732 - val_acc: 0.8568
Epoch 101/500
287s - loss: 0.3725 - acc: 0.8953 - val_loss: 0.7687 - val_acc: 0.8547
Epoch 102/500
287s - loss: 0.3535 - acc: 0.8997 - val_loss: 0.7707 - val_acc: 0.8579
Epoch 103/500
287s - loss: 0.3497 - acc: 0.9005 - val_loss: 0.7728 - val_acc: 0.8537
Epoch 104/500
287s - loss: 0.3385 - acc: 0.9013 - val_loss: 0.7741 - val_acc: 0.8579
Epoch 105/500
288s - loss: 0.3325 - acc: 0.9029 - val_loss: 0.7665 - val_acc: 0.8579
Epoch 106/500
287s - loss: 0.3492 - acc: 0.9024 - val_loss: 0.7633 - val_acc: 0.8558
Epoch 107/500
287s - loss: 0.3288 - acc: 0.9066 - val_loss: 0.7630 - val_acc: 0.8558
Epoch 108/500
287s - loss: 0.3511 - acc: 0.9013 - val_loss: 0.7691 - val_acc: 0.8568
Epoch 109/500
287s - loss: 0.3790 - acc: 0.8942 - val_loss: 0.7640 - val_acc: 0.8579
Epoch 110/500
287s - loss: 0.3328 - acc: 0.9066 - val_loss: 0.7646 - val_acc: 0.8537
Epoch 111/500
287s - loss: 0.3315 - acc: 0.9042 - val_loss: 0.7606 - val_acc: 0.8600
Epoch 112/500
287s - loss: 0.3530 - acc: 0.9026 - val_loss: 0.7675 - val_acc: 0.8589
Epoch 113/500
287s - loss: 0.3467 - acc: 0.9042 - val_loss: 0.7671 - val_acc: 0.8589
Epoch 114/500
287s - loss: 0.3608 - acc: 0.8992 - val_loss: 0.7669 - val_acc: 0.8589
Epoch 115/500
287s - loss: 0.3549 - acc: 0.9037 - val_loss: 0.7656 - val_acc: 0.8558
Epoch 116/500
287s - loss: 0.3304 - acc: 0.9045 - val_loss: 0.7621 - val_acc: 0.8558
Epoch 117/500
288s - loss: 0.3311 - acc: 0.9050 - val_loss: 0.7647 - val_acc: 0.8579
Epoch 118/500
287s - loss: 0.3579 - acc: 0.9013 - val_loss: 0.7620 - val_acc: 0.8600
Epoch 119/500
287s - loss: 0.3424 - acc: 0.9016 - val_loss: 0.7691 - val_acc: 0.8558
Epoch 120/500

Epoch 00119: reducing learning rate to 9.99999901978e-06.
287s - loss: 0.3432 - acc: 0.8984 - val_loss: 0.7647 - val_acc: 0.8579
Epoch 121/500
287s - loss: 0.3640 - acc: 0.9024 - val_loss: 0.7615 - val_acc: 0.8589
Epoch 122/500
287s - loss: 0.3578 - acc: 0.9032 - val_loss: 0.7621 - val_acc: 0.8589
Epoch 123/500
287s - loss: 0.3545 - acc: 0.9024 - val_loss: 0.7631 - val_acc: 0.8600
Epoch 124/500
287s - loss: 0.3351 - acc: 0.9037 - val_loss: 0.7675 - val_acc: 0.8589
Epoch 125/500
287s - loss: 0.3516 - acc: 0.8984 - val_loss: 0.7708 - val_acc: 0.8568
Epoch 126/500
287s - loss: 0.3549 - acc: 0.8992 - val_loss: 0.7650 - val_acc: 0.8589
Epoch 127/500
287s - loss: 0.3396 - acc: 0.9061 - val_loss: 0.7641 - val_acc: 0.8568
Epoch 128/500
287s - loss: 0.3778 - acc: 0.8947 - val_loss: 0.7652 - val_acc: 0.8611
Epoch 129/500
287s - loss: 0.3407 - acc: 0.9042 - val_loss: 0.7625 - val_acc: 0.8579
Epoch 130/500
287s - loss: 0.3469 - acc: 0.9050 - val_loss: 0.7684 - val_acc: 0.8558
Epoch 131/500
287s - loss: 0.3465 - acc: 0.9021 - val_loss: 0.7676 - val_acc: 0.8568
Epoch 132/500
287s - loss: 0.3574 - acc: 0.9008 - val_loss: 0.7621 - val_acc: 0.8568
Epoch 133/500
287s - loss: 0.3297 - acc: 0.9066 - val_loss: 0.7636 - val_acc: 0.8589
Epoch 134/500
287s - loss: 0.3294 - acc: 0.9050 - val_loss: 0.7675 - val_acc: 0.8537
Epoch 135/500
287s - loss: 0.3601 - acc: 0.9042 - val_loss: 0.7636 - val_acc: 0.8579
Epoch 136/500
287s - loss: 0.3369 - acc: 0.9037 - val_loss: 0.7693 - val_acc: 0.8568
Epoch 137/500

Epoch 00136: reducing learning rate to 1e-06.
287s - loss: 0.3403 - acc: 0.8992 - val_loss: 0.7641 - val_acc: 0.8600
Epoch 138/500
287s - loss: 0.3305 - acc: 0.9013 - val_loss: 0.7655 - val_acc: 0.8589
Epoch 139/500
287s - loss: 0.3276 - acc: 0.9032 - val_loss: 0.7690 - val_acc: 0.8558
Epoch 140/500
287s - loss: 0.3706 - acc: 0.8987 - val_loss: 0.7640 - val_acc: 0.8579
Epoch 141/500
287s - loss: 0.3396 - acc: 0.9053 - val_loss: 0.7695 - val_acc: 0.8558
Epoch 142/500
287s - loss: 0.3583 - acc: 0.8989 - val_loss: 0.7698 - val_acc: 0.8579
Epoch 143/500
287s - loss: 0.3578 - acc: 0.9008 - val_loss: 0.7719 - val_acc: 0.8558
Epoch 144/500
287s - loss: 0.3294 - acc: 0.9039 - val_loss: 0.7692 - val_acc: 0.8558
Epoch 145/500
287s - loss: 0.3560 - acc: 0.8963 - val_loss: 0.7697 - val_acc: 0.8589
Epoch 146/500
288s - loss: 0.3449 - acc: 0.9011 - val_loss: 0.7660 - val_acc: 0.8579
Epoch 147/500
287s - loss: 0.3676 - acc: 0.8921 - val_loss: 0.7615 - val_acc: 0.8579
Epoch 148/500
287s - loss: 0.3407 - acc: 0.9032 - val_loss: 0.7660 - val_acc: 0.8579
Epoch 149/500
287s - loss: 0.3379 - acc: 0.9050 - val_loss: 0.7709 - val_acc: 0.8568
Epoch 150/500
287s - loss: 0.3458 - acc: 0.9045 - val_loss: 0.7644 - val_acc: 0.8558
Epoch 151/500
287s - loss: 0.3407 - acc: 0.9018 - val_loss: 0.7639 - val_acc: 0.8537
Epoch 152/500
287s - loss: 0.3466 - acc: 0.8984 - val_loss: 0.7625 - val_acc: 0.8579
Epoch 153/500
287s - loss: 0.3593 - acc: 0.9058 - val_loss: 0.7621 - val_acc: 0.8589
Epoch 154/500
287s - loss: 0.3360 - acc: 0.9026 - val_loss: 0.7705 - val_acc: 0.8579
Epoch 155/500
287s - loss: 0.3333 - acc: 0.9045 - val_loss: 0.7713 - val_acc: 0.8568
Epoch 156/500
287s - loss: 0.3563 - acc: 0.9011 - val_loss: 0.7673 - val_acc: 0.8547
Epoch 157/500
287s - loss: 0.3601 - acc: 0.8955 - val_loss: 0.7680 - val_acc: 0.8568
Epoch 158/500
287s - loss: 0.3471 - acc: 0.8997 - val_loss: 0.7679 - val_acc: 0.8568
Epoch 159/500
287s - loss: 0.3595 - acc: 0.8979 - val_loss: 0.7656 - val_acc: 0.8558
Epoch 160/500
287s - loss: 0.3502 - acc: 0.9029 - val_loss: 0.7653 - val_acc: 0.8589
Epoch 161/500
287s - loss: 0.3671 - acc: 0.8947 - val_loss: 0.7676 - val_acc: 0.8558
Epoch 162/500
287s - loss: 0.3565 - acc: 0.9024 - val_loss: 0.7683 - val_acc: 0.8558
Epoch 163/500
287s - loss: 0.3534 - acc: 0.9037 - val_loss: 0.7719 - val_acc: 0.8505
Epoch 164/500
287s - loss: 0.3246 - acc: 0.9047 - val_loss: 0.7686 - val_acc: 0.8558
Epoch 165/500
287s - loss: 0.3323 - acc: 0.9037 - val_loss: 0.7653 - val_acc: 0.8589
Epoch 166/500
287s - loss: 0.3295 - acc: 0.9079 - val_loss: 0.7679 - val_acc: 0.8589
Epoch 167/500
287s - loss: 0.3532 - acc: 0.8963 - val_loss: 0.7602 - val_acc: 0.8589
Epoch 168/500
288s - loss: 0.3548 - acc: 0.9037 - val_loss: 0.7627 - val_acc: 0.8547
Epoch 169/500
287s - loss: 0.3339 - acc: 0.9039 - val_loss: 0.7648 - val_acc: 0.8579
Epoch 170/500
287s - loss: 0.3471 - acc: 0.9050 - val_loss: 0.7679 - val_acc: 0.8547
Epoch 171/500
287s - loss: 0.3345 - acc: 0.9074 - val_loss: 0.7662 - val_acc: 0.8579
Epoch 172/500
287s - loss: 0.3222 - acc: 0.9111 - val_loss: 0.7704 - val_acc: 0.8568
Epoch 173/500
287s - loss: 0.3611 - acc: 0.9029 - val_loss: 0.7701 - val_acc: 0.8558
Epoch 174/500
287s - loss: 0.3425 - acc: 0.8992 - val_loss: 0.7679 - val_acc: 0.8568
Epoch 175/500
287s - loss: 0.3450 - acc: 0.9047 - val_loss: 0.7685 - val_acc: 0.8547
Epoch 176/500
287s - loss: 0.3710 - acc: 0.8963 - val_loss: 0.7672 - val_acc: 0.8568
Epoch 177/500
287s - loss: 0.3403 - acc: 0.9018 - val_loss: 0.7657 - val_acc: 0.8589
Epoch 178/500
287s - loss: 0.3544 - acc: 0.8984 - val_loss: 0.7690 - val_acc: 0.8526
Epoch 179/500
287s - loss: 0.3433 - acc: 0.9003 - val_loss: 0.7646 - val_acc: 0.8579
Epoch 180/500
287s - loss: 0.3392 - acc: 0.9013 - val_loss: 0.7742 - val_acc: 0.8558
Epoch 181/500
288s - loss: 0.3567 - acc: 0.9005 - val_loss: 0.7620 - val_acc: 0.8558
Epoch 182/500
287s - loss: 0.3332 - acc: 0.9047 - val_loss: 0.7657 - val_acc: 0.8579
Epoch 183/500
287s - loss: 0.3619 - acc: 0.8989 - val_loss: 0.7742 - val_acc: 0.8568
Epoch 184/500
287s - loss: 0.3500 - acc: 0.9013 - val_loss: 0.7700 - val_acc: 0.8568
Epoch 185/500
288s - loss: 0.3630 - acc: 0.8974 - val_loss: 0.7747 - val_acc: 0.8589
Epoch 186/500
287s - loss: 0.3507 - acc: 0.9000 - val_loss: 0.7690 - val_acc: 0.8579
Epoch 187/500
287s - loss: 0.3483 - acc: 0.9026 - val_loss: 0.7648 - val_acc: 0.8589
Epoch 188/500
287s - loss: 0.3416 - acc: 0.9068 - val_loss: 0.7629 - val_acc: 0.8589
Epoch 189/500
287s - loss: 0.3545 - acc: 0.9066 - val_loss: 0.7680 - val_acc: 0.8547
Epoch 190/500
287s - loss: 0.3401 - acc: 0.9037 - val_loss: 0.7688 - val_acc: 0.8568
Epoch 191/500
287s - loss: 0.3449 - acc: 0.8997 - val_loss: 0.7651 - val_acc: 0.8537
Epoch 192/500
287s - loss: 0.3467 - acc: 0.8979 - val_loss: 0.7687 - val_acc: 0.8558
Epoch 193/500
287s - loss: 0.3830 - acc: 0.8947 - val_loss: 0.7677 - val_acc: 0.8558
Epoch 194/500
287s - loss: 0.3403 - acc: 0.9047 - val_loss: 0.7697 - val_acc: 0.8579
Epoch 195/500
288s - loss: 0.3781 - acc: 0.8971 - val_loss: 0.7621 - val_acc: 0.8579
Epoch 196/500
287s - loss: 0.3362 - acc: 0.9021 - val_loss: 0.7699 - val_acc: 0.8537
Epoch 197/500
287s - loss: 0.3533 - acc: 0.9005 - val_loss: 0.7742 - val_acc: 0.8516
Epoch 198/500
287s - loss: 0.3335 - acc: 0.9045 - val_loss: 0.7721 - val_acc: 0.8526
Epoch 199/500
287s - loss: 0.3634 - acc: 0.8982 - val_loss: 0.7692 - val_acc: 0.8568
Epoch 200/500
287s - loss: 0.3787 - acc: 0.8984 - val_loss: 0.7691 - val_acc: 0.8558
Epoch 201/500
287s - loss: 0.3418 - acc: 0.9032 - val_loss: 0.7669 - val_acc: 0.8579
Epoch 202/500
287s - loss: 0.3645 - acc: 0.9011 - val_loss: 0.7636 - val_acc: 0.8600
Epoch 203/500
287s - loss: 0.3668 - acc: 0.8982 - val_loss: 0.7739 - val_acc: 0.8568
Epoch 204/500
287s - loss: 0.3578 - acc: 0.9008 - val_loss: 0.7703 - val_acc: 0.8568
Epoch 205/500
287s - loss: 0.3550 - acc: 0.8984 - val_loss: 0.7652 - val_acc: 0.8568
Epoch 206/500
287s - loss: 0.3653 - acc: 0.8987 - val_loss: 0.7646 - val_acc: 0.8568
Epoch 207/500
287s - loss: 0.3537 - acc: 0.9000 - val_loss: 0.7630 - val_acc: 0.8579
Epoch 208/500
287s - loss: 0.3699 - acc: 0.9003 - val_loss: 0.7645 - val_acc: 0.8558
Epoch 209/500
287s - loss: 0.3699 - acc: 0.8976 - val_loss: 0.7640 - val_acc: 0.8547
Epoch 210/500
287s - loss: 0.3520 - acc: 0.9029 - val_loss: 0.7683 - val_acc: 0.8568
Epoch 211/500
287s - loss: 0.3674 - acc: 0.8950 - val_loss: 0.7683 - val_acc: 0.8547
Epoch 212/500
287s - loss: 0.3496 - acc: 0.9032 - val_loss: 0.7622 - val_acc: 0.8568
Epoch 213/500
287s - loss: 0.3317 - acc: 0.9042 - val_loss: 0.7657 - val_acc: 0.8589
Epoch 214/500
287s - loss: 0.3529 - acc: 0.8974 - val_loss: 0.7682 - val_acc: 0.8579
Epoch 215/500
287s - loss: 0.3478 - acc: 0.9011 - val_loss: 0.7713 - val_acc: 0.8558
Epoch 216/500
287s - loss: 0.3548 - acc: 0.9013 - val_loss: 0.7705 - val_acc: 0.8579
Epoch 217/500
287s - loss: 0.3224 - acc: 0.9039 - val_loss: 0.7707 - val_acc: 0.8589
Epoch 218/500
288s - loss: 0.3523 - acc: 0.8997 - val_loss: 0.7699 - val_acc: 0.8579
Epoch 219/500
287s - loss: 0.3526 - acc: 0.9018 - val_loss: 0.7672 - val_acc: 0.8611
Epoch 220/500
287s - loss: 0.3487 - acc: 0.9047 - val_loss: 0.7656 - val_acc: 0.8600
Epoch 221/500
287s - loss: 0.3431 - acc: 0.9047 - val_loss: 0.7659 - val_acc: 0.8537
Epoch 222/500
287s - loss: 0.3449 - acc: 0.9053 - val_loss: 0.7634 - val_acc: 0.8558
Epoch 223/500
287s - loss: 0.3428 - acc: 0.9026 - val_loss: 0.7628 - val_acc: 0.8547
Epoch 224/500
287s - loss: 0.3760 - acc: 0.9013 - val_loss: 0.7657 - val_acc: 0.8589
Epoch 225/500
287s - loss: 0.3709 - acc: 0.9011 - val_loss: 0.7651 - val_acc: 0.8600
Epoch 226/500
287s - loss: 0.3582 - acc: 0.9016 - val_loss: 0.7667 - val_acc: 0.8579
Epoch 227/500
287s - loss: 0.3451 - acc: 0.9047 - val_loss: 0.7573 - val_acc: 0.8579
Epoch 228/500
287s - loss: 0.3304 - acc: 0.9029 - val_loss: 0.7632 - val_acc: 0.8589
Epoch 229/500
287s - loss: 0.3668 - acc: 0.8955 - val_loss: 0.7719 - val_acc: 0.8579
Epoch 230/500
287s - loss: 0.3594 - acc: 0.9034 - val_loss: 0.7701 - val_acc: 0.8579
Epoch 231/500
287s - loss: 0.3578 - acc: 0.9013 - val_loss: 0.7699 - val_acc: 0.8579
Epoch 232/500
287s - loss: 0.3560 - acc: 0.9000 - val_loss: 0.7651 - val_acc: 0.8579
Epoch 233/500
287s - loss: 0.3677 - acc: 0.8979 - val_loss: 0.7669 - val_acc: 0.8568
Epoch 234/500
287s - loss: 0.3740 - acc: 0.8979 - val_loss: 0.7700 - val_acc: 0.8547
Epoch 235/500
287s - loss: 0.3554 - acc: 0.9005 - val_loss: 0.7629 - val_acc: 0.8558
Epoch 236/500
287s - loss: 0.3488 - acc: 0.9029 - val_loss: 0.7675 - val_acc: 0.8568
Epoch 237/500
288s - loss: 0.3579 - acc: 0.8987 - val_loss: 0.7676 - val_acc: 0.8579
Epoch 238/500
287s - loss: 0.3571 - acc: 0.9011 - val_loss: 0.7688 - val_acc: 0.8579
Epoch 239/500
287s - loss: 0.3676 - acc: 0.8979 - val_loss: 0.7675 - val_acc: 0.8558
Epoch 240/500
288s - loss: 0.3511 - acc: 0.9008 - val_loss: 0.7680 - val_acc: 0.8600
Epoch 241/500
287s - loss: 0.3400 - acc: 0.9055 - val_loss: 0.7720 - val_acc: 0.8579
Epoch 242/500
287s - loss: 0.3555 - acc: 0.8950 - val_loss: 0.7705 - val_acc: 0.8547
Epoch 243/500
287s - loss: 0.3494 - acc: 0.9016 - val_loss: 0.7688 - val_acc: 0.8558
Epoch 244/500
287s - loss: 0.3561 - acc: 0.9021 - val_loss: 0.7671 - val_acc: 0.8600
Epoch 245/500
287s - loss: 0.3369 - acc: 0.8995 - val_loss: 0.7658 - val_acc: 0.8600
Epoch 246/500
287s - loss: 0.3224 - acc: 0.9063 - val_loss: 0.7686 - val_acc: 0.8558
Epoch 247/500
287s - loss: 0.3625 - acc: 0.9034 - val_loss: 0.7721 - val_acc: 0.8568
Epoch 248/500
287s - loss: 0.3308 - acc: 0.9037 - val_loss: 0.7667 - val_acc: 0.8568
Epoch 249/500
287s - loss: 0.3579 - acc: 0.9024 - val_loss: 0.7637 - val_acc: 0.8611
Epoch 250/500
287s - loss: 0.3578 - acc: 0.9032 - val_loss: 0.7685 - val_acc: 0.8558
Epoch 251/500
287s - loss: 0.3659 - acc: 0.8987 - val_loss: 0.7698 - val_acc: 0.8589
Epoch 252/500
287s - loss: 0.3768 - acc: 0.8976 - val_loss: 0.7645 - val_acc: 0.8589
Epoch 253/500
287s - loss: 0.3597 - acc: 0.9018 - val_loss: 0.7744 - val_acc: 0.8568
Epoch 254/500
287s - loss: 0.3279 - acc: 0.9026 - val_loss: 0.7714 - val_acc: 0.8547
Epoch 255/500
287s - loss: 0.3492 - acc: 0.9050 - val_loss: 0.7685 - val_acc: 0.8558
Epoch 256/500
287s - loss: 0.3480 - acc: 0.9021 - val_loss: 0.7726 - val_acc: 0.8589
Epoch 257/500
287s - loss: 0.3667 - acc: 0.8989 - val_loss: 0.7665 - val_acc: 0.8579
Epoch 258/500
287s - loss: 0.3683 - acc: 0.8966 - val_loss: 0.7657 - val_acc: 0.8568
Epoch 259/500
287s - loss: 0.3391 - acc: 0.9061 - val_loss: 0.7651 - val_acc: 0.8537
Epoch 260/500
287s - loss: 0.3957 - acc: 0.8963 - val_loss: 0.7678 - val_acc: 0.8537
Epoch 261/500
287s - loss: 0.3479 - acc: 0.9042 - val_loss: 0.7701 - val_acc: 0.8568
Epoch 262/500
287s - loss: 0.3475 - acc: 0.9005 - val_loss: 0.7704 - val_acc: 0.8537
Epoch 263/500
288s - loss: 0.3664 - acc: 0.8979 - val_loss: 0.7651 - val_acc: 0.8547
Epoch 264/500
287s - loss: 0.3471 - acc: 0.8997 - val_loss: 0.7592 - val_acc: 0.8579
Epoch 265/500
287s - loss: 0.3387 - acc: 0.9045 - val_loss: 0.7657 - val_acc: 0.8568
Epoch 266/500
287s - loss: 0.3586 - acc: 0.9024 - val_loss: 0.7665 - val_acc: 0.8579
Epoch 267/500
287s - loss: 0.3372 - acc: 0.9055 - val_loss: 0.7668 - val_acc: 0.8568
Epoch 268/500
287s - loss: 0.3383 - acc: 0.9066 - val_loss: 0.7635 - val_acc: 0.8568
Epoch 269/500
287s - loss: 0.3729 - acc: 0.8989 - val_loss: 0.7693 - val_acc: 0.8589
Epoch 270/500
287s - loss: 0.3644 - acc: 0.9018 - val_loss: 0.7669 - val_acc: 0.8558
Epoch 271/500
287s - loss: 0.3461 - acc: 0.9013 - val_loss: 0.7697 - val_acc: 0.8558
Epoch 272/500
287s - loss: 0.3642 - acc: 0.8995 - val_loss: 0.7679 - val_acc: 0.8600
Epoch 273/500
287s - loss: 0.3723 - acc: 0.8997 - val_loss: 0.7672 - val_acc: 0.8579
Epoch 274/500
287s - loss: 0.3497 - acc: 0.9029 - val_loss: 0.7688 - val_acc: 0.8579
Epoch 275/500
287s - loss: 0.3631 - acc: 0.8974 - val_loss: 0.7709 - val_acc: 0.8537
Epoch 276/500
287s - loss: 0.3703 - acc: 0.8971 - val_loss: 0.7697 - val_acc: 0.8579
Epoch 277/500
287s - loss: 0.3699 - acc: 0.8997 - val_loss: 0.7679 - val_acc: 0.8568
Epoch 278/500
287s - loss: 0.3383 - acc: 0.9047 - val_loss: 0.7738 - val_acc: 0.8516
Epoch 279/500
287s - loss: 0.3410 - acc: 0.9037 - val_loss: 0.7687 - val_acc: 0.8547
Epoch 280/500
287s - loss: 0.3240 - acc: 0.9055 - val_loss: 0.7710 - val_acc: 0.8589
Epoch 281/500
287s - loss: 0.3449 - acc: 0.9053 - val_loss: 0.7685 - val_acc: 0.8600
Epoch 282/500
287s - loss: 0.3359 - acc: 0.9016 - val_loss: 0.7662 - val_acc: 0.8589
Epoch 283/500
287s - loss: 0.3574 - acc: 0.8997 - val_loss: 0.7742 - val_acc: 0.8579
Epoch 284/500
287s - loss: 0.3535 - acc: 0.8995 - val_loss: 0.7733 - val_acc: 0.8600
Epoch 285/500
288s - loss: 0.3546 - acc: 0.9024 - val_loss: 0.7653 - val_acc: 0.8611
Epoch 286/500
288s - loss: 0.3630 - acc: 0.9037 - val_loss: 0.7689 - val_acc: 0.8589
Epoch 287/500
287s - loss: 0.3741 - acc: 0.8950 - val_loss: 0.7699 - val_acc: 0.8568
Epoch 288/500
287s - loss: 0.3424 - acc: 0.9032 - val_loss: 0.7688 - val_acc: 0.8537
Training loss for fold 1 is 0.21685948591483267 with percent 93.05263159149571
Testing loss for fold 1 is 0.7652301183499788 with percent 86.10526320808812
 
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_5 (InputLayer)             (None, 85, 85, 3)     0                                            
____________________________________________________________________________________________________
conv2d_13 (Conv2D)               (None, 83, 83, 16)    448         input_5[0][0]                    
____________________________________________________________________________________________________
batch_normalization_15 (BatchNor (None, 83, 83, 16)    64          conv2d_13[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_15 (LeakyReLU)       (None, 83, 83, 16)    0           batch_normalization_15[0][0]     
____________________________________________________________________________________________________
conv2d_14 (Conv2D)               (None, 82, 82, 16)    1040        leaky_re_lu_15[0][0]             
____________________________________________________________________________________________________
batch_normalization_16 (BatchNor (None, 82, 82, 16)    64          conv2d_14[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_16 (LeakyReLU)       (None, 82, 82, 16)    0           batch_normalization_16[0][0]     
____________________________________________________________________________________________________
max_pooling2d_7 (MaxPooling2D)   (None, 41, 41, 16)    0           leaky_re_lu_16[0][0]             
____________________________________________________________________________________________________
dropout_11 (Dropout)             (None, 41, 41, 16)    0           max_pooling2d_7[0][0]            
____________________________________________________________________________________________________
conv2d_15 (Conv2D)               (None, 39, 39, 32)    4640        dropout_11[0][0]                 
____________________________________________________________________________________________________
batch_normalization_17 (BatchNor (None, 39, 39, 32)    128         conv2d_15[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_17 (LeakyReLU)       (None, 39, 39, 32)    0           batch_normalization_17[0][0]     
____________________________________________________________________________________________________
conv2d_16 (Conv2D)               (None, 38, 38, 32)    4128        leaky_re_lu_17[0][0]             
____________________________________________________________________________________________________
batch_normalization_18 (BatchNor (None, 38, 38, 32)    128         conv2d_16[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_18 (LeakyReLU)       (None, 38, 38, 32)    0           batch_normalization_18[0][0]     
____________________________________________________________________________________________________
max_pooling2d_8 (MaxPooling2D)   (None, 19, 19, 32)    0           leaky_re_lu_18[0][0]             
____________________________________________________________________________________________________
dropout_12 (Dropout)             (None, 19, 19, 32)    0           max_pooling2d_8[0][0]            
____________________________________________________________________________________________________
conv2d_17 (Conv2D)               (None, 17, 17, 64)    18496       dropout_12[0][0]                 
____________________________________________________________________________________________________
batch_normalization_19 (BatchNor (None, 17, 17, 64)    256         conv2d_17[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_19 (LeakyReLU)       (None, 17, 17, 64)    0           batch_normalization_19[0][0]     
____________________________________________________________________________________________________
conv2d_18 (Conv2D)               (None, 16, 16, 64)    16448       leaky_re_lu_19[0][0]             
____________________________________________________________________________________________________
batch_normalization_20 (BatchNor (None, 16, 16, 64)    256         conv2d_18[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_20 (LeakyReLU)       (None, 16, 16, 64)    0           batch_normalization_20[0][0]     
____________________________________________________________________________________________________
max_pooling2d_9 (MaxPooling2D)   (None, 8, 8, 64)      0           leaky_re_lu_20[0][0]             
____________________________________________________________________________________________________
dropout_13 (Dropout)             (None, 8, 8, 64)      0           max_pooling2d_9[0][0]            
____________________________________________________________________________________________________
flatten_3 (Flatten)              (None, 4096)          0           dropout_13[0][0]                 
____________________________________________________________________________________________________
batch_normalization_21 (BatchNor (None, 4096)          16384       flatten_3[0][0]                  
____________________________________________________________________________________________________
input_6 (InputLayer)             (None, 102)           0                                            
____________________________________________________________________________________________________
concatenate_3 (Concatenate)      (None, 4198)          0           batch_normalization_21[0][0]     
                                                                   input_6[0][0]                    
____________________________________________________________________________________________________
dense_9 (Dense)                  (None, 128)           537472      concatenate_3[0][0]              
____________________________________________________________________________________________________
leaky_re_lu_21 (LeakyReLU)       (None, 128)           0           dense_9[0][0]                    
____________________________________________________________________________________________________
dense_10 (Dense)                 (None, 64)            8256        leaky_re_lu_21[0][0]             
____________________________________________________________________________________________________
dropout_14 (Dropout)             (None, 64)            0           dense_10[0][0]                   
____________________________________________________________________________________________________
dense_11 (Dense)                 (None, 32)            2080        dropout_14[0][0]                 
____________________________________________________________________________________________________
dropout_15 (Dropout)             (None, 32)            0           dense_11[0][0]                   
____________________________________________________________________________________________________
dense_12 (Dense)                 (None, 12)            396         dropout_15[0][0]                 
====================================================================================================
Total params: 610,684
Trainable params: 602,044
Non-trainable params: 8,640
____________________________________________________________________________________________________
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
292s - loss: 2.0410 - acc: 0.3289 - val_loss: 4.0427 - val_acc: 0.1789
Epoch 2/500
289s - loss: 1.6008 - acc: 0.4866 - val_loss: 1.9748 - val_acc: 0.3347
Epoch 3/500
289s - loss: 1.3898 - acc: 0.5611 - val_loss: 2.2148 - val_acc: 0.3968
Epoch 4/500
289s - loss: 1.3463 - acc: 0.5808 - val_loss: 1.2870 - val_acc: 0.6011
Epoch 5/500
289s - loss: 1.3070 - acc: 0.6153 - val_loss: 2.3392 - val_acc: 0.3611
Epoch 6/500
289s - loss: 1.2926 - acc: 0.6068 - val_loss: 1.4530 - val_acc: 0.5716
Epoch 7/500
289s - loss: 1.2267 - acc: 0.6376 - val_loss: 1.6096 - val_acc: 0.5905
Epoch 8/500
289s - loss: 1.1934 - acc: 0.6508 - val_loss: 2.0476 - val_acc: 0.4568
Epoch 9/500
289s - loss: 1.2200 - acc: 0.6611 - val_loss: 1.9938 - val_acc: 0.5000
Epoch 10/500
289s - loss: 1.1660 - acc: 0.6697 - val_loss: 1.7799 - val_acc: 0.5474
Epoch 11/500
289s - loss: 1.2162 - acc: 0.6650 - val_loss: 1.4281 - val_acc: 0.6253
Epoch 12/500
289s - loss: 1.2249 - acc: 0.6537 - val_loss: 2.1534 - val_acc: 0.4958
Epoch 13/500
289s - loss: 1.1790 - acc: 0.6779 - val_loss: 2.0288 - val_acc: 0.4789
Epoch 14/500
289s - loss: 1.2172 - acc: 0.6776 - val_loss: 2.4833 - val_acc: 0.5442
Epoch 15/500
289s - loss: 1.2353 - acc: 0.6882 - val_loss: 2.2804 - val_acc: 0.6042
Epoch 16/500
289s - loss: 1.1757 - acc: 0.6892 - val_loss: 1.6674 - val_acc: 0.6400
Epoch 17/500
289s - loss: 1.1451 - acc: 0.7129 - val_loss: 3.7818 - val_acc: 0.4726
Epoch 18/500
289s - loss: 1.1647 - acc: 0.7039 - val_loss: 1.5248 - val_acc: 0.6863
Epoch 19/500
289s - loss: 1.1669 - acc: 0.7203 - val_loss: 1.8716 - val_acc: 0.6653
Epoch 20/500
290s - loss: 1.2381 - acc: 0.7042 - val_loss: 2.1421 - val_acc: 0.5316
Epoch 21/500
289s - loss: 1.1853 - acc: 0.6908 - val_loss: 1.3949 - val_acc: 0.6895
Epoch 22/500
289s - loss: 1.3487 - acc: 0.6887 - val_loss: 2.6445 - val_acc: 0.5411
Epoch 23/500
288s - loss: 1.1196 - acc: 0.7171 - val_loss: 2.1516 - val_acc: 0.6011
Epoch 24/500
288s - loss: 1.2272 - acc: 0.7024 - val_loss: 4.8985 - val_acc: 0.2442
Epoch 25/500
288s - loss: 1.1996 - acc: 0.7103 - val_loss: 4.0753 - val_acc: 0.5516
Epoch 26/500
288s - loss: 1.1947 - acc: 0.7179 - val_loss: 2.9307 - val_acc: 0.4611
Epoch 27/500
288s - loss: 1.2424 - acc: 0.7116 - val_loss: 4.8908 - val_acc: 0.2284
Epoch 28/500
288s - loss: 1.3264 - acc: 0.6695 - val_loss: 1.5690 - val_acc: 0.6358
Epoch 29/500
288s - loss: 1.0882 - acc: 0.7316 - val_loss: 8.4134 - val_acc: 0.2295
Epoch 30/500

Epoch 00029: reducing learning rate to 0.010000000149.
287s - loss: 1.3275 - acc: 0.7266 - val_loss: 1.4872 - val_acc: 0.6737
Epoch 31/500
286s - loss: 1.1225 - acc: 0.7318 - val_loss: 1.3360 - val_acc: 0.7200
Epoch 32/500
286s - loss: 0.9217 - acc: 0.7697 - val_loss: 1.0888 - val_acc: 0.7505
Epoch 33/500
286s - loss: 0.8960 - acc: 0.7711 - val_loss: 1.0946 - val_acc: 0.7558
Epoch 34/500
286s - loss: 0.8426 - acc: 0.7871 - val_loss: 0.9212 - val_acc: 0.7779
Epoch 35/500
286s - loss: 0.7668 - acc: 0.7929 - val_loss: 0.9875 - val_acc: 0.7747
Epoch 36/500
286s - loss: 0.7553 - acc: 0.7992 - val_loss: 0.8418 - val_acc: 0.7947
Epoch 37/500
286s - loss: 0.7346 - acc: 0.7987 - val_loss: 0.9825 - val_acc: 0.7768
Epoch 38/500
287s - loss: 0.6913 - acc: 0.8095 - val_loss: 0.9300 - val_acc: 0.7811
Epoch 39/500
288s - loss: 0.6649 - acc: 0.8153 - val_loss: 1.0101 - val_acc: 0.7747
Epoch 40/500
288s - loss: 0.6428 - acc: 0.8113 - val_loss: 1.0157 - val_acc: 0.7737
Epoch 41/500
288s - loss: 0.6919 - acc: 0.8074 - val_loss: 0.8934 - val_acc: 0.7842
Epoch 42/500
288s - loss: 0.6580 - acc: 0.8145 - val_loss: 0.9211 - val_acc: 0.7726
Epoch 43/500
288s - loss: 0.6528 - acc: 0.8129 - val_loss: 0.8462 - val_acc: 0.7947
Epoch 44/500
288s - loss: 0.6544 - acc: 0.8176 - val_loss: 1.0348 - val_acc: 0.7779
Epoch 45/500

Epoch 00044: reducing learning rate to 0.000999999977648.
288s - loss: 0.6265 - acc: 0.8161 - val_loss: 0.9173 - val_acc: 0.7853
Epoch 46/500
288s - loss: 0.5801 - acc: 0.8261 - val_loss: 0.9031 - val_acc: 0.7853
Epoch 47/500
288s - loss: 0.6500 - acc: 0.8189 - val_loss: 0.8951 - val_acc: 0.7863
Epoch 48/500
288s - loss: 0.5956 - acc: 0.8261 - val_loss: 0.8942 - val_acc: 0.7853
Epoch 49/500
288s - loss: 0.5851 - acc: 0.8292 - val_loss: 0.8889 - val_acc: 0.7884
Epoch 50/500
288s - loss: 0.6097 - acc: 0.8237 - val_loss: 0.8846 - val_acc: 0.7895
Epoch 51/500
287s - loss: 0.5835 - acc: 0.8247 - val_loss: 0.8871 - val_acc: 0.7874
Epoch 52/500
287s - loss: 0.6363 - acc: 0.8142 - val_loss: 0.8871 - val_acc: 0.7863
Epoch 53/500

Epoch 00052: reducing learning rate to 9.99999931082e-05.
287s - loss: 0.5842 - acc: 0.8247 - val_loss: 0.8710 - val_acc: 0.7874
Epoch 54/500
287s - loss: 0.5775 - acc: 0.8268 - val_loss: 0.8803 - val_acc: 0.7926
Epoch 55/500
287s - loss: 0.5537 - acc: 0.8276 - val_loss: 0.8812 - val_acc: 0.7874
Epoch 56/500
288s - loss: 0.6110 - acc: 0.8197 - val_loss: 0.8781 - val_acc: 0.7874
Epoch 57/500
288s - loss: 0.5830 - acc: 0.8276 - val_loss: 0.8835 - val_acc: 0.7916
Epoch 58/500
288s - loss: 0.6010 - acc: 0.8234 - val_loss: 0.8831 - val_acc: 0.7884
Epoch 59/500
288s - loss: 0.5687 - acc: 0.8300 - val_loss: 0.8778 - val_acc: 0.7916
Epoch 60/500
288s - loss: 0.5858 - acc: 0.8282 - val_loss: 0.8831 - val_acc: 0.7895
Epoch 61/500

Epoch 00060: reducing learning rate to 9.99999901978e-06.
287s - loss: 0.5906 - acc: 0.8258 - val_loss: 0.8816 - val_acc: 0.7884
Epoch 62/500
288s - loss: 0.5932 - acc: 0.8213 - val_loss: 0.8836 - val_acc: 0.7874
Epoch 63/500
288s - loss: 0.5938 - acc: 0.8239 - val_loss: 0.8826 - val_acc: 0.7895
Epoch 64/500
288s - loss: 0.5988 - acc: 0.8197 - val_loss: 0.8764 - val_acc: 0.7916
Epoch 65/500
287s - loss: 0.6091 - acc: 0.8200 - val_loss: 0.8824 - val_acc: 0.7926
Epoch 66/500
288s - loss: 0.6056 - acc: 0.8184 - val_loss: 0.8728 - val_acc: 0.7916
Epoch 67/500
288s - loss: 0.5960 - acc: 0.8229 - val_loss: 0.8807 - val_acc: 0.7895
Epoch 68/500
288s - loss: 0.6197 - acc: 0.8203 - val_loss: 0.8808 - val_acc: 0.7916
Epoch 69/500

Epoch 00068: reducing learning rate to 1e-06.
287s - loss: 0.5980 - acc: 0.8274 - val_loss: 0.8774 - val_acc: 0.7895
Epoch 70/500
287s - loss: 0.5859 - acc: 0.8255 - val_loss: 0.8796 - val_acc: 0.7926
Epoch 71/500
289s - loss: 0.5858 - acc: 0.8226 - val_loss: 0.8865 - val_acc: 0.7874
Epoch 72/500
288s - loss: 0.5931 - acc: 0.8192 - val_loss: 0.8870 - val_acc: 0.7905
Epoch 73/500
288s - loss: 0.5887 - acc: 0.8203 - val_loss: 0.8783 - val_acc: 0.7895
Epoch 74/500
288s - loss: 0.5930 - acc: 0.8203 - val_loss: 0.8755 - val_acc: 0.7874
Epoch 75/500
288s - loss: 0.5989 - acc: 0.8237 - val_loss: 0.8817 - val_acc: 0.7895
Epoch 76/500
288s - loss: 0.6305 - acc: 0.8166 - val_loss: 0.8830 - val_acc: 0.7916
Epoch 77/500
288s - loss: 0.6127 - acc: 0.8216 - val_loss: 0.8795 - val_acc: 0.7895
Epoch 78/500
288s - loss: 0.5721 - acc: 0.8276 - val_loss: 0.8722 - val_acc: 0.7905
Epoch 79/500
288s - loss: 0.5777 - acc: 0.8303 - val_loss: 0.8753 - val_acc: 0.7916
Epoch 80/500
287s - loss: 0.5843 - acc: 0.8297 - val_loss: 0.8770 - val_acc: 0.7874
Epoch 81/500
288s - loss: 0.5839 - acc: 0.8300 - val_loss: 0.8743 - val_acc: 0.7884
Epoch 82/500
287s - loss: 0.5923 - acc: 0.8261 - val_loss: 0.8771 - val_acc: 0.7905
Epoch 83/500
288s - loss: 0.5902 - acc: 0.8187 - val_loss: 0.8787 - val_acc: 0.7874
Epoch 84/500
288s - loss: 0.6069 - acc: 0.8192 - val_loss: 0.8848 - val_acc: 0.7884
Epoch 85/500
288s - loss: 0.6091 - acc: 0.8221 - val_loss: 0.8752 - val_acc: 0.7895
Epoch 86/500
288s - loss: 0.5980 - acc: 0.8208 - val_loss: 0.8707 - val_acc: 0.7916
Epoch 87/500
288s - loss: 0.6135 - acc: 0.8226 - val_loss: 0.8757 - val_acc: 0.7905
Epoch 88/500
288s - loss: 0.5875 - acc: 0.8268 - val_loss: 0.8830 - val_acc: 0.7884
Epoch 89/500
288s - loss: 0.5976 - acc: 0.8216 - val_loss: 0.8778 - val_acc: 0.7884
Epoch 90/500
288s - loss: 0.5603 - acc: 0.8239 - val_loss: 0.8821 - val_acc: 0.7863
Epoch 91/500
287s - loss: 0.5692 - acc: 0.8255 - val_loss: 0.8774 - val_acc: 0.7863
Epoch 92/500
287s - loss: 0.5746 - acc: 0.8268 - val_loss: 0.8786 - val_acc: 0.7863
Epoch 93/500
287s - loss: 0.6020 - acc: 0.8250 - val_loss: 0.8828 - val_acc: 0.7863
Epoch 94/500
288s - loss: 0.5847 - acc: 0.8221 - val_loss: 0.8842 - val_acc: 0.7874
Epoch 95/500
287s - loss: 0.5971 - acc: 0.8263 - val_loss: 0.8818 - val_acc: 0.7895
Epoch 96/500
287s - loss: 0.6155 - acc: 0.8205 - val_loss: 0.8791 - val_acc: 0.7905
Epoch 97/500
288s - loss: 0.6100 - acc: 0.8261 - val_loss: 0.8804 - val_acc: 0.7874
Training loss for fold 2 is 0.46530877150987326 with percent 85.5789473809694
Testing loss for fold 2 is 0.8418457085207889 with percent 79.47368419797797
 
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_7 (InputLayer)             (None, 85, 85, 3)     0                                            
____________________________________________________________________________________________________
conv2d_19 (Conv2D)               (None, 83, 83, 16)    448         input_7[0][0]                    
____________________________________________________________________________________________________
batch_normalization_22 (BatchNor (None, 83, 83, 16)    64          conv2d_19[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_22 (LeakyReLU)       (None, 83, 83, 16)    0           batch_normalization_22[0][0]     
____________________________________________________________________________________________________
conv2d_20 (Conv2D)               (None, 82, 82, 16)    1040        leaky_re_lu_22[0][0]             
____________________________________________________________________________________________________
batch_normalization_23 (BatchNor (None, 82, 82, 16)    64          conv2d_20[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_23 (LeakyReLU)       (None, 82, 82, 16)    0           batch_normalization_23[0][0]     
____________________________________________________________________________________________________
max_pooling2d_10 (MaxPooling2D)  (None, 41, 41, 16)    0           leaky_re_lu_23[0][0]             
____________________________________________________________________________________________________
dropout_16 (Dropout)             (None, 41, 41, 16)    0           max_pooling2d_10[0][0]           
____________________________________________________________________________________________________
conv2d_21 (Conv2D)               (None, 39, 39, 32)    4640        dropout_16[0][0]                 
____________________________________________________________________________________________________
batch_normalization_24 (BatchNor (None, 39, 39, 32)    128         conv2d_21[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_24 (LeakyReLU)       (None, 39, 39, 32)    0           batch_normalization_24[0][0]     
____________________________________________________________________________________________________
conv2d_22 (Conv2D)               (None, 38, 38, 32)    4128        leaky_re_lu_24[0][0]             
____________________________________________________________________________________________________
batch_normalization_25 (BatchNor (None, 38, 38, 32)    128         conv2d_22[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_25 (LeakyReLU)       (None, 38, 38, 32)    0           batch_normalization_25[0][0]     
____________________________________________________________________________________________________
max_pooling2d_11 (MaxPooling2D)  (None, 19, 19, 32)    0           leaky_re_lu_25[0][0]             
____________________________________________________________________________________________________
dropout_17 (Dropout)             (None, 19, 19, 32)    0           max_pooling2d_11[0][0]           
____________________________________________________________________________________________________
conv2d_23 (Conv2D)               (None, 17, 17, 64)    18496       dropout_17[0][0]                 
____________________________________________________________________________________________________
batch_normalization_26 (BatchNor (None, 17, 17, 64)    256         conv2d_23[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_26 (LeakyReLU)       (None, 17, 17, 64)    0           batch_normalization_26[0][0]     
____________________________________________________________________________________________________
conv2d_24 (Conv2D)               (None, 16, 16, 64)    16448       leaky_re_lu_26[0][0]             
____________________________________________________________________________________________________
batch_normalization_27 (BatchNor (None, 16, 16, 64)    256         conv2d_24[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_27 (LeakyReLU)       (None, 16, 16, 64)    0           batch_normalization_27[0][0]     
____________________________________________________________________________________________________
max_pooling2d_12 (MaxPooling2D)  (None, 8, 8, 64)      0           leaky_re_lu_27[0][0]             
____________________________________________________________________________________________________
dropout_18 (Dropout)             (None, 8, 8, 64)      0           max_pooling2d_12[0][0]           
____________________________________________________________________________________________________
flatten_4 (Flatten)              (None, 4096)          0           dropout_18[0][0]                 
____________________________________________________________________________________________________
batch_normalization_28 (BatchNor (None, 4096)          16384       flatten_4[0][0]                  
____________________________________________________________________________________________________
input_8 (InputLayer)             (None, 102)           0                                            
____________________________________________________________________________________________________
concatenate_4 (Concatenate)      (None, 4198)          0           batch_normalization_28[0][0]     
                                                                   input_8[0][0]                    
____________________________________________________________________________________________________
dense_13 (Dense)                 (None, 128)           537472      concatenate_4[0][0]              
____________________________________________________________________________________________________
leaky_re_lu_28 (LeakyReLU)       (None, 128)           0           dense_13[0][0]                   
____________________________________________________________________________________________________
dense_14 (Dense)                 (None, 64)            8256        leaky_re_lu_28[0][0]             
____________________________________________________________________________________________________
dropout_19 (Dropout)             (None, 64)            0           dense_14[0][0]                   
____________________________________________________________________________________________________
dense_15 (Dense)                 (None, 32)            2080        dropout_19[0][0]                 
____________________________________________________________________________________________________
dropout_20 (Dropout)             (None, 32)            0           dense_15[0][0]                   
____________________________________________________________________________________________________
dense_16 (Dense)                 (None, 12)            396         dropout_20[0][0]                 
====================================================================================================
Total params: 610,684
Trainable params: 602,044
Non-trainable params: 8,640
____________________________________________________________________________________________________
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
291s - loss: 2.0330 - acc: 0.3063 - val_loss: 4.5400 - val_acc: 0.1474
Epoch 2/500
288s - loss: 1.5905 - acc: 0.4787 - val_loss: 3.2565 - val_acc: 0.2811
Epoch 3/500
288s - loss: 1.4292 - acc: 0.5389 - val_loss: 1.5543 - val_acc: 0.5284
Epoch 4/500
289s - loss: 1.3391 - acc: 0.5850 - val_loss: 1.2722 - val_acc: 0.6305
Epoch 5/500
288s - loss: 1.2622 - acc: 0.6074 - val_loss: 1.1491 - val_acc: 0.6263
Epoch 6/500
288s - loss: 1.3175 - acc: 0.6047 - val_loss: 1.6963 - val_acc: 0.5295
Epoch 7/500
288s - loss: 1.2230 - acc: 0.6295 - val_loss: 1.0765 - val_acc: 0.6958
Epoch 8/500
289s - loss: 1.1860 - acc: 0.6576 - val_loss: 1.0923 - val_acc: 0.6705
Epoch 9/500
288s - loss: 1.1371 - acc: 0.6771 - val_loss: 2.6022 - val_acc: 0.4642
Epoch 10/500
287s - loss: 1.0886 - acc: 0.6895 - val_loss: 1.8486 - val_acc: 0.5874
Epoch 11/500
287s - loss: 1.1232 - acc: 0.6850 - val_loss: 1.7522 - val_acc: 0.5653
Epoch 12/500
287s - loss: 1.0431 - acc: 0.7087 - val_loss: 1.2147 - val_acc: 0.6832
Epoch 13/500
287s - loss: 1.0886 - acc: 0.7108 - val_loss: 0.9617 - val_acc: 0.6516
Epoch 14/500
287s - loss: 1.0687 - acc: 0.7079 - val_loss: 2.2536 - val_acc: 0.5411
Epoch 15/500
287s - loss: 1.0873 - acc: 0.7053 - val_loss: 1.2200 - val_acc: 0.6811
Epoch 16/500
288s - loss: 1.0508 - acc: 0.7247 - val_loss: 1.4057 - val_acc: 0.7053
Epoch 17/500
287s - loss: 1.0931 - acc: 0.7239 - val_loss: 2.4344 - val_acc: 0.5621
Epoch 18/500
287s - loss: 1.1241 - acc: 0.7153 - val_loss: 1.9330 - val_acc: 0.5958
Epoch 19/500
287s - loss: 1.1762 - acc: 0.7103 - val_loss: 1.7570 - val_acc: 0.6095
Epoch 20/500
287s - loss: 1.0963 - acc: 0.7216 - val_loss: 2.6578 - val_acc: 0.5600
Epoch 21/500
287s - loss: 1.0845 - acc: 0.7271 - val_loss: 1.1806 - val_acc: 0.7126
Epoch 22/500
287s - loss: 1.0844 - acc: 0.7311 - val_loss: 1.0748 - val_acc: 0.7274
Epoch 23/500
287s - loss: 1.1171 - acc: 0.7282 - val_loss: 1.6656 - val_acc: 0.7084
Epoch 24/500
287s - loss: 1.0807 - acc: 0.7350 - val_loss: 3.1015 - val_acc: 0.5347
Epoch 25/500
287s - loss: 1.0581 - acc: 0.7445 - val_loss: 2.0959 - val_acc: 0.5842
Epoch 26/500
287s - loss: 1.2737 - acc: 0.7374 - val_loss: 2.5465 - val_acc: 0.5316
Epoch 27/500
287s - loss: 1.2228 - acc: 0.7045 - val_loss: 1.3988 - val_acc: 0.7116
Epoch 28/500
287s - loss: 1.2434 - acc: 0.7245 - val_loss: 1.9900 - val_acc: 0.6842
Epoch 29/500
287s - loss: 1.2349 - acc: 0.7092 - val_loss: 3.6818 - val_acc: 0.4842
Epoch 30/500
287s - loss: 1.2426 - acc: 0.6939 - val_loss: 1.4791 - val_acc: 0.6716
Epoch 31/500

Epoch 00030: reducing learning rate to 0.010000000149.
288s - loss: 1.3083 - acc: 0.7371 - val_loss: 2.1103 - val_acc: 0.6632
Epoch 32/500
287s - loss: 1.1558 - acc: 0.7266 - val_loss: 1.1925 - val_acc: 0.7653
Epoch 33/500
287s - loss: 0.9256 - acc: 0.7695 - val_loss: 1.1244 - val_acc: 0.7947
Epoch 34/500
287s - loss: 0.8224 - acc: 0.7921 - val_loss: 1.0660 - val_acc: 0.8042
Epoch 35/500
287s - loss: 0.7512 - acc: 0.8018 - val_loss: 1.0394 - val_acc: 0.8084
Epoch 36/500
288s - loss: 0.7978 - acc: 0.8074 - val_loss: 1.1280 - val_acc: 0.8084
Epoch 37/500
287s - loss: 0.7423 - acc: 0.8061 - val_loss: 0.9017 - val_acc: 0.8242
Epoch 38/500
287s - loss: 0.7290 - acc: 0.8176 - val_loss: 0.9929 - val_acc: 0.8242
Epoch 39/500
287s - loss: 0.6616 - acc: 0.8189 - val_loss: 0.9593 - val_acc: 0.8221
Epoch 40/500
287s - loss: 0.6835 - acc: 0.8295 - val_loss: 0.9707 - val_acc: 0.8263
Epoch 41/500
287s - loss: 0.6359 - acc: 0.8313 - val_loss: 0.9599 - val_acc: 0.8326
Epoch 42/500
287s - loss: 0.6790 - acc: 0.8295 - val_loss: 0.9383 - val_acc: 0.8305
Epoch 43/500
287s - loss: 0.6098 - acc: 0.8411 - val_loss: 1.0396 - val_acc: 0.8253
Epoch 44/500
287s - loss: 0.6125 - acc: 0.8471 - val_loss: 0.9760 - val_acc: 0.8274
Epoch 45/500
287s - loss: 0.6011 - acc: 0.8421 - val_loss: 0.9540 - val_acc: 0.8368
Epoch 46/500
287s - loss: 0.5843 - acc: 0.8466 - val_loss: 0.8988 - val_acc: 0.8379
Epoch 47/500
287s - loss: 0.5711 - acc: 0.8484 - val_loss: 0.9639 - val_acc: 0.8347
Epoch 48/500
287s - loss: 0.5801 - acc: 0.8524 - val_loss: 0.9408 - val_acc: 0.8389
Epoch 49/500
287s - loss: 0.5452 - acc: 0.8582 - val_loss: 0.9081 - val_acc: 0.8484
Epoch 50/500
287s - loss: 0.5490 - acc: 0.8576 - val_loss: 0.8646 - val_acc: 0.8368
Epoch 51/500
287s - loss: 0.5396 - acc: 0.8539 - val_loss: 0.8639 - val_acc: 0.8463
Epoch 52/500
288s - loss: 0.5095 - acc: 0.8616 - val_loss: 0.9198 - val_acc: 0.8347
Epoch 53/500
287s - loss: 0.5127 - acc: 0.8621 - val_loss: 0.8922 - val_acc: 0.8389
Epoch 54/500
287s - loss: 0.5037 - acc: 0.8676 - val_loss: 0.9564 - val_acc: 0.8368
Epoch 55/500
287s - loss: 0.5129 - acc: 0.8642 - val_loss: 0.9792 - val_acc: 0.8358
Epoch 56/500
287s - loss: 0.5168 - acc: 0.8637 - val_loss: 0.9603 - val_acc: 0.8347
Epoch 57/500
287s - loss: 0.5080 - acc: 0.8613 - val_loss: 0.8819 - val_acc: 0.8400
Epoch 58/500
287s - loss: 0.4908 - acc: 0.8613 - val_loss: 0.8459 - val_acc: 0.8516
Epoch 59/500
287s - loss: 0.4494 - acc: 0.8747 - val_loss: 0.8706 - val_acc: 0.8411
Epoch 60/500
287s - loss: 0.5161 - acc: 0.8600 - val_loss: 0.8154 - val_acc: 0.8474
Epoch 61/500
287s - loss: 0.4662 - acc: 0.8705 - val_loss: 0.8212 - val_acc: 0.8484
Epoch 62/500
287s - loss: 0.4495 - acc: 0.8716 - val_loss: 0.8491 - val_acc: 0.8453
Epoch 63/500
287s - loss: 0.4528 - acc: 0.8742 - val_loss: 0.8555 - val_acc: 0.8474
Epoch 64/500
287s - loss: 0.4915 - acc: 0.8705 - val_loss: 0.8387 - val_acc: 0.8442
Epoch 65/500
287s - loss: 0.4525 - acc: 0.8742 - val_loss: 0.8319 - val_acc: 0.8453
Epoch 66/500
287s - loss: 0.4157 - acc: 0.8826 - val_loss: 0.8626 - val_acc: 0.8442
Epoch 67/500

Epoch 00066: reducing learning rate to 0.000999999977648.
287s - loss: 0.4381 - acc: 0.8753 - val_loss: 0.8046 - val_acc: 0.8495
Epoch 68/500
287s - loss: 0.4827 - acc: 0.8763 - val_loss: 0.8421 - val_acc: 0.8505
Epoch 69/500
287s - loss: 0.4218 - acc: 0.8824 - val_loss: 0.8642 - val_acc: 0.8484
Epoch 70/500
287s - loss: 0.4409 - acc: 0.8792 - val_loss: 0.8633 - val_acc: 0.8484
Epoch 71/500
287s - loss: 0.4236 - acc: 0.8829 - val_loss: 0.8671 - val_acc: 0.8474
Epoch 72/500
287s - loss: 0.4082 - acc: 0.8792 - val_loss: 0.8572 - val_acc: 0.8505
Epoch 73/500
287s - loss: 0.4548 - acc: 0.8795 - val_loss: 0.8649 - val_acc: 0.8484
Epoch 74/500
287s - loss: 0.4520 - acc: 0.8750 - val_loss: 0.8587 - val_acc: 0.8484
Epoch 75/500

Epoch 00074: reducing learning rate to 9.99999931082e-05.
287s - loss: 0.4135 - acc: 0.8811 - val_loss: 0.8603 - val_acc: 0.8484
Epoch 76/500
287s - loss: 0.4597 - acc: 0.8747 - val_loss: 0.8578 - val_acc: 0.8484
Epoch 77/500
287s - loss: 0.4304 - acc: 0.8808 - val_loss: 0.8557 - val_acc: 0.8495
Epoch 78/500
287s - loss: 0.4401 - acc: 0.8766 - val_loss: 0.8538 - val_acc: 0.8495
Epoch 79/500
287s - loss: 0.4449 - acc: 0.8847 - val_loss: 0.8568 - val_acc: 0.8484
Epoch 80/500
287s - loss: 0.4543 - acc: 0.8761 - val_loss: 0.8555 - val_acc: 0.8484
Epoch 81/500
287s - loss: 0.4026 - acc: 0.8837 - val_loss: 0.8522 - val_acc: 0.8484
Epoch 82/500
287s - loss: 0.4170 - acc: 0.8821 - val_loss: 0.8486 - val_acc: 0.8484
Epoch 83/500

Epoch 00082: reducing learning rate to 9.99999901978e-06.
287s - loss: 0.3985 - acc: 0.8874 - val_loss: 0.8519 - val_acc: 0.8495
Epoch 84/500
287s - loss: 0.4124 - acc: 0.8826 - val_loss: 0.8533 - val_acc: 0.8484
Epoch 85/500
287s - loss: 0.4540 - acc: 0.8803 - val_loss: 0.8576 - val_acc: 0.8484
Epoch 86/500
287s - loss: 0.4200 - acc: 0.8779 - val_loss: 0.8576 - val_acc: 0.8495
Epoch 87/500
287s - loss: 0.4072 - acc: 0.8784 - val_loss: 0.8564 - val_acc: 0.8484
Epoch 88/500
287s - loss: 0.4269 - acc: 0.8842 - val_loss: 0.8546 - val_acc: 0.8484
Epoch 89/500
287s - loss: 0.4468 - acc: 0.8789 - val_loss: 0.8569 - val_acc: 0.8495
Epoch 90/500
287s - loss: 0.4198 - acc: 0.8811 - val_loss: 0.8557 - val_acc: 0.8484
Epoch 91/500

Epoch 00090: reducing learning rate to 1e-06.
287s - loss: 0.4097 - acc: 0.8829 - val_loss: 0.8561 - val_acc: 0.8484
Epoch 92/500
287s - loss: 0.4299 - acc: 0.8784 - val_loss: 0.8549 - val_acc: 0.8505
Epoch 93/500
287s - loss: 0.4195 - acc: 0.8853 - val_loss: 0.8481 - val_acc: 0.8484
Epoch 94/500
287s - loss: 0.4098 - acc: 0.8813 - val_loss: 0.8550 - val_acc: 0.8495
Epoch 95/500
287s - loss: 0.4109 - acc: 0.8797 - val_loss: 0.8538 - val_acc: 0.8484
Epoch 96/500
287s - loss: 0.4385 - acc: 0.8784 - val_loss: 0.8621 - val_acc: 0.8495
Epoch 97/500
287s - loss: 0.4057 - acc: 0.8858 - val_loss: 0.8603 - val_acc: 0.8484
Epoch 98/500
287s - loss: 0.3976 - acc: 0.8842 - val_loss: 0.8561 - val_acc: 0.8484
Epoch 99/500
287s - loss: 0.4118 - acc: 0.8832 - val_loss: 0.8533 - val_acc: 0.8495
Epoch 100/500
287s - loss: 0.4207 - acc: 0.8829 - val_loss: 0.8525 - val_acc: 0.8495
Epoch 101/500
287s - loss: 0.4328 - acc: 0.8834 - val_loss: 0.8544 - val_acc: 0.8495
Epoch 102/500
287s - loss: 0.4456 - acc: 0.8766 - val_loss: 0.8592 - val_acc: 0.8484
Epoch 103/500
287s - loss: 0.4296 - acc: 0.8795 - val_loss: 0.8548 - val_acc: 0.8495
Epoch 104/500
287s - loss: 0.4411 - acc: 0.8787 - val_loss: 0.8499 - val_acc: 0.8484
Epoch 105/500
287s - loss: 0.4302 - acc: 0.8787 - val_loss: 0.8556 - val_acc: 0.8495
Epoch 106/500
287s - loss: 0.4351 - acc: 0.8797 - val_loss: 0.8524 - val_acc: 0.8495
Epoch 107/500
287s - loss: 0.4271 - acc: 0.8816 - val_loss: 0.8618 - val_acc: 0.8484
Epoch 108/500
287s - loss: 0.4017 - acc: 0.8842 - val_loss: 0.8502 - val_acc: 0.8484
Epoch 109/500
287s - loss: 0.4424 - acc: 0.8803 - val_loss: 0.8541 - val_acc: 0.8484
Epoch 110/500
287s - loss: 0.4205 - acc: 0.8805 - val_loss: 0.8554 - val_acc: 0.8484
Epoch 111/500
287s - loss: 0.4208 - acc: 0.8803 - val_loss: 0.8512 - val_acc: 0.8495
Epoch 112/500
287s - loss: 0.4320 - acc: 0.8834 - val_loss: 0.8549 - val_acc: 0.8484
Epoch 113/500
287s - loss: 0.4504 - acc: 0.8768 - val_loss: 0.8533 - val_acc: 0.8495
Epoch 114/500
287s - loss: 0.4363 - acc: 0.8787 - val_loss: 0.8513 - val_acc: 0.8484
Epoch 115/500
287s - loss: 0.3935 - acc: 0.8850 - val_loss: 0.8620 - val_acc: 0.8484
Epoch 116/500
287s - loss: 0.4241 - acc: 0.8795 - val_loss: 0.8582 - val_acc: 0.8495
Epoch 117/500
288s - loss: 0.4396 - acc: 0.8792 - val_loss: 0.8588 - val_acc: 0.8495
Epoch 118/500
287s - loss: 0.4137 - acc: 0.8845 - val_loss: 0.8506 - val_acc: 0.8495
Epoch 119/500
287s - loss: 0.3897 - acc: 0.8866 - val_loss: 0.8528 - val_acc: 0.8484
Epoch 120/500
287s - loss: 0.4240 - acc: 0.8784 - val_loss: 0.8573 - val_acc: 0.8495
Epoch 121/500
287s - loss: 0.4274 - acc: 0.8797 - val_loss: 0.8540 - val_acc: 0.8484
Epoch 122/500
287s - loss: 0.4368 - acc: 0.8816 - val_loss: 0.8507 - val_acc: 0.8474
Epoch 123/500
287s - loss: 0.4130 - acc: 0.8805 - val_loss: 0.8536 - val_acc: 0.8495
Epoch 124/500
287s - loss: 0.4411 - acc: 0.8784 - val_loss: 0.8568 - val_acc: 0.8495
Epoch 125/500
287s - loss: 0.4113 - acc: 0.8853 - val_loss: 0.8541 - val_acc: 0.8495
Epoch 126/500
287s - loss: 0.3896 - acc: 0.8863 - val_loss: 0.8520 - val_acc: 0.8484
Epoch 127/500
287s - loss: 0.4474 - acc: 0.8805 - val_loss: 0.8548 - val_acc: 0.8495
Epoch 128/500
287s - loss: 0.4035 - acc: 0.8842 - val_loss: 0.8546 - val_acc: 0.8495
Training loss for fold 3 is 0.29598093035974 with percent 90.57894735587271
Testing loss for fold 3 is 0.8458788545508134 with percent 85.15789472429375
 
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_9 (InputLayer)             (None, 85, 85, 3)     0                                            
____________________________________________________________________________________________________
conv2d_25 (Conv2D)               (None, 83, 83, 16)    448         input_9[0][0]                    
____________________________________________________________________________________________________
batch_normalization_29 (BatchNor (None, 83, 83, 16)    64          conv2d_25[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_29 (LeakyReLU)       (None, 83, 83, 16)    0           batch_normalization_29[0][0]     
____________________________________________________________________________________________________
conv2d_26 (Conv2D)               (None, 82, 82, 16)    1040        leaky_re_lu_29[0][0]             
____________________________________________________________________________________________________
batch_normalization_30 (BatchNor (None, 82, 82, 16)    64          conv2d_26[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_30 (LeakyReLU)       (None, 82, 82, 16)    0           batch_normalization_30[0][0]     
____________________________________________________________________________________________________
max_pooling2d_13 (MaxPooling2D)  (None, 41, 41, 16)    0           leaky_re_lu_30[0][0]             
____________________________________________________________________________________________________
dropout_21 (Dropout)             (None, 41, 41, 16)    0           max_pooling2d_13[0][0]           
____________________________________________________________________________________________________
conv2d_27 (Conv2D)               (None, 39, 39, 32)    4640        dropout_21[0][0]                 
____________________________________________________________________________________________________
batch_normalization_31 (BatchNor (None, 39, 39, 32)    128         conv2d_27[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_31 (LeakyReLU)       (None, 39, 39, 32)    0           batch_normalization_31[0][0]     
____________________________________________________________________________________________________
conv2d_28 (Conv2D)               (None, 38, 38, 32)    4128        leaky_re_lu_31[0][0]             
____________________________________________________________________________________________________
batch_normalization_32 (BatchNor (None, 38, 38, 32)    128         conv2d_28[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_32 (LeakyReLU)       (None, 38, 38, 32)    0           batch_normalization_32[0][0]     
____________________________________________________________________________________________________
max_pooling2d_14 (MaxPooling2D)  (None, 19, 19, 32)    0           leaky_re_lu_32[0][0]             
____________________________________________________________________________________________________
dropout_22 (Dropout)             (None, 19, 19, 32)    0           max_pooling2d_14[0][0]           
____________________________________________________________________________________________________
conv2d_29 (Conv2D)               (None, 17, 17, 64)    18496       dropout_22[0][0]                 
____________________________________________________________________________________________________
batch_normalization_33 (BatchNor (None, 17, 17, 64)    256         conv2d_29[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_33 (LeakyReLU)       (None, 17, 17, 64)    0           batch_normalization_33[0][0]     
____________________________________________________________________________________________________
conv2d_30 (Conv2D)               (None, 16, 16, 64)    16448       leaky_re_lu_33[0][0]             
____________________________________________________________________________________________________
batch_normalization_34 (BatchNor (None, 16, 16, 64)    256         conv2d_30[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_34 (LeakyReLU)       (None, 16, 16, 64)    0           batch_normalization_34[0][0]     
____________________________________________________________________________________________________
max_pooling2d_15 (MaxPooling2D)  (None, 8, 8, 64)      0           leaky_re_lu_34[0][0]             
____________________________________________________________________________________________________
dropout_23 (Dropout)             (None, 8, 8, 64)      0           max_pooling2d_15[0][0]           
____________________________________________________________________________________________________
flatten_5 (Flatten)              (None, 4096)          0           dropout_23[0][0]                 
____________________________________________________________________________________________________
batch_normalization_35 (BatchNor (None, 4096)          16384       flatten_5[0][0]                  
____________________________________________________________________________________________________
input_10 (InputLayer)            (None, 102)           0                                            
____________________________________________________________________________________________________
concatenate_5 (Concatenate)      (None, 4198)          0           batch_normalization_35[0][0]     
                                                                   input_10[0][0]                   
____________________________________________________________________________________________________
dense_17 (Dense)                 (None, 128)           537472      concatenate_5[0][0]              
____________________________________________________________________________________________________
leaky_re_lu_35 (LeakyReLU)       (None, 128)           0           dense_17[0][0]                   
____________________________________________________________________________________________________
dense_18 (Dense)                 (None, 64)            8256        leaky_re_lu_35[0][0]             
____________________________________________________________________________________________________
dropout_24 (Dropout)             (None, 64)            0           dense_18[0][0]                   
____________________________________________________________________________________________________
dense_19 (Dense)                 (None, 32)            2080        dropout_24[0][0]                 
____________________________________________________________________________________________________
dropout_25 (Dropout)             (None, 32)            0           dense_19[0][0]                   
____________________________________________________________________________________________________
dense_20 (Dense)                 (None, 12)            396         dropout_25[0][0]                 
====================================================================================================
Total params: 610,684
Trainable params: 602,044
Non-trainable params: 8,640
____________________________________________________________________________________________________
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
291s - loss: 2.0100 - acc: 0.3134 - val_loss: 3.3577 - val_acc: 0.1526
Epoch 2/500
287s - loss: 1.5699 - acc: 0.4821 - val_loss: 5.8098 - val_acc: 0.1758
Epoch 3/500
287s - loss: 1.4006 - acc: 0.5555 - val_loss: 1.8364 - val_acc: 0.4737
Epoch 4/500
287s - loss: 1.3577 - acc: 0.5821 - val_loss: 1.2567 - val_acc: 0.6105
Epoch 5/500
287s - loss: 1.3067 - acc: 0.6089 - val_loss: 1.1899 - val_acc: 0.6568
Epoch 6/500
287s - loss: 1.2464 - acc: 0.6408 - val_loss: 1.6265 - val_acc: 0.5947
Epoch 7/500
287s - loss: 1.1868 - acc: 0.6487 - val_loss: 2.6721 - val_acc: 0.4337
Epoch 8/500
287s - loss: 1.1265 - acc: 0.6697 - val_loss: 0.9471 - val_acc: 0.7200
Epoch 9/500
287s - loss: 1.1114 - acc: 0.6826 - val_loss: 2.3837 - val_acc: 0.5958
Epoch 10/500
287s - loss: 1.1111 - acc: 0.6987 - val_loss: 1.9854 - val_acc: 0.4884
Epoch 11/500
287s - loss: 1.0618 - acc: 0.7082 - val_loss: 1.3482 - val_acc: 0.6221
Epoch 12/500
287s - loss: 1.0044 - acc: 0.7139 - val_loss: 1.3572 - val_acc: 0.6937
Epoch 13/500
287s - loss: 1.0784 - acc: 0.7092 - val_loss: 2.5644 - val_acc: 0.5063
Epoch 14/500
287s - loss: 1.0426 - acc: 0.7211 - val_loss: 1.2870 - val_acc: 0.6768
Epoch 15/500
287s - loss: 1.0409 - acc: 0.7176 - val_loss: 1.3081 - val_acc: 0.7337
Epoch 16/500
287s - loss: 1.0772 - acc: 0.7389 - val_loss: 4.0974 - val_acc: 0.3032
Epoch 17/500
287s - loss: 0.9928 - acc: 0.7374 - val_loss: 1.8421 - val_acc: 0.5579
Epoch 18/500
287s - loss: 1.0367 - acc: 0.7413 - val_loss: 1.2880 - val_acc: 0.7274
Epoch 19/500
287s - loss: 1.0599 - acc: 0.7458 - val_loss: 1.3130 - val_acc: 0.6674
Epoch 20/500
287s - loss: 1.0481 - acc: 0.7474 - val_loss: 2.7817 - val_acc: 0.2958
Epoch 21/500
287s - loss: 1.0626 - acc: 0.7445 - val_loss: 3.6121 - val_acc: 0.3842
Epoch 22/500
287s - loss: 0.9903 - acc: 0.7484 - val_loss: 2.2826 - val_acc: 0.5842
Epoch 23/500
286s - loss: 1.0402 - acc: 0.7432 - val_loss: 1.4121 - val_acc: 0.7116
Epoch 24/500

Epoch 00023: reducing learning rate to 0.010000000149.
286s - loss: 1.1155 - acc: 0.7255 - val_loss: 2.2124 - val_acc: 0.4284
Epoch 25/500
286s - loss: 0.9273 - acc: 0.7511 - val_loss: 1.0181 - val_acc: 0.7526
Epoch 26/500
286s - loss: 0.8048 - acc: 0.7747 - val_loss: 0.9263 - val_acc: 0.7589
Epoch 27/500
286s - loss: 0.7021 - acc: 0.7953 - val_loss: 0.8031 - val_acc: 0.7895
Epoch 28/500
286s - loss: 0.6538 - acc: 0.8047 - val_loss: 0.7538 - val_acc: 0.8042
Epoch 29/500
286s - loss: 0.6272 - acc: 0.8184 - val_loss: 0.7322 - val_acc: 0.8074
Epoch 30/500
286s - loss: 0.5945 - acc: 0.8261 - val_loss: 0.7227 - val_acc: 0.8168
Epoch 31/500
286s - loss: 0.5636 - acc: 0.8326 - val_loss: 0.7825 - val_acc: 0.8179
Epoch 32/500
286s - loss: 0.5572 - acc: 0.8389 - val_loss: 0.8519 - val_acc: 0.8053
Epoch 33/500
286s - loss: 0.5608 - acc: 0.8397 - val_loss: 0.6357 - val_acc: 0.8295
Epoch 34/500
286s - loss: 0.5070 - acc: 0.8516 - val_loss: 0.7060 - val_acc: 0.8316
Epoch 35/500
286s - loss: 0.5042 - acc: 0.8571 - val_loss: 0.7856 - val_acc: 0.8242
Epoch 36/500
286s - loss: 0.5139 - acc: 0.8547 - val_loss: 0.6928 - val_acc: 0.8337
Epoch 37/500
286s - loss: 0.4897 - acc: 0.8597 - val_loss: 0.7208 - val_acc: 0.8316
Epoch 38/500
286s - loss: 0.4918 - acc: 0.8574 - val_loss: 0.7775 - val_acc: 0.8232
Epoch 39/500
286s - loss: 0.4847 - acc: 0.8634 - val_loss: 0.7171 - val_acc: 0.8400
Epoch 40/500
286s - loss: 0.4874 - acc: 0.8634 - val_loss: 0.6425 - val_acc: 0.8432
Epoch 41/500
286s - loss: 0.4407 - acc: 0.8695 - val_loss: 0.7219 - val_acc: 0.8421
Epoch 42/500
286s - loss: 0.4516 - acc: 0.8684 - val_loss: 0.7450 - val_acc: 0.8389
Epoch 43/500
287s - loss: 0.4543 - acc: 0.8726 - val_loss: 0.6931 - val_acc: 0.8484
Epoch 44/500
286s - loss: 0.4258 - acc: 0.8761 - val_loss: 0.6840 - val_acc: 0.8495
Epoch 45/500
286s - loss: 0.4074 - acc: 0.8768 - val_loss: 0.7038 - val_acc: 0.8526
Epoch 46/500
286s - loss: 0.4300 - acc: 0.8761 - val_loss: 0.6760 - val_acc: 0.8484
Epoch 47/500
286s - loss: 0.4430 - acc: 0.8758 - val_loss: 0.6485 - val_acc: 0.8600
Epoch 48/500
286s - loss: 0.4235 - acc: 0.8782 - val_loss: 0.7946 - val_acc: 0.8474
Epoch 49/500
286s - loss: 0.4090 - acc: 0.8824 - val_loss: 0.6863 - val_acc: 0.8484
Epoch 50/500
286s - loss: 0.3795 - acc: 0.8900 - val_loss: 0.6968 - val_acc: 0.8589
Epoch 51/500
286s - loss: 0.4310 - acc: 0.8853 - val_loss: 0.7185 - val_acc: 0.8505
Epoch 52/500
286s - loss: 0.4201 - acc: 0.8816 - val_loss: 0.6740 - val_acc: 0.8642
Epoch 53/500
286s - loss: 0.3861 - acc: 0.8882 - val_loss: 0.5898 - val_acc: 0.8632
Epoch 54/500
286s - loss: 0.4334 - acc: 0.8829 - val_loss: 0.5375 - val_acc: 0.8674
Epoch 55/500
286s - loss: 0.4010 - acc: 0.8911 - val_loss: 0.7401 - val_acc: 0.8505
Epoch 56/500
286s - loss: 0.3682 - acc: 0.8953 - val_loss: 0.6370 - val_acc: 0.8621
Epoch 57/500
287s - loss: 0.3995 - acc: 0.8900 - val_loss: 0.5507 - val_acc: 0.8716
Epoch 58/500
286s - loss: 0.3890 - acc: 0.8911 - val_loss: 0.5818 - val_acc: 0.8726
Epoch 59/500
286s - loss: 0.3556 - acc: 0.8916 - val_loss: 0.6781 - val_acc: 0.8589
Epoch 60/500
286s - loss: 0.3937 - acc: 0.8918 - val_loss: 0.5752 - val_acc: 0.8716
Epoch 61/500
286s - loss: 0.3708 - acc: 0.8932 - val_loss: 0.6886 - val_acc: 0.8558
Epoch 62/500
286s - loss: 0.3574 - acc: 0.8950 - val_loss: 0.6171 - val_acc: 0.8632
Epoch 63/500
287s - loss: 0.3323 - acc: 0.8987 - val_loss: 0.6022 - val_acc: 0.8653
Epoch 64/500
286s - loss: 0.3504 - acc: 0.8963 - val_loss: 0.6720 - val_acc: 0.8674
Epoch 65/500
286s - loss: 0.3368 - acc: 0.9005 - val_loss: 0.6511 - val_acc: 0.8621
Epoch 66/500
286s - loss: 0.3678 - acc: 0.8966 - val_loss: 0.5802 - val_acc: 0.8653
Epoch 67/500

Epoch 00066: reducing learning rate to 0.000999999977648.
287s - loss: 0.3317 - acc: 0.9026 - val_loss: 0.7299 - val_acc: 0.8579
Epoch 68/500
286s - loss: 0.3305 - acc: 0.9042 - val_loss: 0.6579 - val_acc: 0.8642
Epoch 69/500
286s - loss: 0.3220 - acc: 0.9047 - val_loss: 0.6405 - val_acc: 0.8695
Epoch 70/500
286s - loss: 0.3132 - acc: 0.9029 - val_loss: 0.6390 - val_acc: 0.8695
Epoch 71/500
286s - loss: 0.3099 - acc: 0.9061 - val_loss: 0.6457 - val_acc: 0.8684
Epoch 72/500
286s - loss: 0.3207 - acc: 0.9008 - val_loss: 0.6320 - val_acc: 0.8663
Epoch 73/500
286s - loss: 0.3363 - acc: 0.9003 - val_loss: 0.6271 - val_acc: 0.8695
Epoch 74/500
286s - loss: 0.3291 - acc: 0.9037 - val_loss: 0.6295 - val_acc: 0.8705
Epoch 75/500

Epoch 00074: reducing learning rate to 9.99999931082e-05.
286s - loss: 0.3079 - acc: 0.9058 - val_loss: 0.6311 - val_acc: 0.8684
Epoch 76/500
286s - loss: 0.3403 - acc: 0.9024 - val_loss: 0.6245 - val_acc: 0.8684
Epoch 77/500
286s - loss: 0.3358 - acc: 0.9047 - val_loss: 0.6290 - val_acc: 0.8705
Epoch 78/500
286s - loss: 0.3102 - acc: 0.9074 - val_loss: 0.6342 - val_acc: 0.8695
Epoch 79/500
286s - loss: 0.3078 - acc: 0.9095 - val_loss: 0.6388 - val_acc: 0.8695
Epoch 80/500
286s - loss: 0.2938 - acc: 0.9063 - val_loss: 0.6322 - val_acc: 0.8684
Epoch 81/500
286s - loss: 0.3160 - acc: 0.9021 - val_loss: 0.6255 - val_acc: 0.8684
Epoch 82/500
286s - loss: 0.2992 - acc: 0.9063 - val_loss: 0.6273 - val_acc: 0.8684
Epoch 83/500

Epoch 00082: reducing learning rate to 9.99999901978e-06.
286s - loss: 0.3159 - acc: 0.9071 - val_loss: 0.6303 - val_acc: 0.8684
Epoch 84/500
286s - loss: 0.3162 - acc: 0.9068 - val_loss: 0.6387 - val_acc: 0.8674
Epoch 85/500
286s - loss: 0.3155 - acc: 0.9037 - val_loss: 0.6290 - val_acc: 0.8695
Epoch 86/500
286s - loss: 0.3076 - acc: 0.9079 - val_loss: 0.6335 - val_acc: 0.8695
Epoch 87/500
286s - loss: 0.3249 - acc: 0.9013 - val_loss: 0.6346 - val_acc: 0.8695
Epoch 88/500
286s - loss: 0.3226 - acc: 0.9016 - val_loss: 0.6202 - val_acc: 0.8684
Epoch 89/500
286s - loss: 0.3184 - acc: 0.9021 - val_loss: 0.6284 - val_acc: 0.8695
Epoch 90/500
286s - loss: 0.3407 - acc: 0.9037 - val_loss: 0.6343 - val_acc: 0.8684
Epoch 91/500

Epoch 00090: reducing learning rate to 1e-06.
286s - loss: 0.3024 - acc: 0.9121 - val_loss: 0.6379 - val_acc: 0.8684
Epoch 92/500
286s - loss: 0.3023 - acc: 0.9055 - val_loss: 0.6305 - val_acc: 0.8695
Epoch 93/500
286s - loss: 0.2863 - acc: 0.9089 - val_loss: 0.6308 - val_acc: 0.8695
Epoch 94/500
286s - loss: 0.3151 - acc: 0.9039 - val_loss: 0.6325 - val_acc: 0.8705
Epoch 95/500
286s - loss: 0.3232 - acc: 0.9058 - val_loss: 0.6302 - val_acc: 0.8695
Epoch 96/500
287s - loss: 0.3282 - acc: 0.9021 - val_loss: 0.6269 - val_acc: 0.8705
Epoch 97/500
286s - loss: 0.3268 - acc: 0.9047 - val_loss: 0.6292 - val_acc: 0.8695
Epoch 98/500
286s - loss: 0.3278 - acc: 0.9029 - val_loss: 0.6371 - val_acc: 0.8684
Epoch 99/500
286s - loss: 0.3326 - acc: 0.9045 - val_loss: 0.6352 - val_acc: 0.8695
Epoch 100/500
286s - loss: 0.3159 - acc: 0.9058 - val_loss: 0.6287 - val_acc: 0.8684
Epoch 101/500
286s - loss: 0.3178 - acc: 0.9053 - val_loss: 0.6324 - val_acc: 0.8705
Epoch 102/500
286s - loss: 0.3164 - acc: 0.9045 - val_loss: 0.6365 - val_acc: 0.8705
Epoch 103/500
286s - loss: 0.2961 - acc: 0.9079 - val_loss: 0.6307 - val_acc: 0.8695
Epoch 104/500
286s - loss: 0.2906 - acc: 0.9084 - val_loss: 0.6324 - val_acc: 0.8695
Epoch 105/500
286s - loss: 0.2946 - acc: 0.9045 - val_loss: 0.6348 - val_acc: 0.8705
Epoch 106/500
286s - loss: 0.3203 - acc: 0.9050 - val_loss: 0.6292 - val_acc: 0.8684
Epoch 107/500
286s - loss: 0.3092 - acc: 0.9087 - val_loss: 0.6229 - val_acc: 0.8695
Epoch 108/500
286s - loss: 0.3000 - acc: 0.9055 - val_loss: 0.6300 - val_acc: 0.8684
Epoch 109/500
286s - loss: 0.3226 - acc: 0.9024 - val_loss: 0.6286 - val_acc: 0.8695
Epoch 110/500
286s - loss: 0.3005 - acc: 0.9068 - val_loss: 0.6335 - val_acc: 0.8695
Epoch 111/500
287s - loss: 0.3176 - acc: 0.9058 - val_loss: 0.6328 - val_acc: 0.8695
Epoch 112/500
286s - loss: 0.3539 - acc: 0.8987 - val_loss: 0.6289 - val_acc: 0.8695
Epoch 113/500
286s - loss: 0.3058 - acc: 0.9029 - val_loss: 0.6294 - val_acc: 0.8695
Epoch 114/500
286s - loss: 0.3352 - acc: 0.9018 - val_loss: 0.6289 - val_acc: 0.8705
Epoch 115/500
286s - loss: 0.3154 - acc: 0.9074 - val_loss: 0.6328 - val_acc: 0.8684
Training loss for fold 4 is 0.22511853713738292 with percent 92.47368419797797
Testing loss for fold 4 is 0.5817587576728118 with percent 87.2631578570918
 
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_11 (InputLayer)            (None, 85, 85, 3)     0                                            
____________________________________________________________________________________________________
conv2d_31 (Conv2D)               (None, 83, 83, 16)    448         input_11[0][0]                   
____________________________________________________________________________________________________
batch_normalization_36 (BatchNor (None, 83, 83, 16)    64          conv2d_31[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_36 (LeakyReLU)       (None, 83, 83, 16)    0           batch_normalization_36[0][0]     
____________________________________________________________________________________________________
conv2d_32 (Conv2D)               (None, 82, 82, 16)    1040        leaky_re_lu_36[0][0]             
____________________________________________________________________________________________________
batch_normalization_37 (BatchNor (None, 82, 82, 16)    64          conv2d_32[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_37 (LeakyReLU)       (None, 82, 82, 16)    0           batch_normalization_37[0][0]     
____________________________________________________________________________________________________
max_pooling2d_16 (MaxPooling2D)  (None, 41, 41, 16)    0           leaky_re_lu_37[0][0]             
____________________________________________________________________________________________________
dropout_26 (Dropout)             (None, 41, 41, 16)    0           max_pooling2d_16[0][0]           
____________________________________________________________________________________________________
conv2d_33 (Conv2D)               (None, 39, 39, 32)    4640        dropout_26[0][0]                 
____________________________________________________________________________________________________
batch_normalization_38 (BatchNor (None, 39, 39, 32)    128         conv2d_33[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_38 (LeakyReLU)       (None, 39, 39, 32)    0           batch_normalization_38[0][0]     
____________________________________________________________________________________________________
conv2d_34 (Conv2D)               (None, 38, 38, 32)    4128        leaky_re_lu_38[0][0]             
____________________________________________________________________________________________________
batch_normalization_39 (BatchNor (None, 38, 38, 32)    128         conv2d_34[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_39 (LeakyReLU)       (None, 38, 38, 32)    0           batch_normalization_39[0][0]     
____________________________________________________________________________________________________
max_pooling2d_17 (MaxPooling2D)  (None, 19, 19, 32)    0           leaky_re_lu_39[0][0]             
____________________________________________________________________________________________________
dropout_27 (Dropout)             (None, 19, 19, 32)    0           max_pooling2d_17[0][0]           
____________________________________________________________________________________________________
conv2d_35 (Conv2D)               (None, 17, 17, 64)    18496       dropout_27[0][0]                 
____________________________________________________________________________________________________
batch_normalization_40 (BatchNor (None, 17, 17, 64)    256         conv2d_35[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_40 (LeakyReLU)       (None, 17, 17, 64)    0           batch_normalization_40[0][0]     
____________________________________________________________________________________________________
conv2d_36 (Conv2D)               (None, 16, 16, 64)    16448       leaky_re_lu_40[0][0]             
____________________________________________________________________________________________________
batch_normalization_41 (BatchNor (None, 16, 16, 64)    256         conv2d_36[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_41 (LeakyReLU)       (None, 16, 16, 64)    0           batch_normalization_41[0][0]     
____________________________________________________________________________________________________
max_pooling2d_18 (MaxPooling2D)  (None, 8, 8, 64)      0           leaky_re_lu_41[0][0]             
____________________________________________________________________________________________________
dropout_28 (Dropout)             (None, 8, 8, 64)      0           max_pooling2d_18[0][0]           
____________________________________________________________________________________________________
flatten_6 (Flatten)              (None, 4096)          0           dropout_28[0][0]                 
____________________________________________________________________________________________________
batch_normalization_42 (BatchNor (None, 4096)          16384       flatten_6[0][0]                  
____________________________________________________________________________________________________
input_12 (InputLayer)            (None, 102)           0                                            
____________________________________________________________________________________________________
concatenate_6 (Concatenate)      (None, 4198)          0           batch_normalization_42[0][0]     
                                                                   input_12[0][0]                   
____________________________________________________________________________________________________
dense_21 (Dense)                 (None, 128)           537472      concatenate_6[0][0]              
____________________________________________________________________________________________________
leaky_re_lu_42 (LeakyReLU)       (None, 128)           0           dense_21[0][0]                   
____________________________________________________________________________________________________
dense_22 (Dense)                 (None, 64)            8256        leaky_re_lu_42[0][0]             
____________________________________________________________________________________________________
dropout_29 (Dropout)             (None, 64)            0           dense_22[0][0]                   
____________________________________________________________________________________________________
dense_23 (Dense)                 (None, 32)            2080        dropout_29[0][0]                 
____________________________________________________________________________________________________
dropout_30 (Dropout)             (None, 32)            0           dense_23[0][0]                   
____________________________________________________________________________________________________
dense_24 (Dense)                 (None, 12)            396         dropout_30[0][0]                 
====================================================================================================
Total params: 610,684
Trainable params: 602,044
Non-trainable params: 8,640
____________________________________________________________________________________________________
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
290s - loss: 2.0054 - acc: 0.3237 - val_loss: 3.4397 - val_acc: 0.1505
Epoch 2/500
286s - loss: 1.5674 - acc: 0.4837 - val_loss: 1.3691 - val_acc: 0.5474
Epoch 3/500
286s - loss: 1.4760 - acc: 0.5382 - val_loss: 2.1659 - val_acc: 0.2547
Epoch 4/500
286s - loss: 1.3391 - acc: 0.5768 - val_loss: 1.9056 - val_acc: 0.4695
Epoch 5/500
286s - loss: 1.2650 - acc: 0.6174 - val_loss: 1.2506 - val_acc: 0.6811
Epoch 6/500
286s - loss: 1.2396 - acc: 0.6321 - val_loss: 1.1764 - val_acc: 0.6726
Epoch 7/500
286s - loss: 1.2372 - acc: 0.6282 - val_loss: 3.4320 - val_acc: 0.4221
Epoch 8/500
286s - loss: 1.2274 - acc: 0.6487 - val_loss: 2.3906 - val_acc: 0.3989
Epoch 9/500
286s - loss: 1.2327 - acc: 0.6553 - val_loss: 4.3454 - val_acc: 0.2442
Epoch 10/500
287s - loss: 1.2289 - acc: 0.6642 - val_loss: 1.3372 - val_acc: 0.6568
Epoch 11/500
286s - loss: 1.1921 - acc: 0.6726 - val_loss: 3.0541 - val_acc: 0.4789
Epoch 12/500
286s - loss: 1.2055 - acc: 0.6639 - val_loss: 1.7353 - val_acc: 0.6242
Epoch 13/500
286s - loss: 1.1686 - acc: 0.6721 - val_loss: 1.9512 - val_acc: 0.5800
Epoch 14/500

Epoch 00013: reducing learning rate to 0.010000000149.
287s - loss: 1.2717 - acc: 0.6658 - val_loss: 1.8579 - val_acc: 0.5537
Epoch 15/500
286s - loss: 1.0864 - acc: 0.6913 - val_loss: 0.8268 - val_acc: 0.7547
Epoch 16/500
287s - loss: 0.8466 - acc: 0.7458 - val_loss: 0.7020 - val_acc: 0.7726
Epoch 17/500
286s - loss: 0.7780 - acc: 0.7634 - val_loss: 0.8623 - val_acc: 0.7621
Epoch 18/500
287s - loss: 0.7266 - acc: 0.7742 - val_loss: 0.6499 - val_acc: 0.7884
Epoch 19/500
286s - loss: 0.6904 - acc: 0.7845 - val_loss: 0.7805 - val_acc: 0.7758
Epoch 20/500
286s - loss: 0.6446 - acc: 0.7963 - val_loss: 0.6913 - val_acc: 0.8126
Epoch 21/500
286s - loss: 0.6236 - acc: 0.8029 - val_loss: 0.7113 - val_acc: 0.7874
Epoch 22/500
286s - loss: 0.5926 - acc: 0.8071 - val_loss: 0.6622 - val_acc: 0.7968
Epoch 23/500
286s - loss: 0.6174 - acc: 0.8000 - val_loss: 0.6499 - val_acc: 0.8189
Epoch 24/500
286s - loss: 0.5918 - acc: 0.8145 - val_loss: 0.7009 - val_acc: 0.7874
Epoch 25/500
286s - loss: 0.5791 - acc: 0.8129 - val_loss: 0.6358 - val_acc: 0.8326
Epoch 26/500
286s - loss: 0.5654 - acc: 0.8168 - val_loss: 0.5858 - val_acc: 0.8368
Epoch 27/500
286s - loss: 0.5380 - acc: 0.8266 - val_loss: 0.6328 - val_acc: 0.8295
Epoch 28/500
286s - loss: 0.5290 - acc: 0.8308 - val_loss: 0.6490 - val_acc: 0.8263
Epoch 29/500
286s - loss: 0.5106 - acc: 0.8334 - val_loss: 0.6578 - val_acc: 0.8284
Epoch 30/500
286s - loss: 0.5069 - acc: 0.8292 - val_loss: 0.6732 - val_acc: 0.8400
Epoch 31/500
286s - loss: 0.4912 - acc: 0.8300 - val_loss: 0.6635 - val_acc: 0.8389
Epoch 32/500
286s - loss: 0.4921 - acc: 0.8308 - val_loss: 0.6389 - val_acc: 0.8326
Epoch 33/500
286s - loss: 0.4941 - acc: 0.8332 - val_loss: 0.5087 - val_acc: 0.8600
Epoch 34/500
286s - loss: 0.4801 - acc: 0.8368 - val_loss: 0.6571 - val_acc: 0.8411
Epoch 35/500
286s - loss: 0.4468 - acc: 0.8395 - val_loss: 0.5808 - val_acc: 0.8484
Epoch 36/500
286s - loss: 0.4966 - acc: 0.8345 - val_loss: 0.6879 - val_acc: 0.8326
Epoch 37/500
286s - loss: 0.4617 - acc: 0.8413 - val_loss: 0.5532 - val_acc: 0.8579
Epoch 38/500
286s - loss: 0.4947 - acc: 0.8363 - val_loss: 0.7318 - val_acc: 0.8295
Epoch 39/500
286s - loss: 0.4363 - acc: 0.8511 - val_loss: 0.5385 - val_acc: 0.8537
Epoch 40/500
286s - loss: 0.4464 - acc: 0.8418 - val_loss: 0.6679 - val_acc: 0.8358
Epoch 41/500
286s - loss: 0.4167 - acc: 0.8571 - val_loss: 0.5211 - val_acc: 0.8589
Epoch 42/500

Epoch 00041: reducing learning rate to 0.000999999977648.
286s - loss: 0.4250 - acc: 0.8542 - val_loss: 0.6539 - val_acc: 0.8442
Epoch 43/500
286s - loss: 0.4407 - acc: 0.8574 - val_loss: 0.5563 - val_acc: 0.8526
Epoch 44/500
286s - loss: 0.3861 - acc: 0.8584 - val_loss: 0.5301 - val_acc: 0.8611
Epoch 45/500
286s - loss: 0.3819 - acc: 0.8642 - val_loss: 0.5266 - val_acc: 0.8568
Epoch 46/500
286s - loss: 0.4117 - acc: 0.8592 - val_loss: 0.5275 - val_acc: 0.8547
Epoch 47/500
286s - loss: 0.3909 - acc: 0.8595 - val_loss: 0.5352 - val_acc: 0.8568
Epoch 48/500
286s - loss: 0.3888 - acc: 0.8661 - val_loss: 0.5379 - val_acc: 0.8526
Epoch 49/500
286s - loss: 0.3792 - acc: 0.8613 - val_loss: 0.5388 - val_acc: 0.8568
Epoch 50/500
286s - loss: 0.4009 - acc: 0.8532 - val_loss: 0.5475 - val_acc: 0.8547
Epoch 51/500
286s - loss: 0.4029 - acc: 0.8547 - val_loss: 0.5307 - val_acc: 0.8558
Epoch 52/500
286s - loss: 0.4088 - acc: 0.8571 - val_loss: 0.5218 - val_acc: 0.8589
Epoch 53/500

Epoch 00052: reducing learning rate to 9.99999931082e-05.
286s - loss: 0.3564 - acc: 0.8697 - val_loss: 0.5118 - val_acc: 0.8558
Epoch 54/500
286s - loss: 0.3805 - acc: 0.8632 - val_loss: 0.5120 - val_acc: 0.8568
Epoch 55/500
286s - loss: 0.3881 - acc: 0.8639 - val_loss: 0.5158 - val_acc: 0.8547
Epoch 56/500
286s - loss: 0.3961 - acc: 0.8592 - val_loss: 0.5157 - val_acc: 0.8537
Epoch 57/500
286s - loss: 0.3829 - acc: 0.8629 - val_loss: 0.5173 - val_acc: 0.8589
Epoch 58/500
286s - loss: 0.3591 - acc: 0.8618 - val_loss: 0.5122 - val_acc: 0.8589
Epoch 59/500
286s - loss: 0.3967 - acc: 0.8618 - val_loss: 0.5138 - val_acc: 0.8589
Epoch 60/500
286s - loss: 0.3842 - acc: 0.8642 - val_loss: 0.5143 - val_acc: 0.8600
Epoch 61/500

Epoch 00060: reducing learning rate to 9.99999901978e-06.
286s - loss: 0.4267 - acc: 0.8487 - val_loss: 0.5121 - val_acc: 0.8579
Epoch 62/500
286s - loss: 0.3814 - acc: 0.8634 - val_loss: 0.5145 - val_acc: 0.8579
Epoch 63/500
286s - loss: 0.3830 - acc: 0.8647 - val_loss: 0.5146 - val_acc: 0.8589
Epoch 64/500
286s - loss: 0.3934 - acc: 0.8626 - val_loss: 0.5136 - val_acc: 0.8579
Epoch 65/500
286s - loss: 0.3865 - acc: 0.8589 - val_loss: 0.5126 - val_acc: 0.8589
Epoch 66/500
286s - loss: 0.3858 - acc: 0.8597 - val_loss: 0.5083 - val_acc: 0.8600
Epoch 67/500
286s - loss: 0.3897 - acc: 0.8605 - val_loss: 0.5154 - val_acc: 0.8579
Epoch 68/500
286s - loss: 0.3846 - acc: 0.8603 - val_loss: 0.5139 - val_acc: 0.8600
Epoch 69/500

Epoch 00068: reducing learning rate to 1e-06.
286s - loss: 0.3880 - acc: 0.8621 - val_loss: 0.5137 - val_acc: 0.8579
Epoch 70/500
286s - loss: 0.3998 - acc: 0.8621 - val_loss: 0.5095 - val_acc: 0.8589
Epoch 71/500
286s - loss: 0.4085 - acc: 0.8603 - val_loss: 0.5141 - val_acc: 0.8589
Epoch 72/500
286s - loss: 0.4007 - acc: 0.8655 - val_loss: 0.5152 - val_acc: 0.8600
Epoch 73/500
286s - loss: 0.3780 - acc: 0.8653 - val_loss: 0.5149 - val_acc: 0.8611
Epoch 74/500
286s - loss: 0.4009 - acc: 0.8579 - val_loss: 0.5174 - val_acc: 0.8579
Epoch 75/500
287s - loss: 0.3748 - acc: 0.8697 - val_loss: 0.5103 - val_acc: 0.8600
Epoch 76/500
286s - loss: 0.3702 - acc: 0.8661 - val_loss: 0.5127 - val_acc: 0.8611
Epoch 77/500
286s - loss: 0.3703 - acc: 0.8626 - val_loss: 0.5146 - val_acc: 0.8600
Epoch 78/500
286s - loss: 0.3739 - acc: 0.8689 - val_loss: 0.5166 - val_acc: 0.8568
Epoch 79/500
286s - loss: 0.3962 - acc: 0.8611 - val_loss: 0.5131 - val_acc: 0.8600
Epoch 80/500
286s - loss: 0.3913 - acc: 0.8597 - val_loss: 0.5172 - val_acc: 0.8589
Epoch 81/500
287s - loss: 0.3574 - acc: 0.8716 - val_loss: 0.5137 - val_acc: 0.8600
Epoch 82/500
286s - loss: 0.3915 - acc: 0.8584 - val_loss: 0.5148 - val_acc: 0.8568
Epoch 83/500
286s - loss: 0.3981 - acc: 0.8566 - val_loss: 0.5142 - val_acc: 0.8589
Epoch 84/500
286s - loss: 0.3898 - acc: 0.8587 - val_loss: 0.5101 - val_acc: 0.8600
Epoch 85/500
286s - loss: 0.4103 - acc: 0.8555 - val_loss: 0.5107 - val_acc: 0.8579
Epoch 86/500
286s - loss: 0.3814 - acc: 0.8529 - val_loss: 0.5116 - val_acc: 0.8611
Epoch 87/500
286s - loss: 0.3904 - acc: 0.8600 - val_loss: 0.5184 - val_acc: 0.8579
Epoch 88/500
286s - loss: 0.4101 - acc: 0.8532 - val_loss: 0.5187 - val_acc: 0.8568
Epoch 89/500
286s - loss: 0.4117 - acc: 0.8624 - val_loss: 0.5173 - val_acc: 0.8589
Epoch 90/500
286s - loss: 0.3724 - acc: 0.8629 - val_loss: 0.5147 - val_acc: 0.8589
Epoch 91/500
286s - loss: 0.3873 - acc: 0.8613 - val_loss: 0.5174 - val_acc: 0.8600
Epoch 92/500
286s - loss: 0.3592 - acc: 0.8666 - val_loss: 0.5198 - val_acc: 0.8589
Epoch 93/500
286s - loss: 0.3977 - acc: 0.8653 - val_loss: 0.5154 - val_acc: 0.8600
Epoch 94/500
286s - loss: 0.3836 - acc: 0.8618 - val_loss: 0.5135 - val_acc: 0.8600
Epoch 95/500
286s - loss: 0.3811 - acc: 0.8671 - val_loss: 0.5131 - val_acc: 0.8579
Epoch 96/500
286s - loss: 0.3906 - acc: 0.8605 - val_loss: 0.5130 - val_acc: 0.8589
Epoch 97/500
286s - loss: 0.4047 - acc: 0.8632 - val_loss: 0.5180 - val_acc: 0.8589
Epoch 98/500
286s - loss: 0.4291 - acc: 0.8576 - val_loss: 0.5145 - val_acc: 0.8589
Epoch 99/500
286s - loss: 0.3588 - acc: 0.8697 - val_loss: 0.5116 - val_acc: 0.8589
Epoch 100/500
286s - loss: 0.4323 - acc: 0.8482 - val_loss: 0.5109 - val_acc: 0.8589
Epoch 101/500
286s - loss: 0.3908 - acc: 0.8579 - val_loss: 0.5134 - val_acc: 0.8600
Epoch 102/500
286s - loss: 0.4124 - acc: 0.8576 - val_loss: 0.5118 - val_acc: 0.8579
Epoch 103/500
286s - loss: 0.3916 - acc: 0.8603 - val_loss: 0.5172 - val_acc: 0.8579
Epoch 104/500
286s - loss: 0.3992 - acc: 0.8642 - val_loss: 0.5142 - val_acc: 0.8579
Epoch 105/500
286s - loss: 0.3732 - acc: 0.8624 - val_loss: 0.5124 - val_acc: 0.8589
Epoch 106/500
286s - loss: 0.3900 - acc: 0.8642 - val_loss: 0.5063 - val_acc: 0.8568
Epoch 107/500
286s - loss: 0.3818 - acc: 0.8679 - val_loss: 0.5160 - val_acc: 0.8589
Epoch 108/500
286s - loss: 0.3910 - acc: 0.8576 - val_loss: 0.5139 - val_acc: 0.8589
Epoch 109/500
286s - loss: 0.3857 - acc: 0.8687 - val_loss: 0.5153 - val_acc: 0.8558
Epoch 110/500
286s - loss: 0.3743 - acc: 0.8634 - val_loss: 0.5162 - val_acc: 0.8589
Epoch 111/500
286s - loss: 0.3947 - acc: 0.8634 - val_loss: 0.5085 - val_acc: 0.8579
Epoch 112/500
286s - loss: 0.3931 - acc: 0.8637 - val_loss: 0.5125 - val_acc: 0.8611
Epoch 113/500
286s - loss: 0.3656 - acc: 0.8661 - val_loss: 0.5146 - val_acc: 0.8579
Epoch 114/500
286s - loss: 0.3847 - acc: 0.8639 - val_loss: 0.5121 - val_acc: 0.8579
Epoch 115/500
286s - loss: 0.3838 - acc: 0.8595 - val_loss: 0.5148 - val_acc: 0.8589
Epoch 116/500
286s - loss: 0.3645 - acc: 0.8650 - val_loss: 0.5128 - val_acc: 0.8611
Epoch 117/500
286s - loss: 0.3826 - acc: 0.8600 - val_loss: 0.5113 - val_acc: 0.8589
Epoch 118/500
286s - loss: 0.3964 - acc: 0.8616 - val_loss: 0.5162 - val_acc: 0.8600
Epoch 119/500
286s - loss: 0.3734 - acc: 0.8634 - val_loss: 0.5142 - val_acc: 0.8589
Epoch 120/500
286s - loss: 0.3867 - acc: 0.8600 - val_loss: 0.5135 - val_acc: 0.8589
Epoch 121/500
286s - loss: 0.3605 - acc: 0.8671 - val_loss: 0.5191 - val_acc: 0.8568
Epoch 122/500
286s - loss: 0.3843 - acc: 0.8611 - val_loss: 0.5154 - val_acc: 0.8589
Epoch 123/500
286s - loss: 0.3729 - acc: 0.8674 - val_loss: 0.5134 - val_acc: 0.8589
Epoch 124/500
286s - loss: 0.3790 - acc: 0.8624 - val_loss: 0.5089 - val_acc: 0.8589
Epoch 125/500
286s - loss: 0.3728 - acc: 0.8655 - val_loss: 0.5112 - val_acc: 0.8589
Epoch 126/500
286s - loss: 0.3851 - acc: 0.8661 - val_loss: 0.5134 - val_acc: 0.8579
Epoch 127/500
286s - loss: 0.3807 - acc: 0.8642 - val_loss: 0.5165 - val_acc: 0.8558
Epoch 128/500
286s - loss: 0.3736 - acc: 0.8605 - val_loss: 0.5095 - val_acc: 0.8589
Epoch 129/500
286s - loss: 0.4033 - acc: 0.8579 - val_loss: 0.5114 - val_acc: 0.8600
Epoch 130/500
286s - loss: 0.3725 - acc: 0.8603 - val_loss: 0.5115 - val_acc: 0.8621
Epoch 131/500
286s - loss: 0.4146 - acc: 0.8589 - val_loss: 0.5091 - val_acc: 0.8589
Epoch 132/500
286s - loss: 0.3771 - acc: 0.8632 - val_loss: 0.5132 - val_acc: 0.8589
Epoch 133/500
286s - loss: 0.4043 - acc: 0.8550 - val_loss: 0.5155 - val_acc: 0.8579
Epoch 134/500
286s - loss: 0.3722 - acc: 0.8616 - val_loss: 0.5139 - val_acc: 0.8589
Epoch 135/500
286s - loss: 0.3908 - acc: 0.8611 - val_loss: 0.5151 - val_acc: 0.8579
Epoch 136/500
286s - loss: 0.4109 - acc: 0.8589 - val_loss: 0.5117 - val_acc: 0.8600
Epoch 137/500
286s - loss: 0.3828 - acc: 0.8603 - val_loss: 0.5089 - val_acc: 0.8589
Epoch 138/500
286s - loss: 0.3874 - acc: 0.8608 - val_loss: 0.5090 - val_acc: 0.8589
Epoch 139/500
286s - loss: 0.3884 - acc: 0.8597 - val_loss: 0.5140 - val_acc: 0.8611
Epoch 140/500
286s - loss: 0.3978 - acc: 0.8595 - val_loss: 0.5142 - val_acc: 0.8589
Epoch 141/500
287s - loss: 0.3721 - acc: 0.8637 - val_loss: 0.5142 - val_acc: 0.8579
Epoch 142/500
287s - loss: 0.4200 - acc: 0.8592 - val_loss: 0.5111 - val_acc: 0.8600
Epoch 143/500
286s - loss: 0.3900 - acc: 0.8616 - val_loss: 0.5109 - val_acc: 0.8589
Epoch 144/500
286s - loss: 0.3726 - acc: 0.8650 - val_loss: 0.5111 - val_acc: 0.8600
Epoch 145/500
286s - loss: 0.3712 - acc: 0.8637 - val_loss: 0.5170 - val_acc: 0.8589
Epoch 146/500
286s - loss: 0.3816 - acc: 0.8587 - val_loss: 0.5188 - val_acc: 0.8579
Epoch 147/500
287s - loss: 0.4038 - acc: 0.8616 - val_loss: 0.5178 - val_acc: 0.8579
Epoch 148/500
286s - loss: 0.4124 - acc: 0.8605 - val_loss: 0.5140 - val_acc: 0.8600
Epoch 149/500
286s - loss: 0.3898 - acc: 0.8574 - val_loss: 0.5137 - val_acc: 0.8600
Epoch 150/500
287s - loss: 0.3663 - acc: 0.8692 - val_loss: 0.5173 - val_acc: 0.8579
Epoch 151/500
287s - loss: 0.3808 - acc: 0.8618 - val_loss: 0.5138 - val_acc: 0.8579
Epoch 152/500
286s - loss: 0.3866 - acc: 0.8668 - val_loss: 0.5153 - val_acc: 0.8589
Epoch 153/500
286s - loss: 0.3579 - acc: 0.8634 - val_loss: 0.5138 - val_acc: 0.8600
Epoch 154/500
287s - loss: 0.3842 - acc: 0.8634 - val_loss: 0.5120 - val_acc: 0.8589
Epoch 155/500
286s - loss: 0.3934 - acc: 0.8613 - val_loss: 0.5124 - val_acc: 0.8579
Epoch 156/500
287s - loss: 0.3830 - acc: 0.8666 - val_loss: 0.5164 - val_acc: 0.8579
Epoch 157/500
286s - loss: 0.3738 - acc: 0.8679 - val_loss: 0.5182 - val_acc: 0.8568
Epoch 158/500
287s - loss: 0.4077 - acc: 0.8621 - val_loss: 0.5126 - val_acc: 0.8600
Epoch 159/500
286s - loss: 0.3861 - acc: 0.8618 - val_loss: 0.5122 - val_acc: 0.8600
Epoch 160/500
286s - loss: 0.3821 - acc: 0.8600 - val_loss: 0.5123 - val_acc: 0.8600
Epoch 161/500
287s - loss: 0.4038 - acc: 0.8561 - val_loss: 0.5109 - val_acc: 0.8589
Epoch 162/500
286s - loss: 0.4084 - acc: 0.8603 - val_loss: 0.5179 - val_acc: 0.8579
Epoch 163/500
286s - loss: 0.3853 - acc: 0.8642 - val_loss: 0.5139 - val_acc: 0.8579
Epoch 164/500
286s - loss: 0.3587 - acc: 0.8684 - val_loss: 0.5163 - val_acc: 0.8579
Epoch 165/500
286s - loss: 0.3905 - acc: 0.8584 - val_loss: 0.5151 - val_acc: 0.8589
Epoch 166/500
286s - loss: 0.3825 - acc: 0.8642 - val_loss: 0.5100 - val_acc: 0.8611
Epoch 167/500
286s - loss: 0.3743 - acc: 0.8676 - val_loss: 0.5092 - val_acc: 0.8611
Training loss for fold 5 is 0.20809626614576893 with percent 92.0
Testing loss for fold 5 is 0.511463904223944 with percent 86.2105263785312
 
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_13 (InputLayer)            (None, 85, 85, 3)     0                                            
____________________________________________________________________________________________________
conv2d_37 (Conv2D)               (None, 83, 83, 16)    448         input_13[0][0]                   
____________________________________________________________________________________________________
batch_normalization_43 (BatchNor (None, 83, 83, 16)    64          conv2d_37[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_43 (LeakyReLU)       (None, 83, 83, 16)    0           batch_normalization_43[0][0]     
____________________________________________________________________________________________________
conv2d_38 (Conv2D)               (None, 82, 82, 16)    1040        leaky_re_lu_43[0][0]             
____________________________________________________________________________________________________
batch_normalization_44 (BatchNor (None, 82, 82, 16)    64          conv2d_38[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_44 (LeakyReLU)       (None, 82, 82, 16)    0           batch_normalization_44[0][0]     
____________________________________________________________________________________________________
max_pooling2d_19 (MaxPooling2D)  (None, 41, 41, 16)    0           leaky_re_lu_44[0][0]             
____________________________________________________________________________________________________
dropout_31 (Dropout)             (None, 41, 41, 16)    0           max_pooling2d_19[0][0]           
____________________________________________________________________________________________________
conv2d_39 (Conv2D)               (None, 39, 39, 32)    4640        dropout_31[0][0]                 
____________________________________________________________________________________________________
batch_normalization_45 (BatchNor (None, 39, 39, 32)    128         conv2d_39[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_45 (LeakyReLU)       (None, 39, 39, 32)    0           batch_normalization_45[0][0]     
____________________________________________________________________________________________________
conv2d_40 (Conv2D)               (None, 38, 38, 32)    4128        leaky_re_lu_45[0][0]             
____________________________________________________________________________________________________
batch_normalization_46 (BatchNor (None, 38, 38, 32)    128         conv2d_40[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_46 (LeakyReLU)       (None, 38, 38, 32)    0           batch_normalization_46[0][0]     
____________________________________________________________________________________________________
max_pooling2d_20 (MaxPooling2D)  (None, 19, 19, 32)    0           leaky_re_lu_46[0][0]             
____________________________________________________________________________________________________
dropout_32 (Dropout)             (None, 19, 19, 32)    0           max_pooling2d_20[0][0]           
____________________________________________________________________________________________________
conv2d_41 (Conv2D)               (None, 17, 17, 64)    18496       dropout_32[0][0]                 
____________________________________________________________________________________________________
batch_normalization_47 (BatchNor (None, 17, 17, 64)    256         conv2d_41[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_47 (LeakyReLU)       (None, 17, 17, 64)    0           batch_normalization_47[0][0]     
____________________________________________________________________________________________________
conv2d_42 (Conv2D)               (None, 16, 16, 64)    16448       leaky_re_lu_47[0][0]             
____________________________________________________________________________________________________
batch_normalization_48 (BatchNor (None, 16, 16, 64)    256         conv2d_42[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_48 (LeakyReLU)       (None, 16, 16, 64)    0           batch_normalization_48[0][0]     
____________________________________________________________________________________________________
max_pooling2d_21 (MaxPooling2D)  (None, 8, 8, 64)      0           leaky_re_lu_48[0][0]             
____________________________________________________________________________________________________
dropout_33 (Dropout)             (None, 8, 8, 64)      0           max_pooling2d_21[0][0]           
____________________________________________________________________________________________________
flatten_7 (Flatten)              (None, 4096)          0           dropout_33[0][0]                 
____________________________________________________________________________________________________
batch_normalization_49 (BatchNor (None, 4096)          16384       flatten_7[0][0]                  
____________________________________________________________________________________________________
input_14 (InputLayer)            (None, 102)           0                                            
____________________________________________________________________________________________________
concatenate_7 (Concatenate)      (None, 4198)          0           batch_normalization_49[0][0]     
                                                                   input_14[0][0]                   
____________________________________________________________________________________________________
dense_25 (Dense)                 (None, 128)           537472      concatenate_7[0][0]              
____________________________________________________________________________________________________
leaky_re_lu_49 (LeakyReLU)       (None, 128)           0           dense_25[0][0]                   
____________________________________________________________________________________________________
dense_26 (Dense)                 (None, 64)            8256        leaky_re_lu_49[0][0]             
____________________________________________________________________________________________________
dropout_34 (Dropout)             (None, 64)            0           dense_26[0][0]                   
____________________________________________________________________________________________________
dense_27 (Dense)                 (None, 32)            2080        dropout_34[0][0]                 
____________________________________________________________________________________________________
dropout_35 (Dropout)             (None, 32)            0           dense_27[0][0]                   
____________________________________________________________________________________________________
dense_28 (Dense)                 (None, 12)            396         dropout_35[0][0]                 
====================================================================================================
Total params: 610,684
Trainable params: 602,044
Non-trainable params: 8,640
____________________________________________________________________________________________________
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
290s - loss: 2.1254 - acc: 0.2905 - val_loss: 3.9483 - val_acc: 0.1537
Epoch 2/500
285s - loss: 1.6450 - acc: 0.4537 - val_loss: 2.1051 - val_acc: 0.3642
Epoch 3/500
285s - loss: 1.4916 - acc: 0.5142 - val_loss: 3.3770 - val_acc: 0.2558
Epoch 4/500
285s - loss: 1.3942 - acc: 0.5661 - val_loss: 1.4832 - val_acc: 0.5295
Epoch 5/500
285s - loss: 1.2462 - acc: 0.6150 - val_loss: 5.7632 - val_acc: 0.1695
Epoch 6/500
285s - loss: 1.1974 - acc: 0.6387 - val_loss: 2.3635 - val_acc: 0.5537
Epoch 7/500
285s - loss: 1.1663 - acc: 0.6645 - val_loss: 2.3548 - val_acc: 0.4516
Epoch 8/500
285s - loss: 1.1408 - acc: 0.6761 - val_loss: 2.0466 - val_acc: 0.4326
Epoch 9/500
285s - loss: 1.1529 - acc: 0.6705 - val_loss: 3.5608 - val_acc: 0.2874
Epoch 10/500
285s - loss: 1.0607 - acc: 0.6947 - val_loss: 3.6338 - val_acc: 0.4189
Epoch 11/500
285s - loss: 1.1286 - acc: 0.6839 - val_loss: 1.6311 - val_acc: 0.5232
Epoch 12/500
285s - loss: 1.0705 - acc: 0.7111 - val_loss: 1.5843 - val_acc: 0.5147
Epoch 13/500
285s - loss: 0.9802 - acc: 0.7321 - val_loss: 1.1613 - val_acc: 0.7179
Epoch 14/500
285s - loss: 1.0356 - acc: 0.7121 - val_loss: 1.7352 - val_acc: 0.6516
Epoch 15/500
286s - loss: 0.9896 - acc: 0.7224 - val_loss: 1.4144 - val_acc: 0.5884
Epoch 16/500
286s - loss: 1.0447 - acc: 0.7311 - val_loss: 1.3021 - val_acc: 0.7253
Epoch 17/500
285s - loss: 1.0220 - acc: 0.7295 - val_loss: 1.5670 - val_acc: 0.6800
Epoch 18/500
284s - loss: 1.0820 - acc: 0.7129 - val_loss: 1.1880 - val_acc: 0.7074
Epoch 19/500
285s - loss: 1.0203 - acc: 0.7297 - val_loss: 1.4143 - val_acc: 0.6337
Epoch 20/500
284s - loss: 1.0200 - acc: 0.7384 - val_loss: 2.9839 - val_acc: 0.5074
Epoch 21/500
284s - loss: 1.1673 - acc: 0.7087 - val_loss: 1.6168 - val_acc: 0.6684
Epoch 22/500
284s - loss: 1.0924 - acc: 0.7266 - val_loss: 1.4126 - val_acc: 0.6800
Epoch 23/500
284s - loss: 1.0935 - acc: 0.7266 - val_loss: 3.6795 - val_acc: 0.5147
Epoch 24/500
284s - loss: 1.0729 - acc: 0.7474 - val_loss: 3.3774 - val_acc: 0.5505
Epoch 25/500

Epoch 00024: reducing learning rate to 0.010000000149.
285s - loss: 1.0478 - acc: 0.7397 - val_loss: 1.7979 - val_acc: 0.6842
Epoch 26/500
284s - loss: 0.8477 - acc: 0.7779 - val_loss: 1.0000 - val_acc: 0.7737
Epoch 27/500
285s - loss: 0.6783 - acc: 0.8124 - val_loss: 0.8546 - val_acc: 0.8074
Epoch 28/500
284s - loss: 0.6487 - acc: 0.8163 - val_loss: 0.8842 - val_acc: 0.8042
Epoch 29/500
285s - loss: 0.5844 - acc: 0.8268 - val_loss: 0.8318 - val_acc: 0.8147
Epoch 30/500
285s - loss: 0.5315 - acc: 0.8408 - val_loss: 0.8800 - val_acc: 0.8232
Epoch 31/500
284s - loss: 0.5392 - acc: 0.8500 - val_loss: 0.7780 - val_acc: 0.8242
Epoch 32/500
285s - loss: 0.5322 - acc: 0.8497 - val_loss: 0.7866 - val_acc: 0.8263
Epoch 33/500
285s - loss: 0.5053 - acc: 0.8534 - val_loss: 0.7368 - val_acc: 0.8411
Epoch 34/500
284s - loss: 0.4868 - acc: 0.8597 - val_loss: 0.7450 - val_acc: 0.8368
Epoch 35/500
285s - loss: 0.4667 - acc: 0.8574 - val_loss: 0.7772 - val_acc: 0.8200
Epoch 36/500
285s - loss: 0.4299 - acc: 0.8624 - val_loss: 0.6711 - val_acc: 0.8484
Epoch 37/500
285s - loss: 0.4754 - acc: 0.8584 - val_loss: 0.7174 - val_acc: 0.8379
Epoch 38/500
285s - loss: 0.4640 - acc: 0.8650 - val_loss: 0.7635 - val_acc: 0.8295
Epoch 39/500
285s - loss: 0.4483 - acc: 0.8587 - val_loss: 0.7080 - val_acc: 0.8326
Epoch 40/500
285s - loss: 0.4039 - acc: 0.8721 - val_loss: 0.7512 - val_acc: 0.8347
Epoch 41/500
285s - loss: 0.3921 - acc: 0.8795 - val_loss: 0.6981 - val_acc: 0.8537
Epoch 42/500
285s - loss: 0.4008 - acc: 0.8742 - val_loss: 0.6675 - val_acc: 0.8547
Epoch 43/500
284s - loss: 0.3789 - acc: 0.8789 - val_loss: 0.6628 - val_acc: 0.8474
Epoch 44/500
284s - loss: 0.4117 - acc: 0.8697 - val_loss: 0.6471 - val_acc: 0.8526
Epoch 45/500
285s - loss: 0.3603 - acc: 0.8808 - val_loss: 0.6561 - val_acc: 0.8505
Epoch 46/500
285s - loss: 0.3968 - acc: 0.8753 - val_loss: 0.6626 - val_acc: 0.8495
Epoch 47/500
285s - loss: 0.3697 - acc: 0.8789 - val_loss: 0.7095 - val_acc: 0.8453
Epoch 48/500
285s - loss: 0.3474 - acc: 0.8858 - val_loss: 0.7513 - val_acc: 0.8379
Epoch 49/500
285s - loss: 0.3410 - acc: 0.8861 - val_loss: 0.6831 - val_acc: 0.8389
Epoch 50/500
285s - loss: 0.3436 - acc: 0.8792 - val_loss: 0.7152 - val_acc: 0.8347
Epoch 51/500

Epoch 00050: reducing learning rate to 0.000999999977648.
285s - loss: 0.3647 - acc: 0.8813 - val_loss: 0.7915 - val_acc: 0.8463
Epoch 52/500
285s - loss: 0.3210 - acc: 0.8908 - val_loss: 0.6837 - val_acc: 0.8547
Epoch 53/500
285s - loss: 0.3330 - acc: 0.8876 - val_loss: 0.6394 - val_acc: 0.8505
Epoch 54/500
285s - loss: 0.3245 - acc: 0.8892 - val_loss: 0.6349 - val_acc: 0.8537
Epoch 55/500
285s - loss: 0.3243 - acc: 0.8926 - val_loss: 0.6259 - val_acc: 0.8537
Epoch 56/500
285s - loss: 0.3243 - acc: 0.8921 - val_loss: 0.6254 - val_acc: 0.8537
Epoch 57/500
285s - loss: 0.3159 - acc: 0.8905 - val_loss: 0.6147 - val_acc: 0.8568
Epoch 58/500
285s - loss: 0.3387 - acc: 0.8855 - val_loss: 0.6282 - val_acc: 0.8558
Epoch 59/500
285s - loss: 0.3194 - acc: 0.8900 - val_loss: 0.6252 - val_acc: 0.8558
Epoch 60/500
285s - loss: 0.3261 - acc: 0.8832 - val_loss: 0.6143 - val_acc: 0.8547
Epoch 61/500
285s - loss: 0.3265 - acc: 0.8884 - val_loss: 0.6157 - val_acc: 0.8568
Epoch 62/500
285s - loss: 0.3337 - acc: 0.8845 - val_loss: 0.6127 - val_acc: 0.8568
Epoch 63/500
285s - loss: 0.3072 - acc: 0.8861 - val_loss: 0.6376 - val_acc: 0.8537
Epoch 64/500
285s - loss: 0.3294 - acc: 0.8937 - val_loss: 0.6323 - val_acc: 0.8558
Epoch 65/500
285s - loss: 0.3243 - acc: 0.8884 - val_loss: 0.6149 - val_acc: 0.8568
Epoch 66/500

Epoch 00065: reducing learning rate to 9.99999931082e-05.
285s - loss: 0.3242 - acc: 0.8947 - val_loss: 0.6195 - val_acc: 0.8568
Epoch 67/500
285s - loss: 0.3110 - acc: 0.8942 - val_loss: 0.6116 - val_acc: 0.8579
Epoch 68/500
285s - loss: 0.3064 - acc: 0.8934 - val_loss: 0.6184 - val_acc: 0.8579
Epoch 69/500
285s - loss: 0.3009 - acc: 0.8924 - val_loss: 0.6175 - val_acc: 0.8579
Epoch 70/500
285s - loss: 0.2937 - acc: 0.8934 - val_loss: 0.6119 - val_acc: 0.8568
Epoch 71/500
285s - loss: 0.3272 - acc: 0.8837 - val_loss: 0.6242 - val_acc: 0.8558
Epoch 72/500
285s - loss: 0.3119 - acc: 0.8847 - val_loss: 0.6155 - val_acc: 0.8579
Epoch 73/500
285s - loss: 0.3175 - acc: 0.8947 - val_loss: 0.6255 - val_acc: 0.8579
Epoch 74/500
285s - loss: 0.3402 - acc: 0.8861 - val_loss: 0.6163 - val_acc: 0.8589
Epoch 75/500
284s - loss: 0.2907 - acc: 0.8889 - val_loss: 0.6049 - val_acc: 0.8568
Epoch 76/500
285s - loss: 0.3146 - acc: 0.8905 - val_loss: 0.6168 - val_acc: 0.8579
Epoch 77/500
285s - loss: 0.2954 - acc: 0.8974 - val_loss: 0.6164 - val_acc: 0.8579
Epoch 78/500
285s - loss: 0.2887 - acc: 0.8937 - val_loss: 0.6144 - val_acc: 0.8558
Epoch 79/500
285s - loss: 0.2969 - acc: 0.8976 - val_loss: 0.6160 - val_acc: 0.8579
Epoch 80/500
285s - loss: 0.3433 - acc: 0.8850 - val_loss: 0.6181 - val_acc: 0.8579
Epoch 81/500
285s - loss: 0.2982 - acc: 0.8929 - val_loss: 0.6058 - val_acc: 0.8558
Epoch 82/500
285s - loss: 0.3262 - acc: 0.8908 - val_loss: 0.6185 - val_acc: 0.8579
Epoch 83/500

Epoch 00082: reducing learning rate to 9.99999901978e-06.
285s - loss: 0.3239 - acc: 0.8868 - val_loss: 0.6187 - val_acc: 0.8579
Epoch 84/500
285s - loss: 0.3086 - acc: 0.8908 - val_loss: 0.6208 - val_acc: 0.8579
Epoch 85/500
285s - loss: 0.2855 - acc: 0.8997 - val_loss: 0.6159 - val_acc: 0.8579
Epoch 86/500
285s - loss: 0.3383 - acc: 0.8868 - val_loss: 0.6164 - val_acc: 0.8579
Epoch 87/500
284s - loss: 0.3190 - acc: 0.8963 - val_loss: 0.6228 - val_acc: 0.8579
Epoch 88/500
285s - loss: 0.2979 - acc: 0.8947 - val_loss: 0.6174 - val_acc: 0.8579
Epoch 89/500
285s - loss: 0.2994 - acc: 0.8918 - val_loss: 0.6185 - val_acc: 0.8568
Epoch 90/500
285s - loss: 0.3300 - acc: 0.8874 - val_loss: 0.6146 - val_acc: 0.8568
Epoch 91/500

Epoch 00090: reducing learning rate to 1e-06.
285s - loss: 0.2788 - acc: 0.8966 - val_loss: 0.6185 - val_acc: 0.8579
Epoch 92/500
285s - loss: 0.3029 - acc: 0.8874 - val_loss: 0.6180 - val_acc: 0.8568
Epoch 93/500
285s - loss: 0.3106 - acc: 0.8921 - val_loss: 0.6116 - val_acc: 0.8579
Epoch 94/500
285s - loss: 0.3153 - acc: 0.8958 - val_loss: 0.6094 - val_acc: 0.8579
Epoch 95/500
285s - loss: 0.3147 - acc: 0.8897 - val_loss: 0.6145 - val_acc: 0.8579
Epoch 96/500
285s - loss: 0.2929 - acc: 0.8929 - val_loss: 0.6104 - val_acc: 0.8568
Epoch 97/500
285s - loss: 0.3358 - acc: 0.8918 - val_loss: 0.6116 - val_acc: 0.8568
Epoch 98/500
285s - loss: 0.2958 - acc: 0.8929 - val_loss: 0.6121 - val_acc: 0.8568
Epoch 99/500
285s - loss: 0.2984 - acc: 0.8974 - val_loss: 0.6095 - val_acc: 0.8547
Epoch 100/500
285s - loss: 0.3122 - acc: 0.8897 - val_loss: 0.6156 - val_acc: 0.8568
Epoch 101/500
285s - loss: 0.3214 - acc: 0.8908 - val_loss: 0.6155 - val_acc: 0.8568
Epoch 102/500
285s - loss: 0.3528 - acc: 0.8829 - val_loss: 0.6070 - val_acc: 0.8568
Epoch 103/500
285s - loss: 0.2935 - acc: 0.8916 - val_loss: 0.6199 - val_acc: 0.8579
Epoch 104/500
285s - loss: 0.2975 - acc: 0.8942 - val_loss: 0.6177 - val_acc: 0.8568
Epoch 105/500
285s - loss: 0.3245 - acc: 0.8889 - val_loss: 0.6283 - val_acc: 0.8558
Epoch 106/500
285s - loss: 0.3236 - acc: 0.8892 - val_loss: 0.6205 - val_acc: 0.8579
Epoch 107/500
284s - loss: 0.3211 - acc: 0.8837 - val_loss: 0.6097 - val_acc: 0.8568
Epoch 108/500
285s - loss: 0.3316 - acc: 0.8903 - val_loss: 0.6184 - val_acc: 0.8579
Epoch 109/500
285s - loss: 0.3080 - acc: 0.8947 - val_loss: 0.6200 - val_acc: 0.8579
Epoch 110/500
285s - loss: 0.3240 - acc: 0.8955 - val_loss: 0.6242 - val_acc: 0.8579
Epoch 111/500
285s - loss: 0.3071 - acc: 0.8937 - val_loss: 0.6155 - val_acc: 0.8568
Epoch 112/500
285s - loss: 0.3058 - acc: 0.8939 - val_loss: 0.6187 - val_acc: 0.8568
Epoch 113/500
285s - loss: 0.2939 - acc: 0.8942 - val_loss: 0.6110 - val_acc: 0.8568
Epoch 114/500
285s - loss: 0.2993 - acc: 0.8911 - val_loss: 0.6186 - val_acc: 0.8568
Epoch 115/500
285s - loss: 0.3106 - acc: 0.8913 - val_loss: 0.6163 - val_acc: 0.8579
Epoch 116/500
285s - loss: 0.3095 - acc: 0.8961 - val_loss: 0.6163 - val_acc: 0.8579
Epoch 117/500
284s - loss: 0.3284 - acc: 0.8934 - val_loss: 0.6165 - val_acc: 0.8568
Epoch 118/500
285s - loss: 0.2981 - acc: 0.8934 - val_loss: 0.6217 - val_acc: 0.8579
Epoch 119/500
285s - loss: 0.3124 - acc: 0.8966 - val_loss: 0.6169 - val_acc: 0.8579
Epoch 120/500
285s - loss: 0.3267 - acc: 0.8897 - val_loss: 0.6099 - val_acc: 0.8568
Epoch 121/500
285s - loss: 0.3061 - acc: 0.8937 - val_loss: 0.6170 - val_acc: 0.8579
Epoch 122/500
285s - loss: 0.3097 - acc: 0.8937 - val_loss: 0.6132 - val_acc: 0.8579
Epoch 123/500
285s - loss: 0.3221 - acc: 0.8895 - val_loss: 0.6240 - val_acc: 0.8579
Epoch 124/500
285s - loss: 0.2955 - acc: 0.9008 - val_loss: 0.6248 - val_acc: 0.8579
Epoch 125/500
285s - loss: 0.2992 - acc: 0.8934 - val_loss: 0.6180 - val_acc: 0.8568
Epoch 126/500
285s - loss: 0.3316 - acc: 0.8829 - val_loss: 0.6197 - val_acc: 0.8579
Epoch 127/500
285s - loss: 0.2831 - acc: 0.8976 - val_loss: 0.6223 - val_acc: 0.8579
Epoch 128/500
285s - loss: 0.3225 - acc: 0.8926 - val_loss: 0.6171 - val_acc: 0.8579
Epoch 129/500
285s - loss: 0.2943 - acc: 0.8950 - val_loss: 0.6102 - val_acc: 0.8568
Epoch 130/500
284s - loss: 0.3160 - acc: 0.8942 - val_loss: 0.6196 - val_acc: 0.8579
Epoch 131/500
284s - loss: 0.3066 - acc: 0.8868 - val_loss: 0.6164 - val_acc: 0.8579
Epoch 132/500
285s - loss: 0.3158 - acc: 0.8921 - val_loss: 0.6230 - val_acc: 0.8568
Epoch 133/500
285s - loss: 0.3153 - acc: 0.8918 - val_loss: 0.6231 - val_acc: 0.8579
Epoch 134/500
285s - loss: 0.3125 - acc: 0.8889 - val_loss: 0.6125 - val_acc: 0.8579
Epoch 135/500
285s - loss: 0.2944 - acc: 0.8916 - val_loss: 0.6101 - val_acc: 0.8558
Epoch 136/500
285s - loss: 0.3290 - acc: 0.8911 - val_loss: 0.6208 - val_acc: 0.8558
Training loss for fold 6 is 0.17678394638394054 with percent 92.36842106517993
Testing loss for fold 6 is 0.6163118968825592 with percent 85.894736904847
 
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_15 (InputLayer)            (None, 85, 85, 3)     0                                            
____________________________________________________________________________________________________
conv2d_43 (Conv2D)               (None, 83, 83, 16)    448         input_15[0][0]                   
____________________________________________________________________________________________________
batch_normalization_50 (BatchNor (None, 83, 83, 16)    64          conv2d_43[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_50 (LeakyReLU)       (None, 83, 83, 16)    0           batch_normalization_50[0][0]     
____________________________________________________________________________________________________
conv2d_44 (Conv2D)               (None, 82, 82, 16)    1040        leaky_re_lu_50[0][0]             
____________________________________________________________________________________________________
batch_normalization_51 (BatchNor (None, 82, 82, 16)    64          conv2d_44[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_51 (LeakyReLU)       (None, 82, 82, 16)    0           batch_normalization_51[0][0]     
____________________________________________________________________________________________________
max_pooling2d_22 (MaxPooling2D)  (None, 41, 41, 16)    0           leaky_re_lu_51[0][0]             
____________________________________________________________________________________________________
dropout_36 (Dropout)             (None, 41, 41, 16)    0           max_pooling2d_22[0][0]           
____________________________________________________________________________________________________
conv2d_45 (Conv2D)               (None, 39, 39, 32)    4640        dropout_36[0][0]                 
____________________________________________________________________________________________________
batch_normalization_52 (BatchNor (None, 39, 39, 32)    128         conv2d_45[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_52 (LeakyReLU)       (None, 39, 39, 32)    0           batch_normalization_52[0][0]     
____________________________________________________________________________________________________
conv2d_46 (Conv2D)               (None, 38, 38, 32)    4128        leaky_re_lu_52[0][0]             
____________________________________________________________________________________________________
batch_normalization_53 (BatchNor (None, 38, 38, 32)    128         conv2d_46[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_53 (LeakyReLU)       (None, 38, 38, 32)    0           batch_normalization_53[0][0]     
____________________________________________________________________________________________________
max_pooling2d_23 (MaxPooling2D)  (None, 19, 19, 32)    0           leaky_re_lu_53[0][0]             
____________________________________________________________________________________________________
dropout_37 (Dropout)             (None, 19, 19, 32)    0           max_pooling2d_23[0][0]           
____________________________________________________________________________________________________
conv2d_47 (Conv2D)               (None, 17, 17, 64)    18496       dropout_37[0][0]                 
____________________________________________________________________________________________________
batch_normalization_54 (BatchNor (None, 17, 17, 64)    256         conv2d_47[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_54 (LeakyReLU)       (None, 17, 17, 64)    0           batch_normalization_54[0][0]     
____________________________________________________________________________________________________
conv2d_48 (Conv2D)               (None, 16, 16, 64)    16448       leaky_re_lu_54[0][0]             
____________________________________________________________________________________________________
batch_normalization_55 (BatchNor (None, 16, 16, 64)    256         conv2d_48[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_55 (LeakyReLU)       (None, 16, 16, 64)    0           batch_normalization_55[0][0]     
____________________________________________________________________________________________________
max_pooling2d_24 (MaxPooling2D)  (None, 8, 8, 64)      0           leaky_re_lu_55[0][0]             
____________________________________________________________________________________________________
dropout_38 (Dropout)             (None, 8, 8, 64)      0           max_pooling2d_24[0][0]           
____________________________________________________________________________________________________
flatten_8 (Flatten)              (None, 4096)          0           dropout_38[0][0]                 
____________________________________________________________________________________________________
batch_normalization_56 (BatchNor (None, 4096)          16384       flatten_8[0][0]                  
____________________________________________________________________________________________________
input_16 (InputLayer)            (None, 102)           0                                            
____________________________________________________________________________________________________
concatenate_8 (Concatenate)      (None, 4198)          0           batch_normalization_56[0][0]     
                                                                   input_16[0][0]                   
____________________________________________________________________________________________________
dense_29 (Dense)                 (None, 128)           537472      concatenate_8[0][0]              
____________________________________________________________________________________________________
leaky_re_lu_56 (LeakyReLU)       (None, 128)           0           dense_29[0][0]                   
____________________________________________________________________________________________________
dense_30 (Dense)                 (None, 64)            8256        leaky_re_lu_56[0][0]             
____________________________________________________________________________________________________
dropout_39 (Dropout)             (None, 64)            0           dense_30[0][0]                   
____________________________________________________________________________________________________
dense_31 (Dense)                 (None, 32)            2080        dropout_39[0][0]                 
____________________________________________________________________________________________________
dropout_40 (Dropout)             (None, 32)            0           dense_31[0][0]                   
____________________________________________________________________________________________________
dense_32 (Dense)                 (None, 12)            396         dropout_40[0][0]                 
====================================================================================================
Total params: 610,684
Trainable params: 602,044
Non-trainable params: 8,640
____________________________________________________________________________________________________
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
289s - loss: 2.0177 - acc: 0.3129 - val_loss: 2.5067 - val_acc: 0.1726
Epoch 2/500
284s - loss: 1.6078 - acc: 0.4732 - val_loss: 2.0959 - val_acc: 0.2853
Epoch 3/500
284s - loss: 1.4456 - acc: 0.5437 - val_loss: 2.3556 - val_acc: 0.2958
Epoch 4/500
284s - loss: 1.3679 - acc: 0.5787 - val_loss: 2.9558 - val_acc: 0.2484
Epoch 5/500
284s - loss: 1.3267 - acc: 0.5963 - val_loss: 2.4286 - val_acc: 0.3884
Epoch 6/500
285s - loss: 1.2534 - acc: 0.6216 - val_loss: 0.9868 - val_acc: 0.7126
Epoch 7/500
284s - loss: 1.2430 - acc: 0.6392 - val_loss: 3.2539 - val_acc: 0.2705
Epoch 8/500
284s - loss: 1.2631 - acc: 0.6266 - val_loss: 1.0763 - val_acc: 0.6695
Epoch 9/500
289s - loss: 1.1857 - acc: 0.6621 - val_loss: 1.6700 - val_acc: 0.5516
Epoch 10/500
290s - loss: 1.1588 - acc: 0.6795 - val_loss: 1.1269 - val_acc: 0.6874
Epoch 11/500
290s - loss: 1.1148 - acc: 0.6942 - val_loss: 1.9762 - val_acc: 0.4832
Epoch 12/500
288s - loss: 1.0611 - acc: 0.7142 - val_loss: 1.4118 - val_acc: 0.6326
Epoch 13/500
285s - loss: 1.1440 - acc: 0.6850 - val_loss: 2.3273 - val_acc: 0.5179
Epoch 14/500
285s - loss: 1.1035 - acc: 0.6984 - val_loss: 2.3992 - val_acc: 0.4358
Epoch 15/500

Epoch 00014: reducing learning rate to 0.010000000149.
285s - loss: 1.0898 - acc: 0.7011 - val_loss: 1.8403 - val_acc: 0.6926
Epoch 16/500
285s - loss: 0.9000 - acc: 0.7471 - val_loss: 0.8586 - val_acc: 0.7684
Epoch 17/500
284s - loss: 0.7131 - acc: 0.7863 - val_loss: 0.7325 - val_acc: 0.7874
Epoch 18/500
284s - loss: 0.6602 - acc: 0.7992 - val_loss: 0.5868 - val_acc: 0.8232
Epoch 19/500
289s - loss: 0.6329 - acc: 0.8111 - val_loss: 0.6203 - val_acc: 0.8242
Epoch 20/500
286s - loss: 0.5869 - acc: 0.8192 - val_loss: 0.7048 - val_acc: 0.8137
Epoch 21/500
284s - loss: 0.5638 - acc: 0.8274 - val_loss: 0.6568 - val_acc: 0.8074
Epoch 22/500
285s - loss: 0.5453 - acc: 0.8313 - val_loss: 0.5603 - val_acc: 0.8432
Epoch 23/500
285s - loss: 0.5245 - acc: 0.8289 - val_loss: 0.5279 - val_acc: 0.8453
Epoch 24/500
284s - loss: 0.5176 - acc: 0.8363 - val_loss: 0.5696 - val_acc: 0.8400
Epoch 25/500
284s - loss: 0.5040 - acc: 0.8424 - val_loss: 0.7158 - val_acc: 0.8189
Epoch 26/500
285s - loss: 0.4913 - acc: 0.8453 - val_loss: 0.5401 - val_acc: 0.8526
Epoch 27/500
284s - loss: 0.4722 - acc: 0.8516 - val_loss: 0.6987 - val_acc: 0.8053
Epoch 28/500
285s - loss: 0.4819 - acc: 0.8487 - val_loss: 0.6169 - val_acc: 0.8379
Epoch 29/500
284s - loss: 0.4498 - acc: 0.8518 - val_loss: 0.6450 - val_acc: 0.8274
Epoch 30/500
284s - loss: 0.4405 - acc: 0.8532 - val_loss: 0.6170 - val_acc: 0.8411
Epoch 31/500
284s - loss: 0.4428 - acc: 0.8600 - val_loss: 0.5002 - val_acc: 0.8579
Epoch 32/500
284s - loss: 0.4288 - acc: 0.8571 - val_loss: 0.6293 - val_acc: 0.8484
Epoch 33/500
284s - loss: 0.4157 - acc: 0.8605 - val_loss: 0.6698 - val_acc: 0.8263
Epoch 34/500
285s - loss: 0.4203 - acc: 0.8621 - val_loss: 0.5557 - val_acc: 0.8558
Epoch 35/500
285s - loss: 0.4098 - acc: 0.8645 - val_loss: 0.5851 - val_acc: 0.8600
Epoch 36/500
284s - loss: 0.3890 - acc: 0.8682 - val_loss: 0.5570 - val_acc: 0.8632
Epoch 37/500
286s - loss: 0.3934 - acc: 0.8695 - val_loss: 0.6520 - val_acc: 0.8379
Epoch 38/500
285s - loss: 0.4100 - acc: 0.8687 - val_loss: 0.5795 - val_acc: 0.8611
Epoch 39/500
284s - loss: 0.3805 - acc: 0.8758 - val_loss: 0.6399 - val_acc: 0.8495
Epoch 40/500
285s - loss: 0.3666 - acc: 0.8700 - val_loss: 0.5482 - val_acc: 0.8653
Epoch 41/500
284s - loss: 0.3622 - acc: 0.8737 - val_loss: 0.6751 - val_acc: 0.8474
Epoch 42/500
