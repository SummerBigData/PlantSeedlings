Using TensorFlow backend.
/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
 
Did not find train data with correct size. Generating...
 
/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.
  warn("The default mode, 'constant', will be changed to 'reflect' in "
Done. Loading train images
 
 
Did not find test data with correct size. Generating...
 
Done. Loading test images
 
Augmentation data size (4750, 102) (794, 102)
/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)
(3800, 85, 85, 3) (3800,)
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_1 (InputLayer)             (None, 85, 85, 3)     0                                            
____________________________________________________________________________________________________
conv2d_1 (Conv2D)                (None, 83, 83, 16)    448         input_1[0][0]                    
____________________________________________________________________________________________________
batch_normalization_1 (BatchNorm (None, 83, 83, 16)    64          conv2d_1[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_1 (LeakyReLU)        (None, 83, 83, 16)    0           batch_normalization_1[0][0]      
____________________________________________________________________________________________________
conv2d_2 (Conv2D)                (None, 82, 82, 16)    1040        leaky_re_lu_1[0][0]              
____________________________________________________________________________________________________
batch_normalization_2 (BatchNorm (None, 82, 82, 16)    64          conv2d_2[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_2 (LeakyReLU)        (None, 82, 82, 16)    0           batch_normalization_2[0][0]      
____________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)   (None, 41, 41, 16)    0           leaky_re_lu_2[0][0]              
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 41, 41, 16)    0           max_pooling2d_1[0][0]            
____________________________________________________________________________________________________
conv2d_3 (Conv2D)                (None, 39, 39, 32)    4640        dropout_1[0][0]                  
____________________________________________________________________________________________________
batch_normalization_3 (BatchNorm (None, 39, 39, 32)    128         conv2d_3[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_3 (LeakyReLU)        (None, 39, 39, 32)    0           batch_normalization_3[0][0]      
____________________________________________________________________________________________________
conv2d_4 (Conv2D)                (None, 38, 38, 32)    4128        leaky_re_lu_3[0][0]              
____________________________________________________________________________________________________
batch_normalization_4 (BatchNorm (None, 38, 38, 32)    128         conv2d_4[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_4 (LeakyReLU)        (None, 38, 38, 32)    0           batch_normalization_4[0][0]      
____________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)   (None, 19, 19, 32)    0           leaky_re_lu_4[0][0]              
____________________________________________________________________________________________________
dropout_2 (Dropout)              (None, 19, 19, 32)    0           max_pooling2d_2[0][0]            
____________________________________________________________________________________________________
conv2d_5 (Conv2D)                (None, 17, 17, 64)    18496       dropout_2[0][0]                  
____________________________________________________________________________________________________
batch_normalization_5 (BatchNorm (None, 17, 17, 64)    256         conv2d_5[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_5 (LeakyReLU)        (None, 17, 17, 64)    0           batch_normalization_5[0][0]      
____________________________________________________________________________________________________
conv2d_6 (Conv2D)                (None, 16, 16, 64)    16448       leaky_re_lu_5[0][0]              
____________________________________________________________________________________________________
batch_normalization_6 (BatchNorm (None, 16, 16, 64)    256         conv2d_6[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_6 (LeakyReLU)        (None, 16, 16, 64)    0           batch_normalization_6[0][0]      
____________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)   (None, 8, 8, 64)      0           leaky_re_lu_6[0][0]              
____________________________________________________________________________________________________
dropout_3 (Dropout)              (None, 8, 8, 64)      0           max_pooling2d_3[0][0]            
____________________________________________________________________________________________________
flatten_1 (Flatten)              (None, 4096)          0           dropout_3[0][0]                  
____________________________________________________________________________________________________
batch_normalization_7 (BatchNorm (None, 4096)          16384       flatten_1[0][0]                  
____________________________________________________________________________________________________
input_2 (InputLayer)             (None, 102)           0                                            
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 4198)          0           batch_normalization_7[0][0]      
                                                                   input_2[0][0]                    
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 128)           537472      concatenate_1[0][0]              
____________________________________________________________________________________________________
leaky_re_lu_7 (LeakyReLU)        (None, 128)           0           dense_1[0][0]                    
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 64)            8256        leaky_re_lu_7[0][0]              
____________________________________________________________________________________________________
dropout_4 (Dropout)              (None, 64)            0           dense_2[0][0]                    
____________________________________________________________________________________________________
dense_3 (Dense)                  (None, 32)            2080        dropout_4[0][0]                  
____________________________________________________________________________________________________
dropout_5 (Dropout)              (None, 32)            0           dense_3[0][0]                    
____________________________________________________________________________________________________
dense_4 (Dense)                  (None, 12)            396         dropout_5[0][0]                  
====================================================================================================
Total params: 610,684
Trainable params: 602,044
Non-trainable params: 8,640
____________________________________________________________________________________________________
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
2018-08-13 23:38:58.218725: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-08-13 23:38:58.219151: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
291s - loss: 2.0095 - acc: 0.3329 - val_loss: 2.4639 - val_acc: 0.2568
Epoch 2/500
290s - loss: 1.6103 - acc: 0.4674 - val_loss: 2.1925 - val_acc: 0.4600
Epoch 3/500
290s - loss: 1.4584 - acc: 0.5289 - val_loss: 2.0718 - val_acc: 0.4389
Epoch 4/500
291s - loss: 1.3586 - acc: 0.5761 - val_loss: 1.3351 - val_acc: 0.5716
Epoch 5/500
291s - loss: 1.2641 - acc: 0.6192 - val_loss: 1.4158 - val_acc: 0.6284
Epoch 6/500
291s - loss: 1.1911 - acc: 0.6553 - val_loss: 2.6211 - val_acc: 0.3653
Epoch 7/500
290s - loss: 1.1602 - acc: 0.6555 - val_loss: 1.3422 - val_acc: 0.6695
Epoch 8/500
290s - loss: 1.1435 - acc: 0.6653 - val_loss: 1.3417 - val_acc: 0.5916
Epoch 9/500
289s - loss: 1.0868 - acc: 0.6842 - val_loss: 1.1539 - val_acc: 0.7063
Epoch 10/500
290s - loss: 1.1329 - acc: 0.6832 - val_loss: 1.2685 - val_acc: 0.7032
Epoch 11/500
289s - loss: 1.0795 - acc: 0.7021 - val_loss: 1.3369 - val_acc: 0.6232
Epoch 12/500
289s - loss: 1.1285 - acc: 0.6892 - val_loss: 2.0213 - val_acc: 0.4979
Epoch 13/500
289s - loss: 1.1450 - acc: 0.6932 - val_loss: 5.5465 - val_acc: 0.3284
Epoch 14/500
288s - loss: 1.1281 - acc: 0.6995 - val_loss: 3.5546 - val_acc: 0.4411
Epoch 15/500
289s - loss: 1.0359 - acc: 0.7174 - val_loss: 1.9150 - val_acc: 0.5737
Epoch 16/500
289s - loss: 1.0940 - acc: 0.7247 - val_loss: 1.9688 - val_acc: 0.5863
Epoch 17/500
288s - loss: 1.1034 - acc: 0.7045 - val_loss: 3.1200 - val_acc: 0.4537
Epoch 18/500

Epoch 00017: reducing learning rate to 0.010000000149.
289s - loss: 1.1285 - acc: 0.7237 - val_loss: 1.4059 - val_acc: 0.7011
Epoch 19/500
288s - loss: 0.9364 - acc: 0.7324 - val_loss: 0.9229 - val_acc: 0.7674
Epoch 20/500
289s - loss: 0.7964 - acc: 0.7645 - val_loss: 1.0176 - val_acc: 0.7526
Epoch 21/500
289s - loss: 0.7476 - acc: 0.7834 - val_loss: 0.7753 - val_acc: 0.7968
Epoch 22/500
289s - loss: 0.6985 - acc: 0.8003 - val_loss: 0.7664 - val_acc: 0.8084
Epoch 23/500
289s - loss: 0.6618 - acc: 0.8045 - val_loss: 0.7811 - val_acc: 0.8042
Epoch 24/500
289s - loss: 0.6378 - acc: 0.8068 - val_loss: 0.7427 - val_acc: 0.8168
Epoch 25/500
289s - loss: 0.6188 - acc: 0.8121 - val_loss: 0.8170 - val_acc: 0.7989
Epoch 26/500
289s - loss: 0.6053 - acc: 0.8147 - val_loss: 0.7261 - val_acc: 0.8221
Epoch 27/500
288s - loss: 0.5730 - acc: 0.8158 - val_loss: 0.7472 - val_acc: 0.8168
Epoch 28/500
288s - loss: 0.5681 - acc: 0.8379 - val_loss: 0.7677 - val_acc: 0.8200
Epoch 29/500
289s - loss: 0.5636 - acc: 0.8282 - val_loss: 0.7587 - val_acc: 0.8274
Epoch 30/500
289s - loss: 0.5549 - acc: 0.8250 - val_loss: 0.6519 - val_acc: 0.8347
Epoch 31/500
288s - loss: 0.5255 - acc: 0.8382 - val_loss: 0.7227 - val_acc: 0.8284
Epoch 32/500
289s - loss: 0.5663 - acc: 0.8361 - val_loss: 0.6228 - val_acc: 0.8484
Epoch 33/500
289s - loss: 0.5283 - acc: 0.8337 - val_loss: 0.6177 - val_acc: 0.8537
Epoch 34/500
289s - loss: 0.4764 - acc: 0.8445 - val_loss: 0.6637 - val_acc: 0.8411
Epoch 35/500
288s - loss: 0.4902 - acc: 0.8437 - val_loss: 0.6470 - val_acc: 0.8474
Epoch 36/500
288s - loss: 0.4801 - acc: 0.8466 - val_loss: 0.6998 - val_acc: 0.8400
Epoch 37/500
288s - loss: 0.4880 - acc: 0.8484 - val_loss: 0.6357 - val_acc: 0.8463
Epoch 38/500
289s - loss: 0.4465 - acc: 0.8503 - val_loss: 0.5910 - val_acc: 0.8537
Epoch 39/500
289s - loss: 0.4225 - acc: 0.8658 - val_loss: 0.6529 - val_acc: 0.8621
Epoch 40/500
289s - loss: 0.4477 - acc: 0.8576 - val_loss: 0.6044 - val_acc: 0.8568
Epoch 41/500
289s - loss: 0.4070 - acc: 0.8655 - val_loss: 0.5933 - val_acc: 0.8621
Epoch 42/500
289s - loss: 0.4207 - acc: 0.8661 - val_loss: 0.6242 - val_acc: 0.8526
Epoch 43/500
289s - loss: 0.4015 - acc: 0.8705 - val_loss: 0.6463 - val_acc: 0.8537
Epoch 44/500
289s - loss: 0.4105 - acc: 0.8689 - val_loss: 0.6221 - val_acc: 0.8558
Epoch 45/500
288s - loss: 0.3914 - acc: 0.8742 - val_loss: 0.6627 - val_acc: 0.8526
Epoch 46/500
289s - loss: 0.3945 - acc: 0.8745 - val_loss: 0.5713 - val_acc: 0.8684
Epoch 47/500
289s - loss: 0.3942 - acc: 0.8724 - val_loss: 0.5759 - val_acc: 0.8663
Epoch 48/500
289s - loss: 0.3389 - acc: 0.8855 - val_loss: 0.6076 - val_acc: 0.8558
Epoch 49/500
288s - loss: 0.3782 - acc: 0.8755 - val_loss: 0.7293 - val_acc: 0.8495
Epoch 50/500
289s - loss: 0.3704 - acc: 0.8834 - val_loss: 0.6706 - val_acc: 0.8579
Epoch 51/500
289s - loss: 0.3358 - acc: 0.8879 - val_loss: 0.6345 - val_acc: 0.8611
Epoch 52/500
289s - loss: 0.3641 - acc: 0.8816 - val_loss: 0.5714 - val_acc: 0.8716
Epoch 53/500
290s - loss: 0.3797 - acc: 0.8818 - val_loss: 0.6905 - val_acc: 0.8453
Epoch 54/500
289s - loss: 0.3411 - acc: 0.8987 - val_loss: 0.6744 - val_acc: 0.8663
Epoch 55/500
289s - loss: 0.3364 - acc: 0.8950 - val_loss: 0.6456 - val_acc: 0.8621
Epoch 56/500
288s - loss: 0.3397 - acc: 0.8905 - val_loss: 0.6628 - val_acc: 0.8642
Epoch 57/500
289s - loss: 0.3603 - acc: 0.8813 - val_loss: 0.6241 - val_acc: 0.8737
Epoch 58/500
290s - loss: 0.3126 - acc: 0.8979 - val_loss: 0.6487 - val_acc: 0.8621
Epoch 59/500
289s - loss: 0.3069 - acc: 0.8995 - val_loss: 0.6112 - val_acc: 0.8695
Epoch 60/500
289s - loss: 0.2994 - acc: 0.8984 - val_loss: 0.7637 - val_acc: 0.8526
Epoch 61/500
288s - loss: 0.2953 - acc: 0.9013 - val_loss: 0.6779 - val_acc: 0.8653
Epoch 62/500
289s - loss: 0.3035 - acc: 0.9034 - val_loss: 0.6278 - val_acc: 0.8684
Epoch 63/500
289s - loss: 0.3175 - acc: 0.8968 - val_loss: 0.6383 - val_acc: 0.8674
Epoch 64/500
288s - loss: 0.2759 - acc: 0.9039 - val_loss: 0.6905 - val_acc: 0.8663
Epoch 65/500
288s - loss: 0.3058 - acc: 0.9024 - val_loss: 0.6908 - val_acc: 0.8716
Epoch 66/500

Epoch 00065: reducing learning rate to 0.000999999977648.
288s - loss: 0.2954 - acc: 0.9039 - val_loss: 0.6755 - val_acc: 0.8589
Epoch 67/500
289s - loss: 0.2668 - acc: 0.9100 - val_loss: 0.6346 - val_acc: 0.8768
Epoch 68/500
290s - loss: 0.2916 - acc: 0.9068 - val_loss: 0.6339 - val_acc: 0.8737
Epoch 69/500
289s - loss: 0.2895 - acc: 0.9124 - val_loss: 0.6389 - val_acc: 0.8779
Epoch 70/500
290s - loss: 0.2573 - acc: 0.9118 - val_loss: 0.6405 - val_acc: 0.8747
Epoch 71/500
289s - loss: 0.2753 - acc: 0.9139 - val_loss: 0.6353 - val_acc: 0.8758
Epoch 72/500
288s - loss: 0.2652 - acc: 0.9166 - val_loss: 0.6482 - val_acc: 0.8758
Epoch 73/500
289s - loss: 0.2856 - acc: 0.9071 - val_loss: 0.6387 - val_acc: 0.8779
Epoch 74/500
289s - loss: 0.2609 - acc: 0.9155 - val_loss: 0.6420 - val_acc: 0.8842
Epoch 75/500
289s - loss: 0.2732 - acc: 0.9124 - val_loss: 0.6446 - val_acc: 0.8768
Epoch 76/500
288s - loss: 0.2786 - acc: 0.9132 - val_loss: 0.6442 - val_acc: 0.8758
Epoch 77/500
288s - loss: 0.2764 - acc: 0.9132 - val_loss: 0.6457 - val_acc: 0.8779
Epoch 78/500
288s - loss: 0.2646 - acc: 0.9084 - val_loss: 0.6475 - val_acc: 0.8758
Epoch 79/500
288s - loss: 0.2491 - acc: 0.9161 - val_loss: 0.6543 - val_acc: 0.8779
Epoch 80/500
288s - loss: 0.2769 - acc: 0.9142 - val_loss: 0.6495 - val_acc: 0.8779
Epoch 81/500
288s - loss: 0.2675 - acc: 0.9121 - val_loss: 0.6506 - val_acc: 0.8768
Epoch 82/500
288s - loss: 0.2892 - acc: 0.9118 - val_loss: 0.6506 - val_acc: 0.8758
Epoch 83/500

Epoch 00082: reducing learning rate to 9.99999931082e-05.
290s - loss: 0.2614 - acc: 0.9134 - val_loss: 0.6561 - val_acc: 0.8747
Epoch 84/500
289s - loss: 0.2662 - acc: 0.9168 - val_loss: 0.6550 - val_acc: 0.8747
Epoch 85/500
289s - loss: 0.2901 - acc: 0.9129 - val_loss: 0.6515 - val_acc: 0.8758
Epoch 86/500
288s - loss: 0.2807 - acc: 0.9113 - val_loss: 0.6528 - val_acc: 0.8747
Epoch 87/500
288s - loss: 0.2615 - acc: 0.9134 - val_loss: 0.6523 - val_acc: 0.8768
Epoch 88/500
288s - loss: 0.2679 - acc: 0.9158 - val_loss: 0.6481 - val_acc: 0.8758
Epoch 89/500
289s - loss: 0.2499 - acc: 0.9147 - val_loss: 0.6479 - val_acc: 0.8768
Epoch 90/500
289s - loss: 0.2884 - acc: 0.9079 - val_loss: 0.6569 - val_acc: 0.8747
Epoch 91/500

Epoch 00090: reducing learning rate to 9.99999901978e-06.
289s - loss: 0.2500 - acc: 0.9197 - val_loss: 0.6502 - val_acc: 0.8768
Epoch 92/500
289s - loss: 0.2449 - acc: 0.9150 - val_loss: 0.6528 - val_acc: 0.8747
Epoch 93/500
289s - loss: 0.2896 - acc: 0.9068 - val_loss: 0.6562 - val_acc: 0.8758
Epoch 94/500
289s - loss: 0.2704 - acc: 0.9134 - val_loss: 0.6585 - val_acc: 0.8768
Epoch 95/500
289s - loss: 0.2574 - acc: 0.9171 - val_loss: 0.6483 - val_acc: 0.8768
Epoch 96/500
290s - loss: 0.2601 - acc: 0.9121 - val_loss: 0.6489 - val_acc: 0.8747
Epoch 97/500
290s - loss: 0.2423 - acc: 0.9142 - val_loss: 0.6523 - val_acc: 0.8758
Epoch 98/500
289s - loss: 0.2696 - acc: 0.9126 - val_loss: 0.6526 - val_acc: 0.8758
Epoch 99/500

Epoch 00098: reducing learning rate to 1e-06.
289s - loss: 0.2495 - acc: 0.9161 - val_loss: 0.6547 - val_acc: 0.8747
Epoch 100/500
290s - loss: 0.2693 - acc: 0.9126 - val_loss: 0.6533 - val_acc: 0.8758
Epoch 101/500
288s - loss: 0.2615 - acc: 0.9182 - val_loss: 0.6543 - val_acc: 0.8768
Epoch 102/500
289s - loss: 0.2688 - acc: 0.9118 - val_loss: 0.6528 - val_acc: 0.8768
Epoch 103/500
288s - loss: 0.2676 - acc: 0.9189 - val_loss: 0.6531 - val_acc: 0.8747
Epoch 104/500
288s - loss: 0.2568 - acc: 0.9137 - val_loss: 0.6508 - val_acc: 0.8758
Epoch 105/500
288s - loss: 0.2572 - acc: 0.9129 - val_loss: 0.6521 - val_acc: 0.8758
Epoch 106/500
288s - loss: 0.2537 - acc: 0.9211 - val_loss: 0.6524 - val_acc: 0.8758
Epoch 107/500
288s - loss: 0.2618 - acc: 0.9153 - val_loss: 0.6499 - val_acc: 0.8758
Training loss for fold 0 is 0.12107949730964672 with percent 95.94736840850429
Testing loss for fold 0 is 0.6419737953888742 with percent 88.42105258138557
 
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_3 (InputLayer)             (None, 85, 85, 3)     0                                            
____________________________________________________________________________________________________
conv2d_7 (Conv2D)                (None, 83, 83, 16)    448         input_3[0][0]                    
____________________________________________________________________________________________________
batch_normalization_8 (BatchNorm (None, 83, 83, 16)    64          conv2d_7[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_8 (LeakyReLU)        (None, 83, 83, 16)    0           batch_normalization_8[0][0]      
____________________________________________________________________________________________________
conv2d_8 (Conv2D)                (None, 82, 82, 16)    1040        leaky_re_lu_8[0][0]              
____________________________________________________________________________________________________
batch_normalization_9 (BatchNorm (None, 82, 82, 16)    64          conv2d_8[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_9 (LeakyReLU)        (None, 82, 82, 16)    0           batch_normalization_9[0][0]      
____________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)   (None, 41, 41, 16)    0           leaky_re_lu_9[0][0]              
____________________________________________________________________________________________________
dropout_6 (Dropout)              (None, 41, 41, 16)    0           max_pooling2d_4[0][0]            
____________________________________________________________________________________________________
conv2d_9 (Conv2D)                (None, 39, 39, 32)    4640        dropout_6[0][0]                  
____________________________________________________________________________________________________
batch_normalization_10 (BatchNor (None, 39, 39, 32)    128         conv2d_9[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_10 (LeakyReLU)       (None, 39, 39, 32)    0           batch_normalization_10[0][0]     
____________________________________________________________________________________________________
conv2d_10 (Conv2D)               (None, 38, 38, 32)    4128        leaky_re_lu_10[0][0]             
____________________________________________________________________________________________________
batch_normalization_11 (BatchNor (None, 38, 38, 32)    128         conv2d_10[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_11 (LeakyReLU)       (None, 38, 38, 32)    0           batch_normalization_11[0][0]     
____________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)   (None, 19, 19, 32)    0           leaky_re_lu_11[0][0]             
____________________________________________________________________________________________________
dropout_7 (Dropout)              (None, 19, 19, 32)    0           max_pooling2d_5[0][0]            
____________________________________________________________________________________________________
conv2d_11 (Conv2D)               (None, 17, 17, 64)    18496       dropout_7[0][0]                  
____________________________________________________________________________________________________
batch_normalization_12 (BatchNor (None, 17, 17, 64)    256         conv2d_11[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_12 (LeakyReLU)       (None, 17, 17, 64)    0           batch_normalization_12[0][0]     
____________________________________________________________________________________________________
conv2d_12 (Conv2D)               (None, 16, 16, 64)    16448       leaky_re_lu_12[0][0]             
____________________________________________________________________________________________________
batch_normalization_13 (BatchNor (None, 16, 16, 64)    256         conv2d_12[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_13 (LeakyReLU)       (None, 16, 16, 64)    0           batch_normalization_13[0][0]     
____________________________________________________________________________________________________
max_pooling2d_6 (MaxPooling2D)   (None, 8, 8, 64)      0           leaky_re_lu_13[0][0]             
____________________________________________________________________________________________________
dropout_8 (Dropout)              (None, 8, 8, 64)      0           max_pooling2d_6[0][0]            
____________________________________________________________________________________________________
flatten_2 (Flatten)              (None, 4096)          0           dropout_8[0][0]                  
____________________________________________________________________________________________________
batch_normalization_14 (BatchNor (None, 4096)          16384       flatten_2[0][0]                  
____________________________________________________________________________________________________
input_4 (InputLayer)             (None, 102)           0                                            
____________________________________________________________________________________________________
concatenate_2 (Concatenate)      (None, 4198)          0           batch_normalization_14[0][0]     
                                                                   input_4[0][0]                    
____________________________________________________________________________________________________
dense_5 (Dense)                  (None, 128)           537472      concatenate_2[0][0]              
____________________________________________________________________________________________________
leaky_re_lu_14 (LeakyReLU)       (None, 128)           0           dense_5[0][0]                    
____________________________________________________________________________________________________
dense_6 (Dense)                  (None, 64)            8256        leaky_re_lu_14[0][0]             
____________________________________________________________________________________________________
dropout_9 (Dropout)              (None, 64)            0           dense_6[0][0]                    
____________________________________________________________________________________________________
dense_7 (Dense)                  (None, 32)            2080        dropout_9[0][0]                  
____________________________________________________________________________________________________
dropout_10 (Dropout)             (None, 32)            0           dense_7[0][0]                    
____________________________________________________________________________________________________
dense_8 (Dense)                  (None, 12)            396         dropout_10[0][0]                 
====================================================================================================
Total params: 610,684
Trainable params: 602,044
Non-trainable params: 8,640
____________________________________________________________________________________________________
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
292s - loss: 2.0036 - acc: 0.3366 - val_loss: 3.5000 - val_acc: 0.1263
Epoch 2/500
289s - loss: 1.5592 - acc: 0.4861 - val_loss: 2.0260 - val_acc: 0.4137
Epoch 3/500
287s - loss: 1.4465 - acc: 0.5376 - val_loss: 1.9195 - val_acc: 0.5295
Epoch 4/500
287s - loss: 1.3725 - acc: 0.5679 - val_loss: 1.1646 - val_acc: 0.6484
Epoch 5/500
287s - loss: 1.2907 - acc: 0.6211 - val_loss: 3.6522 - val_acc: 0.2800
Epoch 6/500
287s - loss: 1.2482 - acc: 0.6305 - val_loss: 2.7932 - val_acc: 0.3232
Epoch 7/500
287s - loss: 1.2277 - acc: 0.6384 - val_loss: 1.2922 - val_acc: 0.6168
Epoch 8/500
287s - loss: 1.1866 - acc: 0.6545 - val_loss: 1.4265 - val_acc: 0.6147
Epoch 9/500
287s - loss: 1.1853 - acc: 0.6729 - val_loss: 6.3384 - val_acc: 0.1632
Epoch 10/500
287s - loss: 1.1355 - acc: 0.6753 - val_loss: 1.2737 - val_acc: 0.6453
Epoch 11/500
288s - loss: 1.1264 - acc: 0.6905 - val_loss: 1.2598 - val_acc: 0.7011
Epoch 12/500
287s - loss: 1.0180 - acc: 0.7153 - val_loss: 1.4490 - val_acc: 0.6642
Epoch 13/500
287s - loss: 0.9939 - acc: 0.7221 - val_loss: 3.0860 - val_acc: 0.4568
Epoch 14/500
287s - loss: 1.0595 - acc: 0.7361 - val_loss: 1.2773 - val_acc: 0.6737
Epoch 15/500
287s - loss: 1.0848 - acc: 0.7071 - val_loss: 1.0299 - val_acc: 0.7611
Epoch 16/500
287s - loss: 1.0408 - acc: 0.7279 - val_loss: 1.3771 - val_acc: 0.6695
Epoch 17/500
287s - loss: 1.0667 - acc: 0.7329 - val_loss: 2.5304 - val_acc: 0.5589
Epoch 18/500
287s - loss: 0.9245 - acc: 0.7616 - val_loss: 2.0231 - val_acc: 0.6368
Epoch 19/500
287s - loss: 1.0595 - acc: 0.7405 - val_loss: 2.2609 - val_acc: 0.5179
Epoch 20/500
287s - loss: 1.0327 - acc: 0.7468 - val_loss: 1.9366 - val_acc: 0.6032
Epoch 21/500
287s - loss: 1.0516 - acc: 0.7258 - val_loss: 1.5054 - val_acc: 0.6832
Epoch 22/500
287s - loss: 1.0316 - acc: 0.7411 - val_loss: 2.3297 - val_acc: 0.6032
Epoch 23/500
287s - loss: 1.1942 - acc: 0.7163 - val_loss: 6.2141 - val_acc: 0.2663
Epoch 24/500

Epoch 00023: reducing learning rate to 0.010000000149.
288s - loss: 1.1880 - acc: 0.7124 - val_loss: 1.5144 - val_acc: 0.6821
Epoch 25/500
287s - loss: 1.0027 - acc: 0.7529 - val_loss: 1.0351 - val_acc: 0.7653
Epoch 26/500
287s - loss: 0.8059 - acc: 0.7918 - val_loss: 0.9321 - val_acc: 0.7958
Epoch 27/500
287s - loss: 0.7477 - acc: 0.8097 - val_loss: 0.8234 - val_acc: 0.8116
Epoch 28/500
287s - loss: 0.7024 - acc: 0.8137 - val_loss: 0.8449 - val_acc: 0.8074
Epoch 29/500
287s - loss: 0.6316 - acc: 0.8287 - val_loss: 0.9137 - val_acc: 0.7989
Epoch 30/500
288s - loss: 0.5951 - acc: 0.8384 - val_loss: 0.8366 - val_acc: 0.8242
Epoch 31/500
287s - loss: 0.6071 - acc: 0.8374 - val_loss: 1.0154 - val_acc: 0.7832
Epoch 32/500
287s - loss: 0.5895 - acc: 0.8434 - val_loss: 0.8198 - val_acc: 0.8253
Epoch 33/500
287s - loss: 0.5627 - acc: 0.8505 - val_loss: 0.8651 - val_acc: 0.8116
Epoch 34/500
287s - loss: 0.5789 - acc: 0.8505 - val_loss: 0.8439 - val_acc: 0.8284
Epoch 35/500
287s - loss: 0.5508 - acc: 0.8547 - val_loss: 0.7737 - val_acc: 0.8211
Epoch 36/500
288s - loss: 0.5304 - acc: 0.8558 - val_loss: 0.8086 - val_acc: 0.8305
Epoch 37/500
287s - loss: 0.5422 - acc: 0.8566 - val_loss: 0.9069 - val_acc: 0.8116
Epoch 38/500
287s - loss: 0.5140 - acc: 0.8647 - val_loss: 0.8706 - val_acc: 0.8221
Epoch 39/500
287s - loss: 0.5072 - acc: 0.8676 - val_loss: 0.8867 - val_acc: 0.8200
Epoch 40/500
287s - loss: 0.5064 - acc: 0.8687 - val_loss: 1.0710 - val_acc: 0.8042
Epoch 41/500
287s - loss: 0.4795 - acc: 0.8700 - val_loss: 0.8395 - val_acc: 0.8284
Epoch 42/500
287s - loss: 0.4742 - acc: 0.8700 - val_loss: 0.8257 - val_acc: 0.8347
Epoch 43/500
287s - loss: 0.4682 - acc: 0.8755 - val_loss: 0.8402 - val_acc: 0.8305
Epoch 44/500
287s - loss: 0.4950 - acc: 0.8674 - val_loss: 0.8384 - val_acc: 0.8326
Epoch 45/500
287s - loss: 0.4484 - acc: 0.8776 - val_loss: 0.8357 - val_acc: 0.8400
Epoch 46/500
287s - loss: 0.4615 - acc: 0.8813 - val_loss: 0.8459 - val_acc: 0.8389
Epoch 47/500
287s - loss: 0.4247 - acc: 0.8837 - val_loss: 0.8899 - val_acc: 0.8263
Epoch 48/500
287s - loss: 0.4307 - acc: 0.8826 - val_loss: 0.9393 - val_acc: 0.8116
Epoch 49/500
287s - loss: 0.4515 - acc: 0.8755 - val_loss: 0.8803 - val_acc: 0.8295
Epoch 50/500
287s - loss: 0.4299 - acc: 0.8837 - val_loss: 0.9313 - val_acc: 0.8326
Epoch 51/500
287s - loss: 0.4134 - acc: 0.8874 - val_loss: 0.8324 - val_acc: 0.8316
Epoch 52/500
287s - loss: 0.4276 - acc: 0.8803 - val_loss: 0.8319 - val_acc: 0.8358
Epoch 53/500
287s - loss: 0.4297 - acc: 0.8842 - val_loss: 1.0265 - val_acc: 0.7947
Epoch 54/500

Epoch 00053: reducing learning rate to 0.000999999977648.
287s - loss: 0.3989 - acc: 0.8939 - val_loss: 1.0510 - val_acc: 0.8253
Epoch 55/500
287s - loss: 0.3953 - acc: 0.8903 - val_loss: 0.8930 - val_acc: 0.8368
Epoch 56/500
287s - loss: 0.3905 - acc: 0.8950 - val_loss: 0.8569 - val_acc: 0.8421
Epoch 57/500
287s - loss: 0.3814 - acc: 0.8955 - val_loss: 0.8280 - val_acc: 0.8432
Epoch 58/500
288s - loss: 0.4153 - acc: 0.8924 - val_loss: 0.8308 - val_acc: 0.8453
Epoch 59/500
287s - loss: 0.4016 - acc: 0.8892 - val_loss: 0.8169 - val_acc: 0.8400
Epoch 60/500
287s - loss: 0.3910 - acc: 0.8913 - val_loss: 0.8139 - val_acc: 0.8379
Epoch 61/500
287s - loss: 0.3949 - acc: 0.8924 - val_loss: 0.8110 - val_acc: 0.8432
Epoch 62/500
287s - loss: 0.3777 - acc: 0.8971 - val_loss: 0.8167 - val_acc: 0.8453
Epoch 63/500
287s - loss: 0.3751 - acc: 0.8950 - val_loss: 0.8136 - val_acc: 0.8432
Epoch 64/500
287s - loss: 0.3519 - acc: 0.9024 - val_loss: 0.8032 - val_acc: 0.8442
Epoch 65/500
288s - loss: 0.4017 - acc: 0.8889 - val_loss: 0.7927 - val_acc: 0.8463
Epoch 66/500
287s - loss: 0.3660 - acc: 0.8974 - val_loss: 0.8016 - val_acc: 0.8495
Epoch 67/500
287s - loss: 0.3778 - acc: 0.8947 - val_loss: 0.7872 - val_acc: 0.8474
Epoch 68/500
287s - loss: 0.3576 - acc: 0.8926 - val_loss: 0.7956 - val_acc: 0.8484
Epoch 69/500
287s - loss: 0.3729 - acc: 0.8945 - val_loss: 0.8021 - val_acc: 0.8474
Epoch 70/500
287s - loss: 0.3795 - acc: 0.8968 - val_loss: 0.7856 - val_acc: 0.8505
Epoch 71/500
287s - loss: 0.3785 - acc: 0.8884 - val_loss: 0.8217 - val_acc: 0.8505
Epoch 72/500
287s - loss: 0.3964 - acc: 0.8908 - val_loss: 0.8014 - val_acc: 0.8453
Epoch 73/500
287s - loss: 0.3780 - acc: 0.8966 - val_loss: 0.8261 - val_acc: 0.8442
Epoch 74/500
287s - loss: 0.3658 - acc: 0.8971 - val_loss: 0.8017 - val_acc: 0.8526
Epoch 75/500
287s - loss: 0.3626 - acc: 0.8982 - val_loss: 0.8034 - val_acc: 0.8432
Epoch 76/500
287s - loss: 0.3538 - acc: 0.8997 - val_loss: 0.7971 - val_acc: 0.8463
Epoch 77/500
287s - loss: 0.3599 - acc: 0.8950 - val_loss: 0.8158 - val_acc: 0.8442
Epoch 78/500
287s - loss: 0.3639 - acc: 0.8984 - val_loss: 0.7942 - val_acc: 0.8526
Epoch 79/500
287s - loss: 0.3580 - acc: 0.9000 - val_loss: 0.8081 - val_acc: 0.8463
Epoch 80/500
287s - loss: 0.3498 - acc: 0.8997 - val_loss: 0.7847 - val_acc: 0.8526
Epoch 81/500
287s - loss: 0.3751 - acc: 0.8945 - val_loss: 0.7888 - val_acc: 0.8526
Epoch 82/500
287s - loss: 0.3635 - acc: 0.9016 - val_loss: 0.7869 - val_acc: 0.8495
Epoch 83/500
287s - loss: 0.3643 - acc: 0.8971 - val_loss: 0.7789 - val_acc: 0.8537
Epoch 84/500
287s - loss: 0.3483 - acc: 0.8997 - val_loss: 0.7705 - val_acc: 0.8526
Epoch 85/500
287s - loss: 0.3588 - acc: 0.9024 - val_loss: 0.7723 - val_acc: 0.8537
Epoch 86/500
287s - loss: 0.3451 - acc: 0.9032 - val_loss: 0.7979 - val_acc: 0.8505
Epoch 87/500
287s - loss: 0.3399 - acc: 0.9024 - val_loss: 0.7948 - val_acc: 0.8505
Epoch 88/500
287s - loss: 0.3690 - acc: 0.8989 - val_loss: 0.7821 - val_acc: 0.8495
Epoch 89/500
287s - loss: 0.3706 - acc: 0.8942 - val_loss: 0.8097 - val_acc: 0.8474
Epoch 90/500
287s - loss: 0.3564 - acc: 0.9003 - val_loss: 0.8065 - val_acc: 0.8484
Epoch 91/500
287s - loss: 0.3446 - acc: 0.9021 - val_loss: 0.7867 - val_acc: 0.8568
Epoch 92/500
287s - loss: 0.3710 - acc: 0.8997 - val_loss: 0.7871 - val_acc: 0.8505
Epoch 93/500
287s - loss: 0.3440 - acc: 0.9013 - val_loss: 0.7708 - val_acc: 0.8537
Epoch 94/500
287s - loss: 0.3316 - acc: 0.9029 - val_loss: 0.7801 - val_acc: 0.8547
Epoch 95/500
287s - loss: 0.3418 - acc: 0.9008 - val_loss: 0.7812 - val_acc: 0.8537
Epoch 96/500
287s - loss: 0.3652 - acc: 0.9016 - val_loss: 0.7697 - val_acc: 0.8558
Epoch 97/500
287s - loss: 0.3761 - acc: 0.8979 - val_loss: 0.7854 - val_acc: 0.8547
Epoch 98/500
287s - loss: 0.3738 - acc: 0.9047 - val_loss: 0.8017 - val_acc: 0.8463
Epoch 99/500
288s - loss: 0.3490 - acc: 0.9000 - val_loss: 0.7703 - val_acc: 0.8547
Epoch 100/500

Epoch 00099: reducing learning rate to 9.99999931082e-05.
287s - loss: 0.3706 - acc: 0.8997 - val_loss: 0.7732 - val_acc: 0.8568
Epoch 101/500
287s - loss: 0.3725 - acc: 0.8953 - val_loss: 0.7687 - val_acc: 0.8547
Epoch 102/500
287s - loss: 0.3535 - acc: 0.8997 - val_loss: 0.7707 - val_acc: 0.8579
Epoch 103/500
287s - loss: 0.3497 - acc: 0.9005 - val_loss: 0.7728 - val_acc: 0.8537
Epoch 104/500
287s - loss: 0.3385 - acc: 0.9013 - val_loss: 0.7741 - val_acc: 0.8579
Epoch 105/500
288s - loss: 0.3325 - acc: 0.9029 - val_loss: 0.7665 - val_acc: 0.8579
Epoch 106/500
287s - loss: 0.3492 - acc: 0.9024 - val_loss: 0.7633 - val_acc: 0.8558
Epoch 107/500
287s - loss: 0.3288 - acc: 0.9066 - val_loss: 0.7630 - val_acc: 0.8558
Epoch 108/500
287s - loss: 0.3511 - acc: 0.9013 - val_loss: 0.7691 - val_acc: 0.8568
Epoch 109/500
287s - loss: 0.3790 - acc: 0.8942 - val_loss: 0.7640 - val_acc: 0.8579
Epoch 110/500
287s - loss: 0.3328 - acc: 0.9066 - val_loss: 0.7646 - val_acc: 0.8537
Epoch 111/500
287s - loss: 0.3315 - acc: 0.9042 - val_loss: 0.7606 - val_acc: 0.8600
Epoch 112/500
287s - loss: 0.3530 - acc: 0.9026 - val_loss: 0.7675 - val_acc: 0.8589
Epoch 113/500
287s - loss: 0.3467 - acc: 0.9042 - val_loss: 0.7671 - val_acc: 0.8589
Epoch 114/500
287s - loss: 0.3608 - acc: 0.8992 - val_loss: 0.7669 - val_acc: 0.8589
Epoch 115/500
287s - loss: 0.3549 - acc: 0.9037 - val_loss: 0.7656 - val_acc: 0.8558
Epoch 116/500
287s - loss: 0.3304 - acc: 0.9045 - val_loss: 0.7621 - val_acc: 0.8558
Epoch 117/500
288s - loss: 0.3311 - acc: 0.9050 - val_loss: 0.7647 - val_acc: 0.8579
Epoch 118/500
287s - loss: 0.3579 - acc: 0.9013 - val_loss: 0.7620 - val_acc: 0.8600
Epoch 119/500
287s - loss: 0.3424 - acc: 0.9016 - val_loss: 0.7691 - val_acc: 0.8558
Epoch 120/500

Epoch 00119: reducing learning rate to 9.99999901978e-06.
287s - loss: 0.3432 - acc: 0.8984 - val_loss: 0.7647 - val_acc: 0.8579
Epoch 121/500
287s - loss: 0.3640 - acc: 0.9024 - val_loss: 0.7615 - val_acc: 0.8589
Epoch 122/500
287s - loss: 0.3578 - acc: 0.9032 - val_loss: 0.7621 - val_acc: 0.8589
Epoch 123/500
287s - loss: 0.3545 - acc: 0.9024 - val_loss: 0.7631 - val_acc: 0.8600
Epoch 124/500
287s - loss: 0.3351 - acc: 0.9037 - val_loss: 0.7675 - val_acc: 0.8589
Epoch 125/500
287s - loss: 0.3516 - acc: 0.8984 - val_loss: 0.7708 - val_acc: 0.8568
Epoch 126/500
287s - loss: 0.3549 - acc: 0.8992 - val_loss: 0.7650 - val_acc: 0.8589
Epoch 127/500
287s - loss: 0.3396 - acc: 0.9061 - val_loss: 0.7641 - val_acc: 0.8568
Epoch 128/500
287s - loss: 0.3778 - acc: 0.8947 - val_loss: 0.7652 - val_acc: 0.8611
Epoch 129/500
287s - loss: 0.3407 - acc: 0.9042 - val_loss: 0.7625 - val_acc: 0.8579
Epoch 130/500
287s - loss: 0.3469 - acc: 0.9050 - val_loss: 0.7684 - val_acc: 0.8558
Epoch 131/500
287s - loss: 0.3465 - acc: 0.9021 - val_loss: 0.7676 - val_acc: 0.8568
Epoch 132/500
287s - loss: 0.3574 - acc: 0.9008 - val_loss: 0.7621 - val_acc: 0.8568
Epoch 133/500
287s - loss: 0.3297 - acc: 0.9066 - val_loss: 0.7636 - val_acc: 0.8589
Epoch 134/500
287s - loss: 0.3294 - acc: 0.9050 - val_loss: 0.7675 - val_acc: 0.8537
Epoch 135/500
287s - loss: 0.3601 - acc: 0.9042 - val_loss: 0.7636 - val_acc: 0.8579
Epoch 136/500
287s - loss: 0.3369 - acc: 0.9037 - val_loss: 0.7693 - val_acc: 0.8568
Epoch 137/500

Epoch 00136: reducing learning rate to 1e-06.
287s - loss: 0.3403 - acc: 0.8992 - val_loss: 0.7641 - val_acc: 0.8600
Epoch 138/500
287s - loss: 0.3305 - acc: 0.9013 - val_loss: 0.7655 - val_acc: 0.8589
Epoch 139/500
287s - loss: 0.3276 - acc: 0.9032 - val_loss: 0.7690 - val_acc: 0.8558
Epoch 140/500
287s - loss: 0.3706 - acc: 0.8987 - val_loss: 0.7640 - val_acc: 0.8579
Epoch 141/500
287s - loss: 0.3396 - acc: 0.9053 - val_loss: 0.7695 - val_acc: 0.8558
Epoch 142/500
287s - loss: 0.3583 - acc: 0.8989 - val_loss: 0.7698 - val_acc: 0.8579
Epoch 143/500
287s - loss: 0.3578 - acc: 0.9008 - val_loss: 0.7719 - val_acc: 0.8558
Epoch 144/500
287s - loss: 0.3294 - acc: 0.9039 - val_loss: 0.7692 - val_acc: 0.8558
Epoch 145/500
287s - loss: 0.3560 - acc: 0.8963 - val_loss: 0.7697 - val_acc: 0.8589
Epoch 146/500
288s - loss: 0.3449 - acc: 0.9011 - val_loss: 0.7660 - val_acc: 0.8579
Epoch 147/500
287s - loss: 0.3676 - acc: 0.8921 - val_loss: 0.7615 - val_acc: 0.8579
Epoch 148/500
287s - loss: 0.3407 - acc: 0.9032 - val_loss: 0.7660 - val_acc: 0.8579
Epoch 149/500
287s - loss: 0.3379 - acc: 0.9050 - val_loss: 0.7709 - val_acc: 0.8568
Epoch 150/500
287s - loss: 0.3458 - acc: 0.9045 - val_loss: 0.7644 - val_acc: 0.8558
Epoch 151/500
287s - loss: 0.3407 - acc: 0.9018 - val_loss: 0.7639 - val_acc: 0.8537
Epoch 152/500
287s - loss: 0.3466 - acc: 0.8984 - val_loss: 0.7625 - val_acc: 0.8579
Epoch 153/500
287s - loss: 0.3593 - acc: 0.9058 - val_loss: 0.7621 - val_acc: 0.8589
Epoch 154/500
287s - loss: 0.3360 - acc: 0.9026 - val_loss: 0.7705 - val_acc: 0.8579
Epoch 155/500
287s - loss: 0.3333 - acc: 0.9045 - val_loss: 0.7713 - val_acc: 0.8568
Epoch 156/500
287s - loss: 0.3563 - acc: 0.9011 - val_loss: 0.7673 - val_acc: 0.8547
Epoch 157/500
287s - loss: 0.3601 - acc: 0.8955 - val_loss: 0.7680 - val_acc: 0.8568
Epoch 158/500
287s - loss: 0.3471 - acc: 0.8997 - val_loss: 0.7679 - val_acc: 0.8568
Epoch 159/500
287s - loss: 0.3595 - acc: 0.8979 - val_loss: 0.7656 - val_acc: 0.8558
Epoch 160/500
287s - loss: 0.3502 - acc: 0.9029 - val_loss: 0.7653 - val_acc: 0.8589
Epoch 161/500
287s - loss: 0.3671 - acc: 0.8947 - val_loss: 0.7676 - val_acc: 0.8558
Epoch 162/500
287s - loss: 0.3565 - acc: 0.9024 - val_loss: 0.7683 - val_acc: 0.8558
Epoch 163/500
287s - loss: 0.3534 - acc: 0.9037 - val_loss: 0.7719 - val_acc: 0.8505
Epoch 164/500
287s - loss: 0.3246 - acc: 0.9047 - val_loss: 0.7686 - val_acc: 0.8558
Epoch 165/500
287s - loss: 0.3323 - acc: 0.9037 - val_loss: 0.7653 - val_acc: 0.8589
Epoch 166/500
287s - loss: 0.3295 - acc: 0.9079 - val_loss: 0.7679 - val_acc: 0.8589
Epoch 167/500
287s - loss: 0.3532 - acc: 0.8963 - val_loss: 0.7602 - val_acc: 0.8589
Epoch 168/500
288s - loss: 0.3548 - acc: 0.9037 - val_loss: 0.7627 - val_acc: 0.8547
Epoch 169/500
287s - loss: 0.3339 - acc: 0.9039 - val_loss: 0.7648 - val_acc: 0.8579
Epoch 170/500
287s - loss: 0.3471 - acc: 0.9050 - val_loss: 0.7679 - val_acc: 0.8547
Epoch 171/500
287s - loss: 0.3345 - acc: 0.9074 - val_loss: 0.7662 - val_acc: 0.8579
Epoch 172/500
287s - loss: 0.3222 - acc: 0.9111 - val_loss: 0.7704 - val_acc: 0.8568
Epoch 173/500
287s - loss: 0.3611 - acc: 0.9029 - val_loss: 0.7701 - val_acc: 0.8558
Epoch 174/500
287s - loss: 0.3425 - acc: 0.8992 - val_loss: 0.7679 - val_acc: 0.8568
Epoch 175/500
287s - loss: 0.3450 - acc: 0.9047 - val_loss: 0.7685 - val_acc: 0.8547
Epoch 176/500
287s - loss: 0.3710 - acc: 0.8963 - val_loss: 0.7672 - val_acc: 0.8568
Epoch 177/500
287s - loss: 0.3403 - acc: 0.9018 - val_loss: 0.7657 - val_acc: 0.8589
Epoch 178/500
287s - loss: 0.3544 - acc: 0.8984 - val_loss: 0.7690 - val_acc: 0.8526
Epoch 179/500
287s - loss: 0.3433 - acc: 0.9003 - val_loss: 0.7646 - val_acc: 0.8579
Epoch 180/500
287s - loss: 0.3392 - acc: 0.9013 - val_loss: 0.7742 - val_acc: 0.8558
Epoch 181/500
288s - loss: 0.3567 - acc: 0.9005 - val_loss: 0.7620 - val_acc: 0.8558
Epoch 182/500
287s - loss: 0.3332 - acc: 0.9047 - val_loss: 0.7657 - val_acc: 0.8579
Epoch 183/500
287s - loss: 0.3619 - acc: 0.8989 - val_loss: 0.7742 - val_acc: 0.8568
Epoch 184/500
287s - loss: 0.3500 - acc: 0.9013 - val_loss: 0.7700 - val_acc: 0.8568
Epoch 185/500
288s - loss: 0.3630 - acc: 0.8974 - val_loss: 0.7747 - val_acc: 0.8589
Epoch 186/500
287s - loss: 0.3507 - acc: 0.9000 - val_loss: 0.7690 - val_acc: 0.8579
Epoch 187/500
287s - loss: 0.3483 - acc: 0.9026 - val_loss: 0.7648 - val_acc: 0.8589
Epoch 188/500
287s - loss: 0.3416 - acc: 0.9068 - val_loss: 0.7629 - val_acc: 0.8589
Epoch 189/500
287s - loss: 0.3545 - acc: 0.9066 - val_loss: 0.7680 - val_acc: 0.8547
Epoch 190/500
287s - loss: 0.3401 - acc: 0.9037 - val_loss: 0.7688 - val_acc: 0.8568
Epoch 191/500
287s - loss: 0.3449 - acc: 0.8997 - val_loss: 0.7651 - val_acc: 0.8537
Epoch 192/500
287s - loss: 0.3467 - acc: 0.8979 - val_loss: 0.7687 - val_acc: 0.8558
Epoch 193/500
287s - loss: 0.3830 - acc: 0.8947 - val_loss: 0.7677 - val_acc: 0.8558
Epoch 194/500
287s - loss: 0.3403 - acc: 0.9047 - val_loss: 0.7697 - val_acc: 0.8579
Epoch 195/500
288s - loss: 0.3781 - acc: 0.8971 - val_loss: 0.7621 - val_acc: 0.8579
Epoch 196/500
287s - loss: 0.3362 - acc: 0.9021 - val_loss: 0.7699 - val_acc: 0.8537
Epoch 197/500
287s - loss: 0.3533 - acc: 0.9005 - val_loss: 0.7742 - val_acc: 0.8516
Epoch 198/500
287s - loss: 0.3335 - acc: 0.9045 - val_loss: 0.7721 - val_acc: 0.8526
Epoch 199/500
287s - loss: 0.3634 - acc: 0.8982 - val_loss: 0.7692 - val_acc: 0.8568
Epoch 200/500
287s - loss: 0.3787 - acc: 0.8984 - val_loss: 0.7691 - val_acc: 0.8558
Epoch 201/500
287s - loss: 0.3418 - acc: 0.9032 - val_loss: 0.7669 - val_acc: 0.8579
Epoch 202/500
287s - loss: 0.3645 - acc: 0.9011 - val_loss: 0.7636 - val_acc: 0.8600
Epoch 203/500
287s - loss: 0.3668 - acc: 0.8982 - val_loss: 0.7739 - val_acc: 0.8568
Epoch 204/500
287s - loss: 0.3578 - acc: 0.9008 - val_loss: 0.7703 - val_acc: 0.8568
Epoch 205/500
287s - loss: 0.3550 - acc: 0.8984 - val_loss: 0.7652 - val_acc: 0.8568
Epoch 206/500
287s - loss: 0.3653 - acc: 0.8987 - val_loss: 0.7646 - val_acc: 0.8568
Epoch 207/500
287s - loss: 0.3537 - acc: 0.9000 - val_loss: 0.7630 - val_acc: 0.8579
Epoch 208/500
287s - loss: 0.3699 - acc: 0.9003 - val_loss: 0.7645 - val_acc: 0.8558
Epoch 209/500
287s - loss: 0.3699 - acc: 0.8976 - val_loss: 0.7640 - val_acc: 0.8547
Epoch 210/500
287s - loss: 0.3520 - acc: 0.9029 - val_loss: 0.7683 - val_acc: 0.8568
Epoch 211/500
287s - loss: 0.3674 - acc: 0.8950 - val_loss: 0.7683 - val_acc: 0.8547
Epoch 212/500
287s - loss: 0.3496 - acc: 0.9032 - val_loss: 0.7622 - val_acc: 0.8568
Epoch 213/500
287s - loss: 0.3317 - acc: 0.9042 - val_loss: 0.7657 - val_acc: 0.8589
Epoch 214/500
287s - loss: 0.3529 - acc: 0.8974 - val_loss: 0.7682 - val_acc: 0.8579
Epoch 215/500
287s - loss: 0.3478 - acc: 0.9011 - val_loss: 0.7713 - val_acc: 0.8558
Epoch 216/500
287s - loss: 0.3548 - acc: 0.9013 - val_loss: 0.7705 - val_acc: 0.8579
Epoch 217/500
287s - loss: 0.3224 - acc: 0.9039 - val_loss: 0.7707 - val_acc: 0.8589
Epoch 218/500
288s - loss: 0.3523 - acc: 0.8997 - val_loss: 0.7699 - val_acc: 0.8579
Epoch 219/500
287s - loss: 0.3526 - acc: 0.9018 - val_loss: 0.7672 - val_acc: 0.8611
Epoch 220/500
287s - loss: 0.3487 - acc: 0.9047 - val_loss: 0.7656 - val_acc: 0.8600
Epoch 221/500
287s - loss: 0.3431 - acc: 0.9047 - val_loss: 0.7659 - val_acc: 0.8537
Epoch 222/500
287s - loss: 0.3449 - acc: 0.9053 - val_loss: 0.7634 - val_acc: 0.8558
Epoch 223/500
287s - loss: 0.3428 - acc: 0.9026 - val_loss: 0.7628 - val_acc: 0.8547
Epoch 224/500
287s - loss: 0.3760 - acc: 0.9013 - val_loss: 0.7657 - val_acc: 0.8589
Epoch 225/500
287s - loss: 0.3709 - acc: 0.9011 - val_loss: 0.7651 - val_acc: 0.8600
Epoch 226/500
287s - loss: 0.3582 - acc: 0.9016 - val_loss: 0.7667 - val_acc: 0.8579
Epoch 227/500
287s - loss: 0.3451 - acc: 0.9047 - val_loss: 0.7573 - val_acc: 0.8579
Epoch 228/500
287s - loss: 0.3304 - acc: 0.9029 - val_loss: 0.7632 - val_acc: 0.8589
Epoch 229/500
287s - loss: 0.3668 - acc: 0.8955 - val_loss: 0.7719 - val_acc: 0.8579
Epoch 230/500
287s - loss: 0.3594 - acc: 0.9034 - val_loss: 0.7701 - val_acc: 0.8579
Epoch 231/500
287s - loss: 0.3578 - acc: 0.9013 - val_loss: 0.7699 - val_acc: 0.8579
Epoch 232/500
287s - loss: 0.3560 - acc: 0.9000 - val_loss: 0.7651 - val_acc: 0.8579
Epoch 233/500
287s - loss: 0.3677 - acc: 0.8979 - val_loss: 0.7669 - val_acc: 0.8568
Epoch 234/500
287s - loss: 0.3740 - acc: 0.8979 - val_loss: 0.7700 - val_acc: 0.8547
Epoch 235/500
287s - loss: 0.3554 - acc: 0.9005 - val_loss: 0.7629 - val_acc: 0.8558
Epoch 236/500
287s - loss: 0.3488 - acc: 0.9029 - val_loss: 0.7675 - val_acc: 0.8568
Epoch 237/500
288s - loss: 0.3579 - acc: 0.8987 - val_loss: 0.7676 - val_acc: 0.8579
Epoch 238/500
287s - loss: 0.3571 - acc: 0.9011 - val_loss: 0.7688 - val_acc: 0.8579
Epoch 239/500
287s - loss: 0.3676 - acc: 0.8979 - val_loss: 0.7675 - val_acc: 0.8558
Epoch 240/500
288s - loss: 0.3511 - acc: 0.9008 - val_loss: 0.7680 - val_acc: 0.8600
Epoch 241/500
287s - loss: 0.3400 - acc: 0.9055 - val_loss: 0.7720 - val_acc: 0.8579
Epoch 242/500
287s - loss: 0.3555 - acc: 0.8950 - val_loss: 0.7705 - val_acc: 0.8547
Epoch 243/500
287s - loss: 0.3494 - acc: 0.9016 - val_loss: 0.7688 - val_acc: 0.8558
Epoch 244/500
287s - loss: 0.3561 - acc: 0.9021 - val_loss: 0.7671 - val_acc: 0.8600
Epoch 245/500
287s - loss: 0.3369 - acc: 0.8995 - val_loss: 0.7658 - val_acc: 0.8600
Epoch 246/500
287s - loss: 0.3224 - acc: 0.9063 - val_loss: 0.7686 - val_acc: 0.8558
Epoch 247/500
287s - loss: 0.3625 - acc: 0.9034 - val_loss: 0.7721 - val_acc: 0.8568
Epoch 248/500
287s - loss: 0.3308 - acc: 0.9037 - val_loss: 0.7667 - val_acc: 0.8568
Epoch 249/500
287s - loss: 0.3579 - acc: 0.9024 - val_loss: 0.7637 - val_acc: 0.8611
Epoch 250/500
287s - loss: 0.3578 - acc: 0.9032 - val_loss: 0.7685 - val_acc: 0.8558
Epoch 251/500
287s - loss: 0.3659 - acc: 0.8987 - val_loss: 0.7698 - val_acc: 0.8589
Epoch 252/500
287s - loss: 0.3768 - acc: 0.8976 - val_loss: 0.7645 - val_acc: 0.8589
Epoch 253/500
287s - loss: 0.3597 - acc: 0.9018 - val_loss: 0.7744 - val_acc: 0.8568
Epoch 254/500
287s - loss: 0.3279 - acc: 0.9026 - val_loss: 0.7714 - val_acc: 0.8547
Epoch 255/500
287s - loss: 0.3492 - acc: 0.9050 - val_loss: 0.7685 - val_acc: 0.8558
Epoch 256/500
287s - loss: 0.3480 - acc: 0.9021 - val_loss: 0.7726 - val_acc: 0.8589
Epoch 257/500
287s - loss: 0.3667 - acc: 0.8989 - val_loss: 0.7665 - val_acc: 0.8579
Epoch 258/500
287s - loss: 0.3683 - acc: 0.8966 - val_loss: 0.7657 - val_acc: 0.8568
Epoch 259/500
287s - loss: 0.3391 - acc: 0.9061 - val_loss: 0.7651 - val_acc: 0.8537
Epoch 260/500
287s - loss: 0.3957 - acc: 0.8963 - val_loss: 0.7678 - val_acc: 0.8537
Epoch 261/500
287s - loss: 0.3479 - acc: 0.9042 - val_loss: 0.7701 - val_acc: 0.8568
Epoch 262/500
287s - loss: 0.3475 - acc: 0.9005 - val_loss: 0.7704 - val_acc: 0.8537
Epoch 263/500
288s - loss: 0.3664 - acc: 0.8979 - val_loss: 0.7651 - val_acc: 0.8547
Epoch 264/500
287s - loss: 0.3471 - acc: 0.8997 - val_loss: 0.7592 - val_acc: 0.8579
Epoch 265/500
287s - loss: 0.3387 - acc: 0.9045 - val_loss: 0.7657 - val_acc: 0.8568
Epoch 266/500
287s - loss: 0.3586 - acc: 0.9024 - val_loss: 0.7665 - val_acc: 0.8579
Epoch 267/500
287s - loss: 0.3372 - acc: 0.9055 - val_loss: 0.7668 - val_acc: 0.8568
Epoch 268/500
287s - loss: 0.3383 - acc: 0.9066 - val_loss: 0.7635 - val_acc: 0.8568
Epoch 269/500
287s - loss: 0.3729 - acc: 0.8989 - val_loss: 0.7693 - val_acc: 0.8589
Epoch 270/500
287s - loss: 0.3644 - acc: 0.9018 - val_loss: 0.7669 - val_acc: 0.8558
Epoch 271/500
287s - loss: 0.3461 - acc: 0.9013 - val_loss: 0.7697 - val_acc: 0.8558
Epoch 272/500
287s - loss: 0.3642 - acc: 0.8995 - val_loss: 0.7679 - val_acc: 0.8600
Epoch 273/500
287s - loss: 0.3723 - acc: 0.8997 - val_loss: 0.7672 - val_acc: 0.8579
Epoch 274/500
287s - loss: 0.3497 - acc: 0.9029 - val_loss: 0.7688 - val_acc: 0.8579
Epoch 275/500
287s - loss: 0.3631 - acc: 0.8974 - val_loss: 0.7709 - val_acc: 0.8537
Epoch 276/500
287s - loss: 0.3703 - acc: 0.8971 - val_loss: 0.7697 - val_acc: 0.8579
Epoch 277/500
287s - loss: 0.3699 - acc: 0.8997 - val_loss: 0.7679 - val_acc: 0.8568
Epoch 278/500
287s - loss: 0.3383 - acc: 0.9047 - val_loss: 0.7738 - val_acc: 0.8516
Epoch 279/500
287s - loss: 0.3410 - acc: 0.9037 - val_loss: 0.7687 - val_acc: 0.8547
Epoch 280/500
287s - loss: 0.3240 - acc: 0.9055 - val_loss: 0.7710 - val_acc: 0.8589
Epoch 281/500
287s - loss: 0.3449 - acc: 0.9053 - val_loss: 0.7685 - val_acc: 0.8600
Epoch 282/500
287s - loss: 0.3359 - acc: 0.9016 - val_loss: 0.7662 - val_acc: 0.8589
Epoch 283/500
287s - loss: 0.3574 - acc: 0.8997 - val_loss: 0.7742 - val_acc: 0.8579
Epoch 284/500
287s - loss: 0.3535 - acc: 0.8995 - val_loss: 0.7733 - val_acc: 0.8600
Epoch 285/500
288s - loss: 0.3546 - acc: 0.9024 - val_loss: 0.7653 - val_acc: 0.8611
Epoch 286/500
288s - loss: 0.3630 - acc: 0.9037 - val_loss: 0.7689 - val_acc: 0.8589
Epoch 287/500
287s - loss: 0.3741 - acc: 0.8950 - val_loss: 0.7699 - val_acc: 0.8568
Epoch 288/500
287s - loss: 0.3424 - acc: 0.9032 - val_loss: 0.7688 - val_acc: 0.8537
Training loss for fold 1 is 0.21685948591483267 with percent 93.05263159149571
Testing loss for fold 1 is 0.7652301183499788 with percent 86.10526320808812
 
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_5 (InputLayer)             (None, 85, 85, 3)     0                                            
____________________________________________________________________________________________________
conv2d_13 (Conv2D)               (None, 83, 83, 16)    448         input_5[0][0]                    
____________________________________________________________________________________________________
batch_normalization_15 (BatchNor (None, 83, 83, 16)    64          conv2d_13[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_15 (LeakyReLU)       (None, 83, 83, 16)    0           batch_normalization_15[0][0]     
____________________________________________________________________________________________________
conv2d_14 (Conv2D)               (None, 82, 82, 16)    1040        leaky_re_lu_15[0][0]             
____________________________________________________________________________________________________
batch_normalization_16 (BatchNor (None, 82, 82, 16)    64          conv2d_14[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_16 (LeakyReLU)       (None, 82, 82, 16)    0           batch_normalization_16[0][0]     
____________________________________________________________________________________________________
max_pooling2d_7 (MaxPooling2D)   (None, 41, 41, 16)    0           leaky_re_lu_16[0][0]             
____________________________________________________________________________________________________
dropout_11 (Dropout)             (None, 41, 41, 16)    0           max_pooling2d_7[0][0]            
____________________________________________________________________________________________________
conv2d_15 (Conv2D)               (None, 39, 39, 32)    4640        dropout_11[0][0]                 
____________________________________________________________________________________________________
batch_normalization_17 (BatchNor (None, 39, 39, 32)    128         conv2d_15[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_17 (LeakyReLU)       (None, 39, 39, 32)    0           batch_normalization_17[0][0]     
____________________________________________________________________________________________________
conv2d_16 (Conv2D)               (None, 38, 38, 32)    4128        leaky_re_lu_17[0][0]             
____________________________________________________________________________________________________
batch_normalization_18 (BatchNor (None, 38, 38, 32)    128         conv2d_16[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_18 (LeakyReLU)       (None, 38, 38, 32)    0           batch_normalization_18[0][0]     
____________________________________________________________________________________________________
max_pooling2d_8 (MaxPooling2D)   (None, 19, 19, 32)    0           leaky_re_lu_18[0][0]             
____________________________________________________________________________________________________
dropout_12 (Dropout)             (None, 19, 19, 32)    0           max_pooling2d_8[0][0]            
____________________________________________________________________________________________________
conv2d_17 (Conv2D)               (None, 17, 17, 64)    18496       dropout_12[0][0]                 
____________________________________________________________________________________________________
batch_normalization_19 (BatchNor (None, 17, 17, 64)    256         conv2d_17[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_19 (LeakyReLU)       (None, 17, 17, 64)    0           batch_normalization_19[0][0]     
____________________________________________________________________________________________________
conv2d_18 (Conv2D)               (None, 16, 16, 64)    16448       leaky_re_lu_19[0][0]             
____________________________________________________________________________________________________
batch_normalization_20 (BatchNor (None, 16, 16, 64)    256         conv2d_18[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_20 (LeakyReLU)       (None, 16, 16, 64)    0           batch_normalization_20[0][0]     
____________________________________________________________________________________________________
max_pooling2d_9 (MaxPooling2D)   (None, 8, 8, 64)      0           leaky_re_lu_20[0][0]             
____________________________________________________________________________________________________
dropout_13 (Dropout)             (None, 8, 8, 64)      0           max_pooling2d_9[0][0]            
____________________________________________________________________________________________________
flatten_3 (Flatten)              (None, 4096)          0           dropout_13[0][0]                 
____________________________________________________________________________________________________
batch_normalization_21 (BatchNor (None, 4096)          16384       flatten_3[0][0]                  
____________________________________________________________________________________________________
input_6 (InputLayer)             (None, 102)           0                                            
____________________________________________________________________________________________________
concatenate_3 (Concatenate)      (None, 4198)          0           batch_normalization_21[0][0]     
                                                                   input_6[0][0]                    
____________________________________________________________________________________________________
dense_9 (Dense)                  (None, 128)           537472      concatenate_3[0][0]              
____________________________________________________________________________________________________
leaky_re_lu_21 (LeakyReLU)       (None, 128)           0           dense_9[0][0]                    
____________________________________________________________________________________________________
dense_10 (Dense)                 (None, 64)            8256        leaky_re_lu_21[0][0]             
____________________________________________________________________________________________________
dropout_14 (Dropout)             (None, 64)            0           dense_10[0][0]                   
____________________________________________________________________________________________________
dense_11 (Dense)                 (None, 32)            2080        dropout_14[0][0]                 
____________________________________________________________________________________________________
dropout_15 (Dropout)             (None, 32)            0           dense_11[0][0]                   
____________________________________________________________________________________________________
dense_12 (Dense)                 (None, 12)            396         dropout_15[0][0]                 
====================================================================================================
Total params: 610,684
Trainable params: 602,044
Non-trainable params: 8,640
____________________________________________________________________________________________________
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
292s - loss: 2.0410 - acc: 0.3289 - val_loss: 4.0427 - val_acc: 0.1789
Epoch 2/500
289s - loss: 1.6008 - acc: 0.4866 - val_loss: 1.9748 - val_acc: 0.3347
Epoch 3/500
289s - loss: 1.3898 - acc: 0.5611 - val_loss: 2.2148 - val_acc: 0.3968
Epoch 4/500
289s - loss: 1.3463 - acc: 0.5808 - val_loss: 1.2870 - val_acc: 0.6011
Epoch 5/500
289s - loss: 1.3070 - acc: 0.6153 - val_loss: 2.3392 - val_acc: 0.3611
Epoch 6/500
289s - loss: 1.2926 - acc: 0.6068 - val_loss: 1.4530 - val_acc: 0.5716
Epoch 7/500
289s - loss: 1.2267 - acc: 0.6376 - val_loss: 1.6096 - val_acc: 0.5905
Epoch 8/500
289s - loss: 1.1934 - acc: 0.6508 - val_loss: 2.0476 - val_acc: 0.4568
Epoch 9/500
289s - loss: 1.2200 - acc: 0.6611 - val_loss: 1.9938 - val_acc: 0.5000
Epoch 10/500
289s - loss: 1.1660 - acc: 0.6697 - val_loss: 1.7799 - val_acc: 0.5474
Epoch 11/500
289s - loss: 1.2162 - acc: 0.6650 - val_loss: 1.4281 - val_acc: 0.6253
Epoch 12/500
289s - loss: 1.2249 - acc: 0.6537 - val_loss: 2.1534 - val_acc: 0.4958
Epoch 13/500
289s - loss: 1.1790 - acc: 0.6779 - val_loss: 2.0288 - val_acc: 0.4789
Epoch 14/500
289s - loss: 1.2172 - acc: 0.6776 - val_loss: 2.4833 - val_acc: 0.5442
Epoch 15/500
289s - loss: 1.2353 - acc: 0.6882 - val_loss: 2.2804 - val_acc: 0.6042
Epoch 16/500
289s - loss: 1.1757 - acc: 0.6892 - val_loss: 1.6674 - val_acc: 0.6400
Epoch 17/500
289s - loss: 1.1451 - acc: 0.7129 - val_loss: 3.7818 - val_acc: 0.4726
Epoch 18/500
289s - loss: 1.1647 - acc: 0.7039 - val_loss: 1.5248 - val_acc: 0.6863
Epoch 19/500
289s - loss: 1.1669 - acc: 0.7203 - val_loss: 1.8716 - val_acc: 0.6653
Epoch 20/500
290s - loss: 1.2381 - acc: 0.7042 - val_loss: 2.1421 - val_acc: 0.5316
Epoch 21/500
289s - loss: 1.1853 - acc: 0.6908 - val_loss: 1.3949 - val_acc: 0.6895
Epoch 22/500
289s - loss: 1.3487 - acc: 0.6887 - val_loss: 2.6445 - val_acc: 0.5411
Epoch 23/500
288s - loss: 1.1196 - acc: 0.7171 - val_loss: 2.1516 - val_acc: 0.6011
Epoch 24/500
288s - loss: 1.2272 - acc: 0.7024 - val_loss: 4.8985 - val_acc: 0.2442
Epoch 25/500
288s - loss: 1.1996 - acc: 0.7103 - val_loss: 4.0753 - val_acc: 0.5516
Epoch 26/500
288s - loss: 1.1947 - acc: 0.7179 - val_loss: 2.9307 - val_acc: 0.4611
Epoch 27/500
288s - loss: 1.2424 - acc: 0.7116 - val_loss: 4.8908 - val_acc: 0.2284
Epoch 28/500
288s - loss: 1.3264 - acc: 0.6695 - val_loss: 1.5690 - val_acc: 0.6358
Epoch 29/500
288s - loss: 1.0882 - acc: 0.7316 - val_loss: 8.4134 - val_acc: 0.2295
Epoch 30/500

Epoch 00029: reducing learning rate to 0.010000000149.
287s - loss: 1.3275 - acc: 0.7266 - val_loss: 1.4872 - val_acc: 0.6737
Epoch 31/500
286s - loss: 1.1225 - acc: 0.7318 - val_loss: 1.3360 - val_acc: 0.7200
Epoch 32/500
286s - loss: 0.9217 - acc: 0.7697 - val_loss: 1.0888 - val_acc: 0.7505
Epoch 33/500
286s - loss: 0.8960 - acc: 0.7711 - val_loss: 1.0946 - val_acc: 0.7558
Epoch 34/500
286s - loss: 0.8426 - acc: 0.7871 - val_loss: 0.9212 - val_acc: 0.7779
Epoch 35/500
286s - loss: 0.7668 - acc: 0.7929 - val_loss: 0.9875 - val_acc: 0.7747
Epoch 36/500
286s - loss: 0.7553 - acc: 0.7992 - val_loss: 0.8418 - val_acc: 0.7947
Epoch 37/500
286s - loss: 0.7346 - acc: 0.7987 - val_loss: 0.9825 - val_acc: 0.7768
Epoch 38/500
287s - loss: 0.6913 - acc: 0.8095 - val_loss: 0.9300 - val_acc: 0.7811
Epoch 39/500
288s - loss: 0.6649 - acc: 0.8153 - val_loss: 1.0101 - val_acc: 0.7747
Epoch 40/500
