Using TensorFlow backend.
/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
 
Did not find train data with correct size. Generating...
 
/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.
  warn("The default mode, 'constant', will be changed to 'reflect' in "
Done. Loading train images
 
 
Did not find test data with correct size. Generating...
 
Done. Loading test images
 
Augmentation data size (4750, 102) (794, 102)
/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)
(3800, 75, 75, 3) (3800,)
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_1 (InputLayer)             (None, 75, 75, 3)     0                                            
____________________________________________________________________________________________________
conv2d_1 (Conv2D)                (None, 73, 73, 16)    448         input_1[0][0]                    
____________________________________________________________________________________________________
batch_normalization_1 (BatchNorm (None, 73, 73, 16)    64          conv2d_1[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_1 (LeakyReLU)        (None, 73, 73, 16)    0           batch_normalization_1[0][0]      
____________________________________________________________________________________________________
conv2d_2 (Conv2D)                (None, 72, 72, 16)    1040        leaky_re_lu_1[0][0]              
____________________________________________________________________________________________________
batch_normalization_2 (BatchNorm (None, 72, 72, 16)    64          conv2d_2[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_2 (LeakyReLU)        (None, 72, 72, 16)    0           batch_normalization_2[0][0]      
____________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)   (None, 36, 36, 16)    0           leaky_re_lu_2[0][0]              
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 36, 36, 16)    0           max_pooling2d_1[0][0]            
____________________________________________________________________________________________________
conv2d_3 (Conv2D)                (None, 34, 34, 32)    4640        dropout_1[0][0]                  
____________________________________________________________________________________________________
batch_normalization_3 (BatchNorm (None, 34, 34, 32)    128         conv2d_3[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_3 (LeakyReLU)        (None, 34, 34, 32)    0           batch_normalization_3[0][0]      
____________________________________________________________________________________________________
conv2d_4 (Conv2D)                (None, 33, 33, 32)    4128        leaky_re_lu_3[0][0]              
____________________________________________________________________________________________________
batch_normalization_4 (BatchNorm (None, 33, 33, 32)    128         conv2d_4[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_4 (LeakyReLU)        (None, 33, 33, 32)    0           batch_normalization_4[0][0]      
____________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)   (None, 16, 16, 32)    0           leaky_re_lu_4[0][0]              
____________________________________________________________________________________________________
dropout_2 (Dropout)              (None, 16, 16, 32)    0           max_pooling2d_2[0][0]            
____________________________________________________________________________________________________
conv2d_5 (Conv2D)                (None, 14, 14, 64)    18496       dropout_2[0][0]                  
____________________________________________________________________________________________________
batch_normalization_5 (BatchNorm (None, 14, 14, 64)    256         conv2d_5[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_5 (LeakyReLU)        (None, 14, 14, 64)    0           batch_normalization_5[0][0]      
____________________________________________________________________________________________________
conv2d_6 (Conv2D)                (None, 13, 13, 64)    16448       leaky_re_lu_5[0][0]              
____________________________________________________________________________________________________
batch_normalization_6 (BatchNorm (None, 13, 13, 64)    256         conv2d_6[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_6 (LeakyReLU)        (None, 13, 13, 64)    0           batch_normalization_6[0][0]      
____________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)   (None, 6, 6, 64)      0           leaky_re_lu_6[0][0]              
____________________________________________________________________________________________________
dropout_3 (Dropout)              (None, 6, 6, 64)      0           max_pooling2d_3[0][0]            
____________________________________________________________________________________________________
flatten_1 (Flatten)              (None, 2304)          0           dropout_3[0][0]                  
____________________________________________________________________________________________________
batch_normalization_7 (BatchNorm (None, 2304)          9216        flatten_1[0][0]                  
____________________________________________________________________________________________________
input_2 (InputLayer)             (None, 102)           0                                            
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 2406)          0           batch_normalization_7[0][0]      
                                                                   input_2[0][0]                    
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 128)           308096      concatenate_1[0][0]              
____________________________________________________________________________________________________
leaky_re_lu_7 (LeakyReLU)        (None, 128)           0           dense_1[0][0]                    
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 64)            8256        leaky_re_lu_7[0][0]              
____________________________________________________________________________________________________
dropout_4 (Dropout)              (None, 64)            0           dense_2[0][0]                    
____________________________________________________________________________________________________
dense_3 (Dense)                  (None, 32)            2080        dropout_4[0][0]                  
____________________________________________________________________________________________________
dropout_5 (Dropout)              (None, 32)            0           dense_3[0][0]                    
____________________________________________________________________________________________________
dense_4 (Dense)                  (None, 12)            396         dropout_5[0][0]                  
====================================================================================================
Total params: 374,140
Trainable params: 369,084
Non-trainable params: 5,056
____________________________________________________________________________________________________
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
2018-08-13 23:35:24.244067: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-08-13 23:35:24.244425: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
233s - loss: 2.0333 - acc: 0.3103 - val_loss: 4.0931 - val_acc: 0.1411
Epoch 2/500
230s - loss: 1.5777 - acc: 0.4911 - val_loss: 3.6089 - val_acc: 0.2179
Epoch 3/500
231s - loss: 1.4371 - acc: 0.5442 - val_loss: 3.2747 - val_acc: 0.2768
Epoch 4/500
231s - loss: 1.3748 - acc: 0.5800 - val_loss: 1.2572 - val_acc: 0.6189
Epoch 5/500
231s - loss: 1.2927 - acc: 0.6026 - val_loss: 2.3330 - val_acc: 0.4126
Epoch 6/500
231s - loss: 1.2056 - acc: 0.6313 - val_loss: 2.1475 - val_acc: 0.3168
Epoch 7/500
232s - loss: 1.1687 - acc: 0.6413 - val_loss: 3.9445 - val_acc: 0.2968
Epoch 8/500
233s - loss: 1.1241 - acc: 0.6666 - val_loss: 2.3886 - val_acc: 0.4653
Epoch 9/500
232s - loss: 1.0784 - acc: 0.6861 - val_loss: 1.1356 - val_acc: 0.6989
Epoch 10/500
232s - loss: 1.0416 - acc: 0.6947 - val_loss: 1.4282 - val_acc: 0.5789
Epoch 11/500
232s - loss: 1.0254 - acc: 0.7037 - val_loss: 1.3909 - val_acc: 0.6105
Epoch 12/500
232s - loss: 0.9850 - acc: 0.7113 - val_loss: 1.8307 - val_acc: 0.5568
Epoch 13/500
230s - loss: 0.9807 - acc: 0.7237 - val_loss: 1.4668 - val_acc: 0.6926
Epoch 14/500
231s - loss: 0.9939 - acc: 0.7192 - val_loss: 1.3680 - val_acc: 0.6642
Epoch 15/500
231s - loss: 0.9504 - acc: 0.7434 - val_loss: 1.1173 - val_acc: 0.6989
Epoch 16/500
231s - loss: 0.9099 - acc: 0.7526 - val_loss: 1.0722 - val_acc: 0.7621
Epoch 17/500
230s - loss: 0.9286 - acc: 0.7574 - val_loss: 1.6660 - val_acc: 0.6674
Epoch 18/500
230s - loss: 0.9537 - acc: 0.7545 - val_loss: 0.9738 - val_acc: 0.7463
Epoch 19/500
230s - loss: 0.9609 - acc: 0.7611 - val_loss: 1.0820 - val_acc: 0.7674
Epoch 20/500
232s - loss: 0.8879 - acc: 0.7695 - val_loss: 1.4724 - val_acc: 0.7137
Epoch 21/500
231s - loss: 0.8947 - acc: 0.7676 - val_loss: 1.3205 - val_acc: 0.7158
Epoch 22/500
230s - loss: 0.9483 - acc: 0.7561 - val_loss: 1.2055 - val_acc: 0.7126
Epoch 23/500
229s - loss: 0.8658 - acc: 0.7755 - val_loss: 1.0101 - val_acc: 0.7232
Epoch 24/500
230s - loss: 0.8661 - acc: 0.7958 - val_loss: 3.6904 - val_acc: 0.4189
Epoch 25/500
229s - loss: 0.9220 - acc: 0.7766 - val_loss: 1.5236 - val_acc: 0.6947
Epoch 26/500
232s - loss: 0.9587 - acc: 0.7747 - val_loss: 1.7909 - val_acc: 0.6474
Epoch 27/500
232s - loss: 0.9773 - acc: 0.7711 - val_loss: 2.5696 - val_acc: 0.5316
Epoch 28/500

Epoch 00027: reducing learning rate to 0.010000000149.
232s - loss: 0.9135 - acc: 0.7776 - val_loss: 1.1193 - val_acc: 0.7484
Epoch 29/500
232s - loss: 0.8168 - acc: 0.7953 - val_loss: 0.8183 - val_acc: 0.8200
Epoch 30/500
233s - loss: 0.6261 - acc: 0.8287 - val_loss: 0.8249 - val_acc: 0.8274
Epoch 31/500
234s - loss: 0.5752 - acc: 0.8389 - val_loss: 0.7726 - val_acc: 0.8389
Epoch 32/500
234s - loss: 0.5550 - acc: 0.8492 - val_loss: 0.8457 - val_acc: 0.8358
Epoch 33/500
232s - loss: 0.5485 - acc: 0.8500 - val_loss: 0.8424 - val_acc: 0.8211
Epoch 34/500
230s - loss: 0.4787 - acc: 0.8589 - val_loss: 0.7693 - val_acc: 0.8453
Epoch 35/500
231s - loss: 0.5101 - acc: 0.8511 - val_loss: 0.7613 - val_acc: 0.8411
Epoch 36/500
232s - loss: 0.4792 - acc: 0.8616 - val_loss: 0.7655 - val_acc: 0.8516
Epoch 37/500
232s - loss: 0.5170 - acc: 0.8545 - val_loss: 0.7478 - val_acc: 0.8526
Epoch 38/500
231s - loss: 0.4610 - acc: 0.8629 - val_loss: 0.7865 - val_acc: 0.8411
Epoch 39/500
233s - loss: 0.4601 - acc: 0.8663 - val_loss: 0.7467 - val_acc: 0.8484
Epoch 40/500
232s - loss: 0.4496 - acc: 0.8705 - val_loss: 0.7494 - val_acc: 0.8411
Epoch 41/500
231s - loss: 0.4443 - acc: 0.8711 - val_loss: 0.7526 - val_acc: 0.8463
Epoch 42/500
230s - loss: 0.4395 - acc: 0.8745 - val_loss: 0.6990 - val_acc: 0.8589
Epoch 43/500
232s - loss: 0.4283 - acc: 0.8776 - val_loss: 0.8338 - val_acc: 0.8295
Epoch 44/500
232s - loss: 0.4024 - acc: 0.8755 - val_loss: 0.7081 - val_acc: 0.8632
Epoch 45/500
232s - loss: 0.4095 - acc: 0.8834 - val_loss: 0.7266 - val_acc: 0.8516
Epoch 46/500
230s - loss: 0.3847 - acc: 0.8855 - val_loss: 0.7078 - val_acc: 0.8611
Epoch 47/500
230s - loss: 0.4037 - acc: 0.8842 - val_loss: 0.7396 - val_acc: 0.8579
Epoch 48/500
230s - loss: 0.3388 - acc: 0.8945 - val_loss: 0.7117 - val_acc: 0.8632
Epoch 49/500
234s - loss: 0.3734 - acc: 0.8905 - val_loss: 0.8312 - val_acc: 0.8453
Epoch 50/500
231s - loss: 0.3791 - acc: 0.8863 - val_loss: 0.8050 - val_acc: 0.8453
Epoch 51/500
230s - loss: 0.3734 - acc: 0.8884 - val_loss: 0.6936 - val_acc: 0.8568
Epoch 52/500
230s - loss: 0.3542 - acc: 0.8900 - val_loss: 0.7285 - val_acc: 0.8611
Epoch 53/500

Epoch 00052: reducing learning rate to 0.000999999977648.
230s - loss: 0.3751 - acc: 0.8929 - val_loss: 0.7859 - val_acc: 0.8547
Epoch 54/500
230s - loss: 0.3340 - acc: 0.8971 - val_loss: 0.7430 - val_acc: 0.8621
Epoch 55/500
230s - loss: 0.3648 - acc: 0.8908 - val_loss: 0.7451 - val_acc: 0.8611
Epoch 56/500
230s - loss: 0.3455 - acc: 0.8971 - val_loss: 0.7447 - val_acc: 0.8589
Epoch 57/500
230s - loss: 0.3587 - acc: 0.8913 - val_loss: 0.7563 - val_acc: 0.8600
Epoch 58/500
230s - loss: 0.3509 - acc: 0.8921 - val_loss: 0.7354 - val_acc: 0.8632
Epoch 59/500
230s - loss: 0.3461 - acc: 0.8976 - val_loss: 0.7265 - val_acc: 0.8642
Epoch 60/500
232s - loss: 0.3242 - acc: 0.8953 - val_loss: 0.7239 - val_acc: 0.8621
Epoch 61/500
233s - loss: 0.3165 - acc: 0.8963 - val_loss: 0.7162 - val_acc: 0.8663
Epoch 62/500
232s - loss: 0.3254 - acc: 0.9021 - val_loss: 0.7151 - val_acc: 0.8663
Epoch 63/500
230s - loss: 0.3408 - acc: 0.8932 - val_loss: 0.7144 - val_acc: 0.8663
Epoch 64/500
230s - loss: 0.3298 - acc: 0.9013 - val_loss: 0.7104 - val_acc: 0.8653
Epoch 65/500
230s - loss: 0.3573 - acc: 0.9008 - val_loss: 0.7105 - val_acc: 0.8674
Epoch 66/500
221s - loss: 0.3486 - acc: 0.8947 - val_loss: 0.7093 - val_acc: 0.8695
Epoch 67/500
222s - loss: 0.3021 - acc: 0.9003 - val_loss: 0.7013 - val_acc: 0.8674
Epoch 68/500
220s - loss: 0.3287 - acc: 0.8987 - val_loss: 0.6906 - val_acc: 0.8674
Epoch 69/500
221s - loss: 0.3206 - acc: 0.8995 - val_loss: 0.7053 - val_acc: 0.8674
Epoch 70/500
219s - loss: 0.3377 - acc: 0.8934 - val_loss: 0.7113 - val_acc: 0.8674
Epoch 71/500
217s - loss: 0.3282 - acc: 0.8995 - val_loss: 0.6971 - val_acc: 0.8684
Epoch 72/500
217s - loss: 0.3168 - acc: 0.9000 - val_loss: 0.7145 - val_acc: 0.8663
Epoch 73/500
216s - loss: 0.3139 - acc: 0.9024 - val_loss: 0.7108 - val_acc: 0.8695
Epoch 74/500
217s - loss: 0.3010 - acc: 0.8974 - val_loss: 0.6954 - val_acc: 0.8684
Epoch 75/500

Epoch 00074: reducing learning rate to 9.99999931082e-05.
217s - loss: 0.3374 - acc: 0.9021 - val_loss: 0.7083 - val_acc: 0.8684
Epoch 76/500
217s - loss: 0.3387 - acc: 0.8961 - val_loss: 0.7030 - val_acc: 0.8705
Epoch 77/500
221s - loss: 0.3292 - acc: 0.8966 - val_loss: 0.7030 - val_acc: 0.8705
Epoch 78/500
218s - loss: 0.3153 - acc: 0.8987 - val_loss: 0.7023 - val_acc: 0.8695
Epoch 79/500
217s - loss: 0.3374 - acc: 0.9021 - val_loss: 0.7034 - val_acc: 0.8695
Epoch 80/500
217s - loss: 0.2945 - acc: 0.9018 - val_loss: 0.7091 - val_acc: 0.8705
Epoch 81/500
217s - loss: 0.3138 - acc: 0.9011 - val_loss: 0.7088 - val_acc: 0.8684
Epoch 82/500
217s - loss: 0.3475 - acc: 0.8974 - val_loss: 0.7020 - val_acc: 0.8695
Epoch 83/500
219s - loss: 0.3059 - acc: 0.9024 - val_loss: 0.7060 - val_acc: 0.8695
Epoch 84/500
216s - loss: 0.3444 - acc: 0.8982 - val_loss: 0.7054 - val_acc: 0.8695
Epoch 85/500

Epoch 00084: reducing learning rate to 9.99999901978e-06.
217s - loss: 0.3158 - acc: 0.9005 - val_loss: 0.7042 - val_acc: 0.8705
Epoch 86/500
216s - loss: 0.3203 - acc: 0.8984 - val_loss: 0.7067 - val_acc: 0.8705
Epoch 87/500
217s - loss: 0.3392 - acc: 0.8974 - val_loss: 0.7043 - val_acc: 0.8695
Epoch 88/500
217s - loss: 0.3103 - acc: 0.8989 - val_loss: 0.7036 - val_acc: 0.8705
Epoch 89/500
216s - loss: 0.3233 - acc: 0.9016 - val_loss: 0.7007 - val_acc: 0.8705
Epoch 90/500
217s - loss: 0.3008 - acc: 0.9066 - val_loss: 0.7049 - val_acc: 0.8695
Epoch 91/500
219s - loss: 0.3228 - acc: 0.8982 - val_loss: 0.6986 - val_acc: 0.8695
Epoch 92/500
217s - loss: 0.3554 - acc: 0.9003 - val_loss: 0.7032 - val_acc: 0.8695
Epoch 93/500

Epoch 00092: reducing learning rate to 1e-06.
215s - loss: 0.3261 - acc: 0.8997 - val_loss: 0.7102 - val_acc: 0.8705
Epoch 94/500
216s - loss: 0.3551 - acc: 0.8942 - val_loss: 0.7109 - val_acc: 0.8684
Epoch 95/500
217s - loss: 0.3485 - acc: 0.8950 - val_loss: 0.7016 - val_acc: 0.8705
Epoch 96/500
216s - loss: 0.3140 - acc: 0.9021 - val_loss: 0.7033 - val_acc: 0.8695
Epoch 97/500
216s - loss: 0.3179 - acc: 0.8997 - val_loss: 0.6999 - val_acc: 0.8695
Epoch 98/500
215s - loss: 0.3272 - acc: 0.8976 - val_loss: 0.7029 - val_acc: 0.8695
Epoch 99/500
218s - loss: 0.3174 - acc: 0.8997 - val_loss: 0.7066 - val_acc: 0.8684
Epoch 100/500
217s - loss: 0.3158 - acc: 0.9024 - val_loss: 0.7054 - val_acc: 0.8705
Epoch 101/500
217s - loss: 0.3218 - acc: 0.9024 - val_loss: 0.7098 - val_acc: 0.8705
Epoch 102/500
218s - loss: 0.3513 - acc: 0.8974 - val_loss: 0.7092 - val_acc: 0.8695
Epoch 103/500
217s - loss: 0.3152 - acc: 0.9021 - val_loss: 0.7050 - val_acc: 0.8705
Epoch 104/500
217s - loss: 0.3338 - acc: 0.8982 - val_loss: 0.7021 - val_acc: 0.8705
Epoch 105/500
217s - loss: 0.3018 - acc: 0.9008 - val_loss: 0.7055 - val_acc: 0.8705
Epoch 106/500
217s - loss: 0.3202 - acc: 0.9003 - val_loss: 0.7108 - val_acc: 0.8695
Epoch 107/500
217s - loss: 0.3300 - acc: 0.8984 - val_loss: 0.7036 - val_acc: 0.8705
Epoch 108/500
217s - loss: 0.2985 - acc: 0.9039 - val_loss: 0.7032 - val_acc: 0.8705
Epoch 109/500
218s - loss: 0.3276 - acc: 0.9008 - val_loss: 0.7057 - val_acc: 0.8705
Epoch 110/500
216s - loss: 0.3309 - acc: 0.8921 - val_loss: 0.7057 - val_acc: 0.8684
Epoch 111/500
217s - loss: 0.3279 - acc: 0.9008 - val_loss: 0.7075 - val_acc: 0.8674
Epoch 112/500
216s - loss: 0.3169 - acc: 0.9047 - val_loss: 0.7066 - val_acc: 0.8695
Epoch 113/500
216s - loss: 0.3149 - acc: 0.9005 - val_loss: 0.7068 - val_acc: 0.8705
Epoch 114/500
216s - loss: 0.3148 - acc: 0.9003 - val_loss: 0.7062 - val_acc: 0.8705
Epoch 115/500
216s - loss: 0.3311 - acc: 0.8995 - val_loss: 0.7063 - val_acc: 0.8705
Epoch 116/500
216s - loss: 0.3131 - acc: 0.9000 - val_loss: 0.6984 - val_acc: 0.8705
Epoch 117/500
215s - loss: 0.3157 - acc: 0.9047 - val_loss: 0.7071 - val_acc: 0.8695
Epoch 118/500
217s - loss: 0.3282 - acc: 0.9003 - val_loss: 0.7001 - val_acc: 0.8695
Epoch 119/500
216s - loss: 0.3134 - acc: 0.8987 - val_loss: 0.7051 - val_acc: 0.8705
Epoch 120/500
216s - loss: 0.3295 - acc: 0.8987 - val_loss: 0.7079 - val_acc: 0.8705
Epoch 121/500
218s - loss: 0.3197 - acc: 0.9042 - val_loss: 0.7048 - val_acc: 0.8684
Epoch 122/500
217s - loss: 0.3234 - acc: 0.8995 - val_loss: 0.7079 - val_acc: 0.8705
Epoch 123/500
216s - loss: 0.3186 - acc: 0.9003 - val_loss: 0.7077 - val_acc: 0.8705
Epoch 124/500
215s - loss: 0.3176 - acc: 0.9000 - val_loss: 0.7047 - val_acc: 0.8695
Epoch 125/500
217s - loss: 0.3567 - acc: 0.9008 - val_loss: 0.7052 - val_acc: 0.8684
Epoch 126/500
216s - loss: 0.3226 - acc: 0.9024 - val_loss: 0.7072 - val_acc: 0.8684
Epoch 127/500
216s - loss: 0.3280 - acc: 0.8976 - val_loss: 0.7011 - val_acc: 0.8695
Epoch 128/500
216s - loss: 0.3116 - acc: 0.9003 - val_loss: 0.7057 - val_acc: 0.8705
Epoch 129/500
216s - loss: 0.3113 - acc: 0.9061 - val_loss: 0.7025 - val_acc: 0.8705
Training loss for fold 0 is 0.19100658181319505 with percent 92.9210526190306
Testing loss for fold 0 is 0.7029581609525178 with percent 87.05263152875399
 
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_3 (InputLayer)             (None, 75, 75, 3)     0                                            
____________________________________________________________________________________________________
conv2d_7 (Conv2D)                (None, 73, 73, 16)    448         input_3[0][0]                    
____________________________________________________________________________________________________
batch_normalization_8 (BatchNorm (None, 73, 73, 16)    64          conv2d_7[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_8 (LeakyReLU)        (None, 73, 73, 16)    0           batch_normalization_8[0][0]      
____________________________________________________________________________________________________
conv2d_8 (Conv2D)                (None, 72, 72, 16)    1040        leaky_re_lu_8[0][0]              
____________________________________________________________________________________________________
batch_normalization_9 (BatchNorm (None, 72, 72, 16)    64          conv2d_8[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_9 (LeakyReLU)        (None, 72, 72, 16)    0           batch_normalization_9[0][0]      
____________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)   (None, 36, 36, 16)    0           leaky_re_lu_9[0][0]              
____________________________________________________________________________________________________
dropout_6 (Dropout)              (None, 36, 36, 16)    0           max_pooling2d_4[0][0]            
____________________________________________________________________________________________________
conv2d_9 (Conv2D)                (None, 34, 34, 32)    4640        dropout_6[0][0]                  
____________________________________________________________________________________________________
batch_normalization_10 (BatchNor (None, 34, 34, 32)    128         conv2d_9[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_10 (LeakyReLU)       (None, 34, 34, 32)    0           batch_normalization_10[0][0]     
____________________________________________________________________________________________________
conv2d_10 (Conv2D)               (None, 33, 33, 32)    4128        leaky_re_lu_10[0][0]             
____________________________________________________________________________________________________
batch_normalization_11 (BatchNor (None, 33, 33, 32)    128         conv2d_10[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_11 (LeakyReLU)       (None, 33, 33, 32)    0           batch_normalization_11[0][0]     
____________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)   (None, 16, 16, 32)    0           leaky_re_lu_11[0][0]             
____________________________________________________________________________________________________
dropout_7 (Dropout)              (None, 16, 16, 32)    0           max_pooling2d_5[0][0]            
____________________________________________________________________________________________________
conv2d_11 (Conv2D)               (None, 14, 14, 64)    18496       dropout_7[0][0]                  
____________________________________________________________________________________________________
batch_normalization_12 (BatchNor (None, 14, 14, 64)    256         conv2d_11[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_12 (LeakyReLU)       (None, 14, 14, 64)    0           batch_normalization_12[0][0]     
____________________________________________________________________________________________________
conv2d_12 (Conv2D)               (None, 13, 13, 64)    16448       leaky_re_lu_12[0][0]             
____________________________________________________________________________________________________
batch_normalization_13 (BatchNor (None, 13, 13, 64)    256         conv2d_12[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_13 (LeakyReLU)       (None, 13, 13, 64)    0           batch_normalization_13[0][0]     
____________________________________________________________________________________________________
max_pooling2d_6 (MaxPooling2D)   (None, 6, 6, 64)      0           leaky_re_lu_13[0][0]             
____________________________________________________________________________________________________
dropout_8 (Dropout)              (None, 6, 6, 64)      0           max_pooling2d_6[0][0]            
____________________________________________________________________________________________________
flatten_2 (Flatten)              (None, 2304)          0           dropout_8[0][0]                  
____________________________________________________________________________________________________
batch_normalization_14 (BatchNor (None, 2304)          9216        flatten_2[0][0]                  
____________________________________________________________________________________________________
input_4 (InputLayer)             (None, 102)           0                                            
____________________________________________________________________________________________________
concatenate_2 (Concatenate)      (None, 2406)          0           batch_normalization_14[0][0]     
                                                                   input_4[0][0]                    
____________________________________________________________________________________________________
dense_5 (Dense)                  (None, 128)           308096      concatenate_2[0][0]              
____________________________________________________________________________________________________
leaky_re_lu_14 (LeakyReLU)       (None, 128)           0           dense_5[0][0]                    
____________________________________________________________________________________________________
dense_6 (Dense)                  (None, 64)            8256        leaky_re_lu_14[0][0]             
____________________________________________________________________________________________________
dropout_9 (Dropout)              (None, 64)            0           dense_6[0][0]                    
____________________________________________________________________________________________________
dense_7 (Dense)                  (None, 32)            2080        dropout_9[0][0]                  
____________________________________________________________________________________________________
dropout_10 (Dropout)             (None, 32)            0           dense_7[0][0]                    
____________________________________________________________________________________________________
dense_8 (Dense)                  (None, 12)            396         dropout_10[0][0]                 
====================================================================================================
Total params: 374,140
Trainable params: 369,084
Non-trainable params: 5,056
____________________________________________________________________________________________________
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
222s - loss: 2.0718 - acc: 0.2903 - val_loss: 1.9292 - val_acc: 0.3389
Epoch 2/500
221s - loss: 1.6106 - acc: 0.4574 - val_loss: 2.6756 - val_acc: 0.2853
Epoch 3/500
227s - loss: 1.4818 - acc: 0.5208 - val_loss: 2.2756 - val_acc: 0.4221
Epoch 4/500
227s - loss: 1.3390 - acc: 0.5705 - val_loss: 2.1540 - val_acc: 0.4158
Epoch 5/500
227s - loss: 1.2830 - acc: 0.6168 - val_loss: 3.0780 - val_acc: 0.4305
Epoch 6/500
227s - loss: 1.2511 - acc: 0.6266 - val_loss: 1.9716 - val_acc: 0.5105
Epoch 7/500
228s - loss: 1.2333 - acc: 0.6374 - val_loss: 1.6646 - val_acc: 0.5684
Epoch 8/500
227s - loss: 1.1473 - acc: 0.6600 - val_loss: 3.3330 - val_acc: 0.4074
Epoch 9/500
228s - loss: 1.1490 - acc: 0.6729 - val_loss: 1.6855 - val_acc: 0.5768
Epoch 10/500
227s - loss: 1.0816 - acc: 0.6911 - val_loss: 1.1038 - val_acc: 0.7358
Epoch 11/500
227s - loss: 1.1332 - acc: 0.6871 - val_loss: 1.9774 - val_acc: 0.5274
Epoch 12/500
227s - loss: 1.1073 - acc: 0.6905 - val_loss: 1.1233 - val_acc: 0.7042
Epoch 13/500
228s - loss: 1.0535 - acc: 0.7005 - val_loss: 3.8714 - val_acc: 0.3042
Epoch 14/500
229s - loss: 1.0181 - acc: 0.7089 - val_loss: 1.4181 - val_acc: 0.6526
Epoch 15/500
230s - loss: 1.0886 - acc: 0.6979 - val_loss: 1.1994 - val_acc: 0.6653
Epoch 16/500
229s - loss: 1.0776 - acc: 0.7000 - val_loss: 2.7706 - val_acc: 0.4442
Epoch 17/500
230s - loss: 1.0601 - acc: 0.7163 - val_loss: 3.0460 - val_acc: 0.4779
Epoch 18/500
229s - loss: 1.0563 - acc: 0.7026 - val_loss: 1.0970 - val_acc: 0.6821
Epoch 19/500

Epoch 00018: reducing learning rate to 0.010000000149.
230s - loss: 1.0727 - acc: 0.7163 - val_loss: 1.5763 - val_acc: 0.6253
Epoch 20/500
229s - loss: 0.8971 - acc: 0.7489 - val_loss: 0.7874 - val_acc: 0.8095
Epoch 21/500
228s - loss: 0.7167 - acc: 0.7955 - val_loss: 0.7590 - val_acc: 0.8200
Epoch 22/500
228s - loss: 0.6871 - acc: 0.8016 - val_loss: 0.7088 - val_acc: 0.8242
Epoch 23/500
227s - loss: 0.6355 - acc: 0.8184 - val_loss: 0.7425 - val_acc: 0.8158
Epoch 24/500
227s - loss: 0.5727 - acc: 0.8245 - val_loss: 0.6990 - val_acc: 0.8274
Epoch 25/500
227s - loss: 0.5789 - acc: 0.8276 - val_loss: 0.6614 - val_acc: 0.8347
Epoch 26/500
227s - loss: 0.5357 - acc: 0.8392 - val_loss: 0.6825 - val_acc: 0.8326
Epoch 27/500
227s - loss: 0.5181 - acc: 0.8484 - val_loss: 0.6635 - val_acc: 0.8453
Epoch 28/500
227s - loss: 0.5090 - acc: 0.8437 - val_loss: 0.7061 - val_acc: 0.8316
Epoch 29/500
227s - loss: 0.4737 - acc: 0.8539 - val_loss: 0.6465 - val_acc: 0.8400
Epoch 30/500
228s - loss: 0.4739 - acc: 0.8479 - val_loss: 0.6493 - val_acc: 0.8453
Epoch 31/500
229s - loss: 0.4539 - acc: 0.8587 - val_loss: 0.6370 - val_acc: 0.8516
Epoch 32/500
227s - loss: 0.4607 - acc: 0.8526 - val_loss: 0.6691 - val_acc: 0.8347
Epoch 33/500
228s - loss: 0.4262 - acc: 0.8600 - val_loss: 0.6936 - val_acc: 0.8516
Epoch 34/500
230s - loss: 0.4554 - acc: 0.8608 - val_loss: 0.6718 - val_acc: 0.8495
Epoch 35/500
230s - loss: 0.4415 - acc: 0.8611 - val_loss: 0.6310 - val_acc: 0.8537
Epoch 36/500
227s - loss: 0.4028 - acc: 0.8684 - val_loss: 0.6960 - val_acc: 0.8463
Epoch 37/500
227s - loss: 0.4160 - acc: 0.8721 - val_loss: 0.6862 - val_acc: 0.8463
Epoch 38/500
229s - loss: 0.4157 - acc: 0.8687 - val_loss: 0.6343 - val_acc: 0.8526
Epoch 39/500
230s - loss: 0.3921 - acc: 0.8761 - val_loss: 0.6081 - val_acc: 0.8611
Epoch 40/500
228s - loss: 0.4123 - acc: 0.8684 - val_loss: 0.7394 - val_acc: 0.8337
Epoch 41/500
229s - loss: 0.3898 - acc: 0.8779 - val_loss: 0.7508 - val_acc: 0.8263
Epoch 42/500
229s - loss: 0.3937 - acc: 0.8734 - val_loss: 0.6251 - val_acc: 0.8537
Epoch 43/500
229s - loss: 0.3896 - acc: 0.8713 - val_loss: 0.6179 - val_acc: 0.8547
Epoch 44/500
229s - loss: 0.3702 - acc: 0.8787 - val_loss: 0.6160 - val_acc: 0.8642
Epoch 45/500
227s - loss: 0.3599 - acc: 0.8847 - val_loss: 0.7059 - val_acc: 0.8411
Epoch 46/500
227s - loss: 0.3341 - acc: 0.8847 - val_loss: 0.6405 - val_acc: 0.8579
Epoch 47/500
228s - loss: 0.3451 - acc: 0.8945 - val_loss: 0.5965 - val_acc: 0.8653
Epoch 48/500
227s - loss: 0.3795 - acc: 0.8834 - val_loss: 0.7149 - val_acc: 0.8474
Epoch 49/500
227s - loss: 0.3612 - acc: 0.8868 - val_loss: 0.6446 - val_acc: 0.8526
Epoch 50/500
226s - loss: 0.3427 - acc: 0.8874 - val_loss: 0.7351 - val_acc: 0.8516
Epoch 51/500
227s - loss: 0.3322 - acc: 0.8871 - val_loss: 0.6764 - val_acc: 0.8537
Epoch 52/500
228s - loss: 0.3568 - acc: 0.8924 - val_loss: 0.6402 - val_acc: 0.8516
Epoch 53/500
229s - loss: 0.3442 - acc: 0.8900 - val_loss: 0.6202 - val_acc: 0.8653
Epoch 54/500
228s - loss: 0.3192 - acc: 0.8929 - val_loss: 0.6245 - val_acc: 0.8663
Epoch 55/500
226s - loss: 0.3280 - acc: 0.8971 - val_loss: 0.6761 - val_acc: 0.8642
Epoch 56/500
228s - loss: 0.2903 - acc: 0.9005 - val_loss: 0.6496 - val_acc: 0.8579
Epoch 57/500
228s - loss: 0.3333 - acc: 0.9011 - val_loss: 0.6544 - val_acc: 0.8642
Epoch 58/500
228s - loss: 0.3191 - acc: 0.9034 - val_loss: 0.7433 - val_acc: 0.8453
Epoch 59/500
228s - loss: 0.2927 - acc: 0.9079 - val_loss: 0.6952 - val_acc: 0.8600
Epoch 60/500
228s - loss: 0.3035 - acc: 0.9103 - val_loss: 0.6043 - val_acc: 0.8674
Epoch 61/500
226s - loss: 0.2980 - acc: 0.9076 - val_loss: 0.7687 - val_acc: 0.8568
Epoch 62/500
226s - loss: 0.2983 - acc: 0.9034 - val_loss: 0.6791 - val_acc: 0.8632
Epoch 63/500
226s - loss: 0.2581 - acc: 0.9129 - val_loss: 0.6593 - val_acc: 0.8832
Epoch 64/500
227s - loss: 0.3130 - acc: 0.9071 - val_loss: 0.6618 - val_acc: 0.8684
Epoch 65/500
228s - loss: 0.3118 - acc: 0.9113 - val_loss: 0.6614 - val_acc: 0.8705
Epoch 66/500
228s - loss: 0.2811 - acc: 0.9074 - val_loss: 0.6778 - val_acc: 0.8663
Epoch 67/500
228s - loss: 0.2755 - acc: 0.9097 - val_loss: 0.7292 - val_acc: 0.8621
Epoch 68/500
229s - loss: 0.2653 - acc: 0.9158 - val_loss: 0.6647 - val_acc: 0.8789
Epoch 69/500
228s - loss: 0.2861 - acc: 0.9089 - val_loss: 0.6685 - val_acc: 0.8653
Epoch 70/500
228s - loss: 0.2911 - acc: 0.9113 - val_loss: 0.5891 - val_acc: 0.8674
Epoch 71/500
229s - loss: 0.2626 - acc: 0.9116 - val_loss: 0.6391 - val_acc: 0.8674
Epoch 72/500

Epoch 00071: reducing learning rate to 0.000999999977648.
229s - loss: 0.2418 - acc: 0.9179 - val_loss: 0.6652 - val_acc: 0.8716
Epoch 73/500
229s - loss: 0.2246 - acc: 0.9208 - val_loss: 0.6225 - val_acc: 0.8789
Epoch 74/500
229s - loss: 0.2182 - acc: 0.9287 - val_loss: 0.6130 - val_acc: 0.8789
Epoch 75/500
229s - loss: 0.2248 - acc: 0.9258 - val_loss: 0.6236 - val_acc: 0.8789
Epoch 76/500
228s - loss: 0.2228 - acc: 0.9242 - val_loss: 0.6080 - val_acc: 0.8779
Epoch 77/500
228s - loss: 0.2367 - acc: 0.9229 - val_loss: 0.6052 - val_acc: 0.8800
Epoch 78/500
228s - loss: 0.2283 - acc: 0.9245 - val_loss: 0.6040 - val_acc: 0.8789
Epoch 79/500
228s - loss: 0.2509 - acc: 0.9208 - val_loss: 0.6157 - val_acc: 0.8779
Epoch 80/500

Epoch 00079: reducing learning rate to 9.99999931082e-05.
228s - loss: 0.2283 - acc: 0.9226 - val_loss: 0.6215 - val_acc: 0.8758
Epoch 81/500
228s - loss: 0.2390 - acc: 0.9253 - val_loss: 0.6208 - val_acc: 0.8789
Epoch 82/500
229s - loss: 0.2548 - acc: 0.9245 - val_loss: 0.6196 - val_acc: 0.8800
Epoch 83/500
228s - loss: 0.2135 - acc: 0.9247 - val_loss: 0.6146 - val_acc: 0.8779
Epoch 84/500
229s - loss: 0.2140 - acc: 0.9326 - val_loss: 0.6167 - val_acc: 0.8800
Epoch 85/500
229s - loss: 0.2335 - acc: 0.9261 - val_loss: 0.6242 - val_acc: 0.8779
Epoch 86/500
229s - loss: 0.2105 - acc: 0.9289 - val_loss: 0.6185 - val_acc: 0.8800
Epoch 87/500
229s - loss: 0.2331 - acc: 0.9229 - val_loss: 0.6178 - val_acc: 0.8811
Epoch 88/500

Epoch 00087: reducing learning rate to 9.99999901978e-06.
228s - loss: 0.2397 - acc: 0.9237 - val_loss: 0.6077 - val_acc: 0.8800
Epoch 89/500
228s - loss: 0.2277 - acc: 0.9229 - val_loss: 0.6177 - val_acc: 0.8811
Epoch 90/500
228s - loss: 0.2251 - acc: 0.9263 - val_loss: 0.6177 - val_acc: 0.8800
Epoch 91/500
228s - loss: 0.2272 - acc: 0.9321 - val_loss: 0.6240 - val_acc: 0.8800
Epoch 92/500
228s - loss: 0.2248 - acc: 0.9279 - val_loss: 0.6226 - val_acc: 0.8789
Epoch 93/500
228s - loss: 0.2281 - acc: 0.9261 - val_loss: 0.6181 - val_acc: 0.8800
Epoch 94/500
228s - loss: 0.2275 - acc: 0.9218 - val_loss: 0.6163 - val_acc: 0.8800
Epoch 95/500
228s - loss: 0.2169 - acc: 0.9218 - val_loss: 0.6196 - val_acc: 0.8789
Epoch 96/500

Epoch 00095: reducing learning rate to 1e-06.
228s - loss: 0.2289 - acc: 0.9258 - val_loss: 0.6162 - val_acc: 0.8811
Epoch 97/500
229s - loss: 0.2282 - acc: 0.9287 - val_loss: 0.6145 - val_acc: 0.8811
Epoch 98/500
229s - loss: 0.2632 - acc: 0.9195 - val_loss: 0.6174 - val_acc: 0.8811
Epoch 99/500
228s - loss: 0.2445 - acc: 0.9284 - val_loss: 0.6171 - val_acc: 0.8821
Epoch 100/500
228s - loss: 0.2446 - acc: 0.9213 - val_loss: 0.6168 - val_acc: 0.8821
Epoch 101/500
228s - loss: 0.2288 - acc: 0.9263 - val_loss: 0.6170 - val_acc: 0.8821
Epoch 102/500
228s - loss: 0.2277 - acc: 0.9263 - val_loss: 0.6172 - val_acc: 0.8811
Epoch 103/500
229s - loss: 0.2459 - acc: 0.9253 - val_loss: 0.6227 - val_acc: 0.8811
Epoch 104/500
228s - loss: 0.2339 - acc: 0.9221 - val_loss: 0.6225 - val_acc: 0.8811
Epoch 105/500
228s - loss: 0.2259 - acc: 0.9208 - val_loss: 0.6146 - val_acc: 0.8811
Epoch 106/500
228s - loss: 0.2203 - acc: 0.9232 - val_loss: 0.6112 - val_acc: 0.8800
Epoch 107/500
229s - loss: 0.2350 - acc: 0.9234 - val_loss: 0.6125 - val_acc: 0.8800
Epoch 108/500
226s - loss: 0.2461 - acc: 0.9182 - val_loss: 0.6159 - val_acc: 0.8789
Epoch 109/500
227s - loss: 0.2383 - acc: 0.9242 - val_loss: 0.6202 - val_acc: 0.8811
Epoch 110/500
228s - loss: 0.2261 - acc: 0.9268 - val_loss: 0.6136 - val_acc: 0.8800
Epoch 111/500
229s - loss: 0.2213 - acc: 0.9263 - val_loss: 0.6112 - val_acc: 0.8821
Epoch 112/500
228s - loss: 0.2215 - acc: 0.9282 - val_loss: 0.6131 - val_acc: 0.8821
Epoch 113/500
229s - loss: 0.2281 - acc: 0.9274 - val_loss: 0.6178 - val_acc: 0.8832
Epoch 114/500
227s - loss: 0.2275 - acc: 0.9250 - val_loss: 0.6104 - val_acc: 0.8821
Epoch 115/500
229s - loss: 0.2343 - acc: 0.9268 - val_loss: 0.6171 - val_acc: 0.8800
Epoch 116/500
229s - loss: 0.2106 - acc: 0.9289 - val_loss: 0.6200 - val_acc: 0.8800
Epoch 117/500
229s - loss: 0.2363 - acc: 0.9218 - val_loss: 0.6185 - val_acc: 0.8832
Epoch 118/500
227s - loss: 0.2419 - acc: 0.9245 - val_loss: 0.6235 - val_acc: 0.8800
Epoch 119/500
226s - loss: 0.2086 - acc: 0.9250 - val_loss: 0.6180 - val_acc: 0.8800
Epoch 120/500
226s - loss: 0.2202 - acc: 0.9316 - val_loss: 0.6232 - val_acc: 0.8800
Epoch 121/500
226s - loss: 0.2330 - acc: 0.9268 - val_loss: 0.6211 - val_acc: 0.8811
Epoch 122/500
226s - loss: 0.2096 - acc: 0.9268 - val_loss: 0.6184 - val_acc: 0.8832
Epoch 123/500
226s - loss: 0.2204 - acc: 0.9255 - val_loss: 0.6155 - val_acc: 0.8821
Epoch 124/500
226s - loss: 0.2311 - acc: 0.9255 - val_loss: 0.6218 - val_acc: 0.8832
Epoch 125/500
226s - loss: 0.2133 - acc: 0.9300 - val_loss: 0.6160 - val_acc: 0.8811
Epoch 126/500
226s - loss: 0.2492 - acc: 0.9218 - val_loss: 0.6201 - val_acc: 0.8811
Epoch 127/500
226s - loss: 0.2134 - acc: 0.9271 - val_loss: 0.6201 - val_acc: 0.8811
Epoch 128/500
226s - loss: 0.2438 - acc: 0.9192 - val_loss: 0.6205 - val_acc: 0.8811
Epoch 129/500
226s - loss: 0.2360 - acc: 0.9263 - val_loss: 0.6165 - val_acc: 0.8800
Epoch 130/500
226s - loss: 0.2290 - acc: 0.9226 - val_loss: 0.6147 - val_acc: 0.8811
Epoch 131/500
226s - loss: 0.2093 - acc: 0.9308 - val_loss: 0.6090 - val_acc: 0.8821
Training loss for fold 1 is 0.09542406599380468 with percent 95.84210526315789
Testing loss for fold 1 is 0.6177591084179125 with percent 88.3157895238776
 
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_5 (InputLayer)             (None, 75, 75, 3)     0                                            
____________________________________________________________________________________________________
conv2d_13 (Conv2D)               (None, 73, 73, 16)    448         input_5[0][0]                    
____________________________________________________________________________________________________
batch_normalization_15 (BatchNor (None, 73, 73, 16)    64          conv2d_13[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_15 (LeakyReLU)       (None, 73, 73, 16)    0           batch_normalization_15[0][0]     
____________________________________________________________________________________________________
conv2d_14 (Conv2D)               (None, 72, 72, 16)    1040        leaky_re_lu_15[0][0]             
____________________________________________________________________________________________________
batch_normalization_16 (BatchNor (None, 72, 72, 16)    64          conv2d_14[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_16 (LeakyReLU)       (None, 72, 72, 16)    0           batch_normalization_16[0][0]     
____________________________________________________________________________________________________
max_pooling2d_7 (MaxPooling2D)   (None, 36, 36, 16)    0           leaky_re_lu_16[0][0]             
____________________________________________________________________________________________________
dropout_11 (Dropout)             (None, 36, 36, 16)    0           max_pooling2d_7[0][0]            
____________________________________________________________________________________________________
conv2d_15 (Conv2D)               (None, 34, 34, 32)    4640        dropout_11[0][0]                 
____________________________________________________________________________________________________
batch_normalization_17 (BatchNor (None, 34, 34, 32)    128         conv2d_15[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_17 (LeakyReLU)       (None, 34, 34, 32)    0           batch_normalization_17[0][0]     
____________________________________________________________________________________________________
conv2d_16 (Conv2D)               (None, 33, 33, 32)    4128        leaky_re_lu_17[0][0]             
____________________________________________________________________________________________________
batch_normalization_18 (BatchNor (None, 33, 33, 32)    128         conv2d_16[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_18 (LeakyReLU)       (None, 33, 33, 32)    0           batch_normalization_18[0][0]     
____________________________________________________________________________________________________
max_pooling2d_8 (MaxPooling2D)   (None, 16, 16, 32)    0           leaky_re_lu_18[0][0]             
____________________________________________________________________________________________________
dropout_12 (Dropout)             (None, 16, 16, 32)    0           max_pooling2d_8[0][0]            
____________________________________________________________________________________________________
conv2d_17 (Conv2D)               (None, 14, 14, 64)    18496       dropout_12[0][0]                 
____________________________________________________________________________________________________
batch_normalization_19 (BatchNor (None, 14, 14, 64)    256         conv2d_17[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_19 (LeakyReLU)       (None, 14, 14, 64)    0           batch_normalization_19[0][0]     
____________________________________________________________________________________________________
conv2d_18 (Conv2D)               (None, 13, 13, 64)    16448       leaky_re_lu_19[0][0]             
____________________________________________________________________________________________________
batch_normalization_20 (BatchNor (None, 13, 13, 64)    256         conv2d_18[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_20 (LeakyReLU)       (None, 13, 13, 64)    0           batch_normalization_20[0][0]     
____________________________________________________________________________________________________
max_pooling2d_9 (MaxPooling2D)   (None, 6, 6, 64)      0           leaky_re_lu_20[0][0]             
____________________________________________________________________________________________________
dropout_13 (Dropout)             (None, 6, 6, 64)      0           max_pooling2d_9[0][0]            
____________________________________________________________________________________________________
flatten_3 (Flatten)              (None, 2304)          0           dropout_13[0][0]                 
____________________________________________________________________________________________________
batch_normalization_21 (BatchNor (None, 2304)          9216        flatten_3[0][0]                  
____________________________________________________________________________________________________
input_6 (InputLayer)             (None, 102)           0                                            
____________________________________________________________________________________________________
concatenate_3 (Concatenate)      (None, 2406)          0           batch_normalization_21[0][0]     
                                                                   input_6[0][0]                    
____________________________________________________________________________________________________
dense_9 (Dense)                  (None, 128)           308096      concatenate_3[0][0]              
____________________________________________________________________________________________________
leaky_re_lu_21 (LeakyReLU)       (None, 128)           0           dense_9[0][0]                    
____________________________________________________________________________________________________
dense_10 (Dense)                 (None, 64)            8256        leaky_re_lu_21[0][0]             
____________________________________________________________________________________________________
dropout_14 (Dropout)             (None, 64)            0           dense_10[0][0]                   
____________________________________________________________________________________________________
dense_11 (Dense)                 (None, 32)            2080        dropout_14[0][0]                 
____________________________________________________________________________________________________
dropout_15 (Dropout)             (None, 32)            0           dense_11[0][0]                   
____________________________________________________________________________________________________
dense_12 (Dense)                 (None, 12)            396         dropout_15[0][0]                 
====================================================================================================
Total params: 374,140
Trainable params: 369,084
Non-trainable params: 5,056
____________________________________________________________________________________________________
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
232s - loss: 2.0817 - acc: 0.3074 - val_loss: 3.1848 - val_acc: 0.1400
Epoch 2/500
228s - loss: 1.6256 - acc: 0.4682 - val_loss: 1.8130 - val_acc: 0.3295
Epoch 3/500
228s - loss: 1.4254 - acc: 0.5447 - val_loss: 1.2489 - val_acc: 0.5842
Epoch 4/500
228s - loss: 1.3340 - acc: 0.5953 - val_loss: 2.5293 - val_acc: 0.2768
Epoch 5/500
226s - loss: 1.3091 - acc: 0.6055 - val_loss: 1.5714 - val_acc: 0.5400
Epoch 6/500
226s - loss: 1.2722 - acc: 0.6250 - val_loss: 2.4131 - val_acc: 0.3674
Epoch 7/500
226s - loss: 1.2321 - acc: 0.6321 - val_loss: 1.8501 - val_acc: 0.4600
Epoch 8/500
228s - loss: 1.2139 - acc: 0.6334 - val_loss: 1.2575 - val_acc: 0.5737
Epoch 9/500
229s - loss: 1.1560 - acc: 0.6637 - val_loss: 1.0731 - val_acc: 0.7284
Epoch 10/500
226s - loss: 1.1986 - acc: 0.6776 - val_loss: 1.8568 - val_acc: 0.5926
Epoch 11/500
226s - loss: 1.2535 - acc: 0.6600 - val_loss: 1.5365 - val_acc: 0.5989
Epoch 12/500
226s - loss: 1.2148 - acc: 0.6637 - val_loss: 2.6599 - val_acc: 0.5337
Epoch 13/500
228s - loss: 1.2125 - acc: 0.6763 - val_loss: 1.6091 - val_acc: 0.5442
Epoch 14/500
228s - loss: 1.1206 - acc: 0.7032 - val_loss: 1.1119 - val_acc: 0.7137
Epoch 15/500
228s - loss: 1.1858 - acc: 0.6821 - val_loss: 1.1301 - val_acc: 0.6832
Epoch 16/500
229s - loss: 1.1405 - acc: 0.7053 - val_loss: 3.6165 - val_acc: 0.2716
Epoch 17/500
228s - loss: 1.2162 - acc: 0.6816 - val_loss: 1.7927 - val_acc: 0.6232
Epoch 18/500

Epoch 00017: reducing learning rate to 0.010000000149.
229s - loss: 1.1252 - acc: 0.7037 - val_loss: 1.8644 - val_acc: 0.6126
Epoch 19/500
229s - loss: 1.0573 - acc: 0.7255 - val_loss: 1.0678 - val_acc: 0.7147
Epoch 20/500
229s - loss: 0.8607 - acc: 0.7682 - val_loss: 1.0230 - val_acc: 0.7305
Epoch 21/500
226s - loss: 0.8135 - acc: 0.7829 - val_loss: 0.8718 - val_acc: 0.7716
Epoch 22/500
226s - loss: 0.7796 - acc: 0.7861 - val_loss: 0.8698 - val_acc: 0.7779
Epoch 23/500
226s - loss: 0.7096 - acc: 0.8024 - val_loss: 0.8509 - val_acc: 0.7853
Epoch 24/500
226s - loss: 0.6851 - acc: 0.8050 - val_loss: 0.8279 - val_acc: 0.7747
Epoch 25/500
228s - loss: 0.6718 - acc: 0.8055 - val_loss: 0.8760 - val_acc: 0.7811
Epoch 26/500
229s - loss: 0.6623 - acc: 0.8100 - val_loss: 0.8582 - val_acc: 0.7832
Epoch 27/500
229s - loss: 0.6594 - acc: 0.8092 - val_loss: 0.8975 - val_acc: 0.7842
Epoch 28/500
229s - loss: 0.6658 - acc: 0.8150 - val_loss: 0.8525 - val_acc: 0.7811
Epoch 29/500
229s - loss: 0.6006 - acc: 0.8253 - val_loss: 0.7926 - val_acc: 0.8000
Epoch 30/500
226s - loss: 0.6427 - acc: 0.8155 - val_loss: 0.8817 - val_acc: 0.7874
Epoch 31/500
226s - loss: 0.6367 - acc: 0.8174 - val_loss: 0.7973 - val_acc: 0.7842
Epoch 32/500
226s - loss: 0.5742 - acc: 0.8253 - val_loss: 0.7512 - val_acc: 0.8032
Epoch 33/500
227s - loss: 0.6076 - acc: 0.8261 - val_loss: 0.7776 - val_acc: 0.7968
Epoch 34/500
227s - loss: 0.6109 - acc: 0.8221 - val_loss: 0.7248 - val_acc: 0.8063
Epoch 35/500
226s - loss: 0.5775 - acc: 0.8339 - val_loss: 0.7654 - val_acc: 0.8011
Epoch 36/500
226s - loss: 0.5535 - acc: 0.8300 - val_loss: 0.7961 - val_acc: 0.7874
Epoch 37/500
226s - loss: 0.5378 - acc: 0.8334 - val_loss: 0.7296 - val_acc: 0.8084
Epoch 38/500
227s - loss: 0.5360 - acc: 0.8358 - val_loss: 0.7632 - val_acc: 0.8084
Epoch 39/500
226s - loss: 0.5395 - acc: 0.8361 - val_loss: 0.8039 - val_acc: 0.8000
Epoch 40/500
226s - loss: 0.5221 - acc: 0.8397 - val_loss: 0.7390 - val_acc: 0.8105
Epoch 41/500
226s - loss: 0.5167 - acc: 0.8392 - val_loss: 0.7423 - val_acc: 0.8042
Epoch 42/500
226s - loss: 0.5338 - acc: 0.8389 - val_loss: 0.7230 - val_acc: 0.8095
Epoch 43/500
226s - loss: 0.5228 - acc: 0.8403 - val_loss: 0.7535 - val_acc: 0.8011
Epoch 44/500
226s - loss: 0.5262 - acc: 0.8371 - val_loss: 0.8125 - val_acc: 0.7895
Epoch 45/500
226s - loss: 0.5138 - acc: 0.8405 - val_loss: 0.7813 - val_acc: 0.8000
Epoch 46/500
226s - loss: 0.4885 - acc: 0.8450 - val_loss: 0.7572 - val_acc: 0.8095
Epoch 47/500
228s - loss: 0.5260 - acc: 0.8416 - val_loss: 0.7619 - val_acc: 0.8000
Epoch 48/500
228s - loss: 0.4967 - acc: 0.8432 - val_loss: 0.7595 - val_acc: 0.7979
Epoch 49/500

Epoch 00048: reducing learning rate to 0.000999999977648.
228s - loss: 0.4897 - acc: 0.8434 - val_loss: 0.7520 - val_acc: 0.8063
Epoch 50/500
228s - loss: 0.4804 - acc: 0.8468 - val_loss: 0.7340 - val_acc: 0.8053
Epoch 51/500
229s - loss: 0.4705 - acc: 0.8482 - val_loss: 0.7421 - val_acc: 0.8063
Epoch 52/500
228s - loss: 0.4803 - acc: 0.8468 - val_loss: 0.7435 - val_acc: 0.8042
Epoch 53/500
228s - loss: 0.4724 - acc: 0.8484 - val_loss: 0.7463 - val_acc: 0.8021
Epoch 54/500
228s - loss: 0.4944 - acc: 0.8432 - val_loss: 0.7474 - val_acc: 0.8042
Epoch 55/500
228s - loss: 0.4629 - acc: 0.8500 - val_loss: 0.7591 - val_acc: 0.7979
Epoch 56/500
228s - loss: 0.4538 - acc: 0.8518 - val_loss: 0.7477 - val_acc: 0.8063
Epoch 57/500

Epoch 00056: reducing learning rate to 9.99999931082e-05.
228s - loss: 0.4509 - acc: 0.8497 - val_loss: 0.7522 - val_acc: 0.8063
Epoch 58/500
228s - loss: 0.4628 - acc: 0.8526 - val_loss: 0.7514 - val_acc: 0.8053
Epoch 59/500
227s - loss: 0.4444 - acc: 0.8553 - val_loss: 0.7497 - val_acc: 0.8084
Epoch 60/500
228s - loss: 0.4629 - acc: 0.8534 - val_loss: 0.7472 - val_acc: 0.8084
Epoch 61/500
228s - loss: 0.4603 - acc: 0.8518 - val_loss: 0.7488 - val_acc: 0.8084
Epoch 62/500
228s - loss: 0.4574 - acc: 0.8516 - val_loss: 0.7491 - val_acc: 0.8053
Epoch 63/500
227s - loss: 0.4649 - acc: 0.8511 - val_loss: 0.7469 - val_acc: 0.8074
Epoch 64/500
226s - loss: 0.4713 - acc: 0.8511 - val_loss: 0.7443 - val_acc: 0.8105
Epoch 65/500

Epoch 00064: reducing learning rate to 9.99999901978e-06.
226s - loss: 0.4786 - acc: 0.8487 - val_loss: 0.7465 - val_acc: 0.8074
Epoch 66/500
226s - loss: 0.4850 - acc: 0.8476 - val_loss: 0.7407 - val_acc: 0.8095
Epoch 67/500
226s - loss: 0.4600 - acc: 0.8524 - val_loss: 0.7433 - val_acc: 0.8095
Epoch 68/500
226s - loss: 0.4483 - acc: 0.8511 - val_loss: 0.7440 - val_acc: 0.8084
Epoch 69/500
227s - loss: 0.4422 - acc: 0.8526 - val_loss: 0.7466 - val_acc: 0.8074
Epoch 70/500
228s - loss: 0.4495 - acc: 0.8511 - val_loss: 0.7438 - val_acc: 0.8084
Epoch 71/500
228s - loss: 0.4590 - acc: 0.8508 - val_loss: 0.7465 - val_acc: 0.8074
Epoch 72/500
227s - loss: 0.4756 - acc: 0.8487 - val_loss: 0.7465 - val_acc: 0.8084
Epoch 73/500

Epoch 00072: reducing learning rate to 1e-06.
227s - loss: 0.4589 - acc: 0.8526 - val_loss: 0.7490 - val_acc: 0.8074
Epoch 74/500
227s - loss: 0.4639 - acc: 0.8484 - val_loss: 0.7486 - val_acc: 0.8053
Epoch 75/500
228s - loss: 0.4583 - acc: 0.8516 - val_loss: 0.7490 - val_acc: 0.8074
Epoch 76/500
228s - loss: 0.4433 - acc: 0.8529 - val_loss: 0.7460 - val_acc: 0.8074
Epoch 77/500
228s - loss: 0.4799 - acc: 0.8495 - val_loss: 0.7439 - val_acc: 0.8084
Epoch 78/500
228s - loss: 0.4383 - acc: 0.8539 - val_loss: 0.7388 - val_acc: 0.8105
Epoch 79/500
228s - loss: 0.4558 - acc: 0.8526 - val_loss: 0.7367 - val_acc: 0.8095
Epoch 80/500
228s - loss: 0.4651 - acc: 0.8513 - val_loss: 0.7437 - val_acc: 0.8084
Epoch 81/500
228s - loss: 0.4645 - acc: 0.8526 - val_loss: 0.7454 - val_acc: 0.8084
Epoch 82/500
228s - loss: 0.4587 - acc: 0.8532 - val_loss: 0.7414 - val_acc: 0.8095
Epoch 83/500
228s - loss: 0.4741 - acc: 0.8497 - val_loss: 0.7466 - val_acc: 0.8063
Epoch 84/500
228s - loss: 0.4701 - acc: 0.8518 - val_loss: 0.7491 - val_acc: 0.8063
Epoch 85/500
228s - loss: 0.4476 - acc: 0.8529 - val_loss: 0.7455 - val_acc: 0.8074
Epoch 86/500
228s - loss: 0.4699 - acc: 0.8479 - val_loss: 0.7391 - val_acc: 0.8116
Epoch 87/500
226s - loss: 0.4706 - acc: 0.8463 - val_loss: 0.7449 - val_acc: 0.8095
Epoch 88/500
228s - loss: 0.4611 - acc: 0.8524 - val_loss: 0.7497 - val_acc: 0.8074
Epoch 89/500
228s - loss: 0.4552 - acc: 0.8492 - val_loss: 0.7460 - val_acc: 0.8074
Epoch 90/500
228s - loss: 0.4433 - acc: 0.8553 - val_loss: 0.7478 - val_acc: 0.8084
Epoch 91/500
228s - loss: 0.4768 - acc: 0.8492 - val_loss: 0.7424 - val_acc: 0.8095
Epoch 92/500
227s - loss: 0.4672 - acc: 0.8513 - val_loss: 0.7438 - val_acc: 0.8084
Epoch 93/500
225s - loss: 0.4774 - acc: 0.8526 - val_loss: 0.7536 - val_acc: 0.8063
Epoch 94/500
226s - loss: 0.4513 - acc: 0.8505 - val_loss: 0.7485 - val_acc: 0.8084
Epoch 95/500
227s - loss: 0.4678 - acc: 0.8497 - val_loss: 0.7458 - val_acc: 0.8084
Epoch 96/500
228s - loss: 0.4440 - acc: 0.8547 - val_loss: 0.7425 - val_acc: 0.8095
Epoch 97/500
228s - loss: 0.4490 - acc: 0.8524 - val_loss: 0.7510 - val_acc: 0.8074
Epoch 98/500
229s - loss: 0.4707 - acc: 0.8503 - val_loss: 0.7561 - val_acc: 0.8032
Epoch 99/500
229s - loss: 0.4373 - acc: 0.8537 - val_loss: 0.7500 - val_acc: 0.8063
Epoch 100/500
228s - loss: 0.4610 - acc: 0.8524 - val_loss: 0.7488 - val_acc: 0.8084
Epoch 101/500
226s - loss: 0.4435 - acc: 0.8539 - val_loss: 0.7520 - val_acc: 0.8074
Epoch 102/500
226s - loss: 0.4567 - acc: 0.8539 - val_loss: 0.7452 - val_acc: 0.8074
Epoch 103/500
226s - loss: 0.4705 - acc: 0.8497 - val_loss: 0.7439 - val_acc: 0.8084
Training loss for fold 2 is 0.32164157616464717 with percent 87.71052631578947
Testing loss for fold 2 is 0.7391309532993718 with percent 81.15789476193879
 
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_7 (InputLayer)             (None, 75, 75, 3)     0                                            
____________________________________________________________________________________________________
conv2d_19 (Conv2D)               (None, 73, 73, 16)    448         input_7[0][0]                    
____________________________________________________________________________________________________
batch_normalization_22 (BatchNor (None, 73, 73, 16)    64          conv2d_19[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_22 (LeakyReLU)       (None, 73, 73, 16)    0           batch_normalization_22[0][0]     
____________________________________________________________________________________________________
conv2d_20 (Conv2D)               (None, 72, 72, 16)    1040        leaky_re_lu_22[0][0]             
____________________________________________________________________________________________________
batch_normalization_23 (BatchNor (None, 72, 72, 16)    64          conv2d_20[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_23 (LeakyReLU)       (None, 72, 72, 16)    0           batch_normalization_23[0][0]     
____________________________________________________________________________________________________
max_pooling2d_10 (MaxPooling2D)  (None, 36, 36, 16)    0           leaky_re_lu_23[0][0]             
____________________________________________________________________________________________________
dropout_16 (Dropout)             (None, 36, 36, 16)    0           max_pooling2d_10[0][0]           
____________________________________________________________________________________________________
conv2d_21 (Conv2D)               (None, 34, 34, 32)    4640        dropout_16[0][0]                 
____________________________________________________________________________________________________
batch_normalization_24 (BatchNor (None, 34, 34, 32)    128         conv2d_21[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_24 (LeakyReLU)       (None, 34, 34, 32)    0           batch_normalization_24[0][0]     
____________________________________________________________________________________________________
conv2d_22 (Conv2D)               (None, 33, 33, 32)    4128        leaky_re_lu_24[0][0]             
____________________________________________________________________________________________________
batch_normalization_25 (BatchNor (None, 33, 33, 32)    128         conv2d_22[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_25 (LeakyReLU)       (None, 33, 33, 32)    0           batch_normalization_25[0][0]     
____________________________________________________________________________________________________
max_pooling2d_11 (MaxPooling2D)  (None, 16, 16, 32)    0           leaky_re_lu_25[0][0]             
____________________________________________________________________________________________________
dropout_17 (Dropout)             (None, 16, 16, 32)    0           max_pooling2d_11[0][0]           
____________________________________________________________________________________________________
conv2d_23 (Conv2D)               (None, 14, 14, 64)    18496       dropout_17[0][0]                 
____________________________________________________________________________________________________
batch_normalization_26 (BatchNor (None, 14, 14, 64)    256         conv2d_23[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_26 (LeakyReLU)       (None, 14, 14, 64)    0           batch_normalization_26[0][0]     
____________________________________________________________________________________________________
conv2d_24 (Conv2D)               (None, 13, 13, 64)    16448       leaky_re_lu_26[0][0]             
____________________________________________________________________________________________________
batch_normalization_27 (BatchNor (None, 13, 13, 64)    256         conv2d_24[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_27 (LeakyReLU)       (None, 13, 13, 64)    0           batch_normalization_27[0][0]     
____________________________________________________________________________________________________
max_pooling2d_12 (MaxPooling2D)  (None, 6, 6, 64)      0           leaky_re_lu_27[0][0]             
____________________________________________________________________________________________________
dropout_18 (Dropout)             (None, 6, 6, 64)      0           max_pooling2d_12[0][0]           
____________________________________________________________________________________________________
flatten_4 (Flatten)              (None, 2304)          0           dropout_18[0][0]                 
____________________________________________________________________________________________________
batch_normalization_28 (BatchNor (None, 2304)          9216        flatten_4[0][0]                  
____________________________________________________________________________________________________
input_8 (InputLayer)             (None, 102)           0                                            
____________________________________________________________________________________________________
concatenate_4 (Concatenate)      (None, 2406)          0           batch_normalization_28[0][0]     
                                                                   input_8[0][0]                    
____________________________________________________________________________________________________
dense_13 (Dense)                 (None, 128)           308096      concatenate_4[0][0]              
____________________________________________________________________________________________________
leaky_re_lu_28 (LeakyReLU)       (None, 128)           0           dense_13[0][0]                   
____________________________________________________________________________________________________
dense_14 (Dense)                 (None, 64)            8256        leaky_re_lu_28[0][0]             
____________________________________________________________________________________________________
dropout_19 (Dropout)             (None, 64)            0           dense_14[0][0]                   
____________________________________________________________________________________________________
dense_15 (Dense)                 (None, 32)            2080        dropout_19[0][0]                 
____________________________________________________________________________________________________
dropout_20 (Dropout)             (None, 32)            0           dense_15[0][0]                   
____________________________________________________________________________________________________
dense_16 (Dense)                 (None, 12)            396         dropout_20[0][0]                 
====================================================================================================
Total params: 374,140
Trainable params: 369,084
Non-trainable params: 5,056
____________________________________________________________________________________________________
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
232s - loss: 2.0592 - acc: 0.3050 - val_loss: 3.5972 - val_acc: 0.1768
Epoch 2/500
227s - loss: 1.6424 - acc: 0.4497 - val_loss: 2.7220 - val_acc: 0.4232
Epoch 3/500
228s - loss: 1.5307 - acc: 0.5024 - val_loss: 1.2135 - val_acc: 0.5495
Epoch 4/500
227s - loss: 1.4135 - acc: 0.5447 - val_loss: 1.6297 - val_acc: 0.4968
Epoch 5/500
228s - loss: 1.3599 - acc: 0.5687 - val_loss: 1.5671 - val_acc: 0.5611
Epoch 6/500
227s - loss: 1.3668 - acc: 0.5884 - val_loss: 1.7101 - val_acc: 0.5074
Epoch 7/500
227s - loss: 1.2971 - acc: 0.6053 - val_loss: 1.0657 - val_acc: 0.6779
Epoch 8/500
228s - loss: 1.2777 - acc: 0.6111 - val_loss: 1.4339 - val_acc: 0.6063
Epoch 9/500
229s - loss: 1.1982 - acc: 0.6471 - val_loss: 2.2292 - val_acc: 0.4979
Epoch 10/500
229s - loss: 1.1817 - acc: 0.6547 - val_loss: 1.0593 - val_acc: 0.6632
Epoch 11/500
229s - loss: 1.1124 - acc: 0.6668 - val_loss: 1.6791 - val_acc: 0.5505
Epoch 12/500
229s - loss: 1.0957 - acc: 0.6837 - val_loss: 1.7426 - val_acc: 0.5337
Epoch 13/500
229s - loss: 1.1017 - acc: 0.6926 - val_loss: 1.4923 - val_acc: 0.5937
Epoch 14/500
229s - loss: 1.1434 - acc: 0.6832 - val_loss: 3.4521 - val_acc: 0.1863
Epoch 15/500
230s - loss: 1.1084 - acc: 0.6853 - val_loss: 1.8602 - val_acc: 0.4705
Epoch 16/500

Epoch 00015: reducing learning rate to 0.010000000149.
230s - loss: 1.0885 - acc: 0.6953 - val_loss: 2.2455 - val_acc: 0.5421
Epoch 17/500
229s - loss: 0.8988 - acc: 0.7326 - val_loss: 0.8617 - val_acc: 0.7516
Epoch 18/500
227s - loss: 0.7507 - acc: 0.7732 - val_loss: 0.7489 - val_acc: 0.7926
Epoch 19/500
226s - loss: 0.6908 - acc: 0.8063 - val_loss: 0.7965 - val_acc: 0.8042
Epoch 20/500
226s - loss: 0.6358 - acc: 0.8126 - val_loss: 0.7142 - val_acc: 0.8105
Epoch 21/500
226s - loss: 0.6718 - acc: 0.8084 - val_loss: 0.6790 - val_acc: 0.8168
Epoch 22/500
226s - loss: 0.6185 - acc: 0.8139 - val_loss: 0.8084 - val_acc: 0.7968
Epoch 23/500
227s - loss: 0.5918 - acc: 0.8287 - val_loss: 0.6857 - val_acc: 0.8116
Epoch 24/500
228s - loss: 0.5421 - acc: 0.8405 - val_loss: 0.6746 - val_acc: 0.8263
Epoch 25/500
227s - loss: 0.5876 - acc: 0.8318 - val_loss: 0.7047 - val_acc: 0.8232
Epoch 26/500
228s - loss: 0.5356 - acc: 0.8453 - val_loss: 0.6295 - val_acc: 0.8358
Epoch 27/500
228s - loss: 0.5423 - acc: 0.8421 - val_loss: 0.7525 - val_acc: 0.8211
Epoch 28/500
228s - loss: 0.5294 - acc: 0.8447 - val_loss: 0.7096 - val_acc: 0.8168
Epoch 29/500
228s - loss: 0.4801 - acc: 0.8505 - val_loss: 0.5913 - val_acc: 0.8474
Epoch 30/500
226s - loss: 0.5242 - acc: 0.8476 - val_loss: 0.9063 - val_acc: 0.8032
Epoch 31/500
227s - loss: 0.5257 - acc: 0.8495 - val_loss: 0.5964 - val_acc: 0.8400
Epoch 32/500
228s - loss: 0.4901 - acc: 0.8561 - val_loss: 0.7404 - val_acc: 0.8358
Epoch 33/500
228s - loss: 0.4957 - acc: 0.8537 - val_loss: 0.5618 - val_acc: 0.8516
Epoch 34/500
226s - loss: 0.4680 - acc: 0.8624 - val_loss: 0.6001 - val_acc: 0.8421
Epoch 35/500
227s - loss: 0.4619 - acc: 0.8605 - val_loss: 0.5986 - val_acc: 0.8421
Epoch 36/500
228s - loss: 0.4859 - acc: 0.8539 - val_loss: 0.5870 - val_acc: 0.8463
Epoch 37/500
227s - loss: 0.4570 - acc: 0.8629 - val_loss: 0.5617 - val_acc: 0.8411
Epoch 38/500
228s - loss: 0.4482 - acc: 0.8637 - val_loss: 0.5746 - val_acc: 0.8526
Epoch 39/500
226s - loss: 0.4467 - acc: 0.8574 - val_loss: 0.5622 - val_acc: 0.8537
Epoch 40/500
226s - loss: 0.4309 - acc: 0.8703 - val_loss: 0.6058 - val_acc: 0.8526
Epoch 41/500
226s - loss: 0.4140 - acc: 0.8721 - val_loss: 0.5313 - val_acc: 0.8547
Epoch 42/500
226s - loss: 0.4290 - acc: 0.8742 - val_loss: 0.6222 - val_acc: 0.8421
Epoch 43/500
226s - loss: 0.3905 - acc: 0.8795 - val_loss: 0.5417 - val_acc: 0.8526
Epoch 44/500
226s - loss: 0.3972 - acc: 0.8779 - val_loss: 0.5950 - val_acc: 0.8453
Epoch 45/500
226s - loss: 0.3990 - acc: 0.8808 - val_loss: 0.5499 - val_acc: 0.8558
Epoch 46/500
226s - loss: 0.4218 - acc: 0.8721 - val_loss: 0.6195 - val_acc: 0.8516
Epoch 47/500
226s - loss: 0.3778 - acc: 0.8842 - val_loss: 0.5967 - val_acc: 0.8600
Epoch 48/500
225s - loss: 0.4060 - acc: 0.8761 - val_loss: 0.5484 - val_acc: 0.8484
Epoch 49/500
226s - loss: 0.3792 - acc: 0.8784 - val_loss: 0.5399 - val_acc: 0.8568
Epoch 50/500
226s - loss: 0.3893 - acc: 0.8813 - val_loss: 0.5089 - val_acc: 0.8705
Epoch 51/500
226s - loss: 0.3899 - acc: 0.8850 - val_loss: 0.5596 - val_acc: 0.8558
Epoch 52/500
227s - loss: 0.3651 - acc: 0.8874 - val_loss: 0.6049 - val_acc: 0.8453
Epoch 53/500
228s - loss: 0.3685 - acc: 0.8918 - val_loss: 0.5593 - val_acc: 0.8505
Epoch 54/500
228s - loss: 0.3461 - acc: 0.8858 - val_loss: 0.6702 - val_acc: 0.8168
Epoch 55/500
228s - loss: 0.3512 - acc: 0.8895 - val_loss: 0.5309 - val_acc: 0.8737
Epoch 56/500
226s - loss: 0.3306 - acc: 0.8924 - val_loss: 0.6624 - val_acc: 0.8568
Epoch 57/500
226s - loss: 0.3139 - acc: 0.8950 - val_loss: 0.5523 - val_acc: 0.8705
Epoch 58/500
226s - loss: 0.3642 - acc: 0.8863 - val_loss: 0.6981 - val_acc: 0.8474
Epoch 59/500
226s - loss: 0.3492 - acc: 0.8908 - val_loss: 0.5630 - val_acc: 0.8663
Epoch 60/500
228s - loss: 0.3526 - acc: 0.8939 - val_loss: 0.6174 - val_acc: 0.8495
Epoch 61/500
228s - loss: 0.3548 - acc: 0.8905 - val_loss: 0.5990 - val_acc: 0.8558
Epoch 62/500
228s - loss: 0.3484 - acc: 0.8974 - val_loss: 0.6215 - val_acc: 0.8453
Epoch 63/500
228s - loss: 0.3344 - acc: 0.8950 - val_loss: 0.5600 - val_acc: 0.8684
Epoch 64/500
228s - loss: 0.3403 - acc: 0.8987 - val_loss: 0.5359 - val_acc: 0.8821
Epoch 65/500
226s - loss: 0.3220 - acc: 0.9011 - val_loss: 0.5690 - val_acc: 0.8653
Epoch 66/500
228s - loss: 0.3352 - acc: 0.9050 - val_loss: 0.5951 - val_acc: 0.8516
Epoch 67/500
228s - loss: 0.3108 - acc: 0.9018 - val_loss: 0.5514 - val_acc: 0.8716
Epoch 68/500
229s - loss: 0.3469 - acc: 0.8995 - val_loss: 0.5153 - val_acc: 0.8779
Epoch 69/500
229s - loss: 0.2968 - acc: 0.9095 - val_loss: 0.5083 - val_acc: 0.8800
Epoch 70/500
229s - loss: 0.3332 - acc: 0.8992 - val_loss: 0.5699 - val_acc: 0.8621
Epoch 71/500
228s - loss: 0.3068 - acc: 0.9095 - val_loss: 0.5529 - val_acc: 0.8747
Epoch 72/500
229s - loss: 0.3108 - acc: 0.9068 - val_loss: 0.6876 - val_acc: 0.8400
Epoch 73/500

Epoch 00072: reducing learning rate to 0.000999999977648.
228s - loss: 0.2978 - acc: 0.9092 - val_loss: 0.5289 - val_acc: 0.8789
Epoch 74/500
228s - loss: 0.3138 - acc: 0.9055 - val_loss: 0.5214 - val_acc: 0.8779
Epoch 75/500
228s - loss: 0.2979 - acc: 0.9118 - val_loss: 0.5039 - val_acc: 0.8811
Epoch 76/500
228s - loss: 0.2997 - acc: 0.9118 - val_loss: 0.4951 - val_acc: 0.8842
Epoch 77/500
226s - loss: 0.2639 - acc: 0.9176 - val_loss: 0.4998 - val_acc: 0.8800
Epoch 78/500
226s - loss: 0.3142 - acc: 0.9050 - val_loss: 0.4915 - val_acc: 0.8821
Epoch 79/500
226s - loss: 0.2594 - acc: 0.9137 - val_loss: 0.4930 - val_acc: 0.8821
Epoch 80/500
227s - loss: 0.2701 - acc: 0.9124 - val_loss: 0.4869 - val_acc: 0.8821
Epoch 81/500
228s - loss: 0.2611 - acc: 0.9189 - val_loss: 0.4969 - val_acc: 0.8800
Epoch 82/500
228s - loss: 0.2736 - acc: 0.9137 - val_loss: 0.4906 - val_acc: 0.8832
Epoch 83/500
228s - loss: 0.2774 - acc: 0.9166 - val_loss: 0.4903 - val_acc: 0.8811
Epoch 84/500
228s - loss: 0.2624 - acc: 0.9174 - val_loss: 0.4898 - val_acc: 0.8832
Epoch 85/500

Epoch 00084: reducing learning rate to 9.99999931082e-05.
228s - loss: 0.2726 - acc: 0.9153 - val_loss: 0.4981 - val_acc: 0.8832
Epoch 86/500
229s - loss: 0.2598 - acc: 0.9168 - val_loss: 0.4934 - val_acc: 0.8853
Epoch 87/500
226s - loss: 0.2714 - acc: 0.9166 - val_loss: 0.4923 - val_acc: 0.8842
Epoch 88/500
226s - loss: 0.2754 - acc: 0.9142 - val_loss: 0.4916 - val_acc: 0.8832
Epoch 89/500
227s - loss: 0.2623 - acc: 0.9203 - val_loss: 0.4940 - val_acc: 0.8853
Epoch 90/500
229s - loss: 0.2520 - acc: 0.9253 - val_loss: 0.4937 - val_acc: 0.8853
Epoch 91/500
228s - loss: 0.3021 - acc: 0.9166 - val_loss: 0.4917 - val_acc: 0.8853
Epoch 92/500
227s - loss: 0.2872 - acc: 0.9145 - val_loss: 0.4909 - val_acc: 0.8853
Epoch 93/500
228s - loss: 0.2842 - acc: 0.9158 - val_loss: 0.4923 - val_acc: 0.8832
Epoch 94/500
229s - loss: 0.2641 - acc: 0.9195 - val_loss: 0.4892 - val_acc: 0.8853
Epoch 95/500

Epoch 00094: reducing learning rate to 9.99999901978e-06.
228s - loss: 0.2609 - acc: 0.9189 - val_loss: 0.4937 - val_acc: 0.8853
Epoch 96/500
228s - loss: 0.2863 - acc: 0.9184 - val_loss: 0.4936 - val_acc: 0.8842
Epoch 97/500
228s - loss: 0.2859 - acc: 0.9192 - val_loss: 0.4914 - val_acc: 0.8853
Epoch 98/500
229s - loss: 0.2418 - acc: 0.9232 - val_loss: 0.4911 - val_acc: 0.8853
Epoch 99/500
229s - loss: 0.2540 - acc: 0.9211 - val_loss: 0.4879 - val_acc: 0.8853
Epoch 100/500
228s - loss: 0.2552 - acc: 0.9226 - val_loss: 0.4901 - val_acc: 0.8874
Epoch 101/500
226s - loss: 0.2633 - acc: 0.9161 - val_loss: 0.4904 - val_acc: 0.8853
Epoch 102/500
227s - loss: 0.2739 - acc: 0.9189 - val_loss: 0.4920 - val_acc: 0.8853
Epoch 103/500
228s - loss: 0.2665 - acc: 0.9182 - val_loss: 0.4910 - val_acc: 0.8853
Epoch 104/500
228s - loss: 0.2774 - acc: 0.9147 - val_loss: 0.4879 - val_acc: 0.8853
Epoch 105/500
228s - loss: 0.2735 - acc: 0.9155 - val_loss: 0.4912 - val_acc: 0.8853
Epoch 106/500
228s - loss: 0.2614 - acc: 0.9168 - val_loss: 0.4912 - val_acc: 0.8842
Epoch 107/500
226s - loss: 0.2788 - acc: 0.9192 - val_loss: 0.4940 - val_acc: 0.8863
Epoch 108/500
226s - loss: 0.2623 - acc: 0.9226 - val_loss: 0.4906 - val_acc: 0.8863
Epoch 109/500

Epoch 00108: reducing learning rate to 1e-06.
226s - loss: 0.2556 - acc: 0.9247 - val_loss: 0.4928 - val_acc: 0.8842
Epoch 110/500
226s - loss: 0.2508 - acc: 0.9195 - val_loss: 0.4911 - val_acc: 0.8853
Epoch 111/500
226s - loss: 0.2758 - acc: 0.9108 - val_loss: 0.4929 - val_acc: 0.8853
Epoch 112/500
227s - loss: 0.2734 - acc: 0.9211 - val_loss: 0.4919 - val_acc: 0.8853
Epoch 113/500
227s - loss: 0.2860 - acc: 0.9111 - val_loss: 0.4885 - val_acc: 0.8853
Epoch 114/500
227s - loss: 0.2581 - acc: 0.9224 - val_loss: 0.4904 - val_acc: 0.8863
Epoch 115/500
228s - loss: 0.2498 - acc: 0.9237 - val_loss: 0.4950 - val_acc: 0.8842
Epoch 116/500
227s - loss: 0.2544 - acc: 0.9203 - val_loss: 0.4939 - val_acc: 0.8853
Epoch 117/500
226s - loss: 0.2740 - acc: 0.9134 - val_loss: 0.4929 - val_acc: 0.8863
Epoch 118/500
226s - loss: 0.2779 - acc: 0.9218 - val_loss: 0.4904 - val_acc: 0.8874
Epoch 119/500
226s - loss: 0.2633 - acc: 0.9224 - val_loss: 0.4881 - val_acc: 0.8853
Epoch 120/500
226s - loss: 0.2555 - acc: 0.9271 - val_loss: 0.4911 - val_acc: 0.8853
Epoch 121/500
226s - loss: 0.2590 - acc: 0.9229 - val_loss: 0.4892 - val_acc: 0.8853
Epoch 122/500
226s - loss: 0.2712 - acc: 0.9166 - val_loss: 0.4913 - val_acc: 0.8853
Epoch 123/500
226s - loss: 0.2774 - acc: 0.9197 - val_loss: 0.4921 - val_acc: 0.8863
Epoch 124/500
226s - loss: 0.2778 - acc: 0.9203 - val_loss: 0.4934 - val_acc: 0.8853
Epoch 125/500
226s - loss: 0.2519 - acc: 0.9187 - val_loss: 0.4908 - val_acc: 0.8853
Epoch 126/500
226s - loss: 0.2475 - acc: 0.9247 - val_loss: 0.4916 - val_acc: 0.8874
Epoch 127/500
226s - loss: 0.2769 - acc: 0.9171 - val_loss: 0.4904 - val_acc: 0.8853
Epoch 128/500
226s - loss: 0.2661 - acc: 0.9158 - val_loss: 0.4893 - val_acc: 0.8863
Epoch 129/500
226s - loss: 0.2536 - acc: 0.9211 - val_loss: 0.4924 - val_acc: 0.8842
Epoch 130/500
226s - loss: 0.2751 - acc: 0.9184 - val_loss: 0.4895 - val_acc: 0.8853
Epoch 131/500
226s - loss: 0.2745 - acc: 0.9187 - val_loss: 0.4909 - val_acc: 0.8853
Epoch 132/500
226s - loss: 0.2485 - acc: 0.9242 - val_loss: 0.4892 - val_acc: 0.8853
Epoch 133/500
226s - loss: 0.2587 - acc: 0.9208 - val_loss: 0.4913 - val_acc: 0.8853
Epoch 134/500
226s - loss: 0.2768 - acc: 0.9147 - val_loss: 0.4906 - val_acc: 0.8853
Epoch 135/500
226s - loss: 0.2734 - acc: 0.9176 - val_loss: 0.4910 - val_acc: 0.8853
Epoch 136/500
226s - loss: 0.2668 - acc: 0.9182 - val_loss: 0.4891 - val_acc: 0.8853
Epoch 137/500
226s - loss: 0.2606 - acc: 0.9229 - val_loss: 0.4897 - val_acc: 0.8853
Epoch 138/500
226s - loss: 0.2447 - acc: 0.9218 - val_loss: 0.4914 - val_acc: 0.8853
Epoch 139/500
226s - loss: 0.2389 - acc: 0.9245 - val_loss: 0.4910 - val_acc: 0.8863
Epoch 140/500
227s - loss: 0.2794 - acc: 0.9126 - val_loss: 0.4902 - val_acc: 0.8853
Epoch 141/500
227s - loss: 0.2393 - acc: 0.9229 - val_loss: 0.4905 - val_acc: 0.8863
Training loss for fold 3 is 0.13731587958963293 with percent 94.63157894736842
Testing loss for fold 3 is 0.490133952968999 with percent 88.73684209271481
 
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_9 (InputLayer)             (None, 75, 75, 3)     0                                            
____________________________________________________________________________________________________
conv2d_25 (Conv2D)               (None, 73, 73, 16)    448         input_9[0][0]                    
____________________________________________________________________________________________________
batch_normalization_29 (BatchNor (None, 73, 73, 16)    64          conv2d_25[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_29 (LeakyReLU)       (None, 73, 73, 16)    0           batch_normalization_29[0][0]     
____________________________________________________________________________________________________
conv2d_26 (Conv2D)               (None, 72, 72, 16)    1040        leaky_re_lu_29[0][0]             
____________________________________________________________________________________________________
batch_normalization_30 (BatchNor (None, 72, 72, 16)    64          conv2d_26[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_30 (LeakyReLU)       (None, 72, 72, 16)    0           batch_normalization_30[0][0]     
____________________________________________________________________________________________________
max_pooling2d_13 (MaxPooling2D)  (None, 36, 36, 16)    0           leaky_re_lu_30[0][0]             
____________________________________________________________________________________________________
dropout_21 (Dropout)             (None, 36, 36, 16)    0           max_pooling2d_13[0][0]           
____________________________________________________________________________________________________
conv2d_27 (Conv2D)               (None, 34, 34, 32)    4640        dropout_21[0][0]                 
____________________________________________________________________________________________________
batch_normalization_31 (BatchNor (None, 34, 34, 32)    128         conv2d_27[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_31 (LeakyReLU)       (None, 34, 34, 32)    0           batch_normalization_31[0][0]     
____________________________________________________________________________________________________
conv2d_28 (Conv2D)               (None, 33, 33, 32)    4128        leaky_re_lu_31[0][0]             
____________________________________________________________________________________________________
batch_normalization_32 (BatchNor (None, 33, 33, 32)    128         conv2d_28[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_32 (LeakyReLU)       (None, 33, 33, 32)    0           batch_normalization_32[0][0]     
____________________________________________________________________________________________________
max_pooling2d_14 (MaxPooling2D)  (None, 16, 16, 32)    0           leaky_re_lu_32[0][0]             
____________________________________________________________________________________________________
dropout_22 (Dropout)             (None, 16, 16, 32)    0           max_pooling2d_14[0][0]           
____________________________________________________________________________________________________
conv2d_29 (Conv2D)               (None, 14, 14, 64)    18496       dropout_22[0][0]                 
____________________________________________________________________________________________________
batch_normalization_33 (BatchNor (None, 14, 14, 64)    256         conv2d_29[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_33 (LeakyReLU)       (None, 14, 14, 64)    0           batch_normalization_33[0][0]     
____________________________________________________________________________________________________
conv2d_30 (Conv2D)               (None, 13, 13, 64)    16448       leaky_re_lu_33[0][0]             
____________________________________________________________________________________________________
batch_normalization_34 (BatchNor (None, 13, 13, 64)    256         conv2d_30[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_34 (LeakyReLU)       (None, 13, 13, 64)    0           batch_normalization_34[0][0]     
____________________________________________________________________________________________________
max_pooling2d_15 (MaxPooling2D)  (None, 6, 6, 64)      0           leaky_re_lu_34[0][0]             
____________________________________________________________________________________________________
dropout_23 (Dropout)             (None, 6, 6, 64)      0           max_pooling2d_15[0][0]           
____________________________________________________________________________________________________
flatten_5 (Flatten)              (None, 2304)          0           dropout_23[0][0]                 
____________________________________________________________________________________________________
batch_normalization_35 (BatchNor (None, 2304)          9216        flatten_5[0][0]                  
____________________________________________________________________________________________________
input_10 (InputLayer)            (None, 102)           0                                            
____________________________________________________________________________________________________
concatenate_5 (Concatenate)      (None, 2406)          0           batch_normalization_35[0][0]     
                                                                   input_10[0][0]                   
____________________________________________________________________________________________________
dense_17 (Dense)                 (None, 128)           308096      concatenate_5[0][0]              
____________________________________________________________________________________________________
leaky_re_lu_35 (LeakyReLU)       (None, 128)           0           dense_17[0][0]                   
____________________________________________________________________________________________________
dense_18 (Dense)                 (None, 64)            8256        leaky_re_lu_35[0][0]             
____________________________________________________________________________________________________
dropout_24 (Dropout)             (None, 64)            0           dense_18[0][0]                   
____________________________________________________________________________________________________
dense_19 (Dense)                 (None, 32)            2080        dropout_24[0][0]                 
____________________________________________________________________________________________________
dropout_25 (Dropout)             (None, 32)            0           dense_19[0][0]                   
____________________________________________________________________________________________________
dense_20 (Dense)                 (None, 12)            396         dropout_25[0][0]                 
====================================================================================================
Total params: 374,140
Trainable params: 369,084
Non-trainable params: 5,056
____________________________________________________________________________________________________
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
230s - loss: 2.0100 - acc: 0.3174 - val_loss: 1.7304 - val_acc: 0.3884
Epoch 2/500
226s - loss: 1.6092 - acc: 0.4711 - val_loss: 2.0919 - val_acc: 0.4042
Epoch 3/500
227s - loss: 1.4953 - acc: 0.5105 - val_loss: 1.7575 - val_acc: 0.5274
Epoch 4/500
226s - loss: 1.4421 - acc: 0.5484 - val_loss: 1.3346 - val_acc: 0.5642
Epoch 5/500
226s - loss: 1.3141 - acc: 0.5903 - val_loss: 1.7032 - val_acc: 0.4368
Epoch 6/500
228s - loss: 1.2340 - acc: 0.6321 - val_loss: 6.1313 - val_acc: 0.2116
Epoch 7/500
228s - loss: 1.1925 - acc: 0.6432 - val_loss: 1.1454 - val_acc: 0.6274
Epoch 8/500
226s - loss: 1.1673 - acc: 0.6695 - val_loss: 1.6418 - val_acc: 0.5126
Epoch 9/500
226s - loss: 1.1188 - acc: 0.6787 - val_loss: 2.0298 - val_acc: 0.4863
Epoch 10/500
226s - loss: 1.0748 - acc: 0.6955 - val_loss: 1.4112 - val_acc: 0.5958
Epoch 11/500
227s - loss: 1.0611 - acc: 0.7103 - val_loss: 1.2613 - val_acc: 0.7179
Epoch 12/500
226s - loss: 0.9875 - acc: 0.7268 - val_loss: 1.3980 - val_acc: 0.6705
Epoch 13/500
226s - loss: 1.0229 - acc: 0.7139 - val_loss: 0.9477 - val_acc: 0.7284
Epoch 14/500
226s - loss: 0.9277 - acc: 0.7468 - val_loss: 0.8976 - val_acc: 0.7211
Epoch 15/500
226s - loss: 0.9798 - acc: 0.7347 - val_loss: 1.6026 - val_acc: 0.5200
Epoch 16/500
228s - loss: 0.9685 - acc: 0.7395 - val_loss: 2.5237 - val_acc: 0.4663
Epoch 17/500
228s - loss: 0.9297 - acc: 0.7492 - val_loss: 1.5452 - val_acc: 0.6737
Epoch 18/500
228s - loss: 0.9453 - acc: 0.7461 - val_loss: 1.4790 - val_acc: 0.7253
Epoch 19/500
228s - loss: 0.9901 - acc: 0.7426 - val_loss: 1.8627 - val_acc: 0.6432
Epoch 20/500
228s - loss: 0.8993 - acc: 0.7618 - val_loss: 1.9362 - val_acc: 0.6189
Epoch 21/500
228s - loss: 0.9032 - acc: 0.7589 - val_loss: 2.8413 - val_acc: 0.4842
Epoch 22/500

Epoch 00021: reducing learning rate to 0.010000000149.
228s - loss: 0.9977 - acc: 0.7526 - val_loss: 1.6350 - val_acc: 0.7116
Epoch 23/500
228s - loss: 0.8276 - acc: 0.7916 - val_loss: 0.9113 - val_acc: 0.8021
Epoch 24/500
225s - loss: 0.6175 - acc: 0.8332 - val_loss: 0.8131 - val_acc: 0.8147
Epoch 25/500
225s - loss: 0.5781 - acc: 0.8432 - val_loss: 0.6692 - val_acc: 0.8411
Epoch 26/500
225s - loss: 0.5345 - acc: 0.8474 - val_loss: 0.6436 - val_acc: 0.8411
Epoch 27/500
226s - loss: 0.5057 - acc: 0.8563 - val_loss: 0.6528 - val_acc: 0.8400
Epoch 28/500
227s - loss: 0.5225 - acc: 0.8521 - val_loss: 0.6323 - val_acc: 0.8379
Epoch 29/500
227s - loss: 0.4873 - acc: 0.8566 - val_loss: 0.5834 - val_acc: 0.8484
Epoch 30/500
225s - loss: 0.4494 - acc: 0.8668 - val_loss: 0.5909 - val_acc: 0.8474
Epoch 31/500
225s - loss: 0.4208 - acc: 0.8666 - val_loss: 0.5174 - val_acc: 0.8621
Epoch 32/500
226s - loss: 0.4042 - acc: 0.8729 - val_loss: 0.5914 - val_acc: 0.8516
Epoch 33/500
226s - loss: 0.4038 - acc: 0.8721 - val_loss: 0.5551 - val_acc: 0.8547
Epoch 34/500
226s - loss: 0.3932 - acc: 0.8739 - val_loss: 0.5551 - val_acc: 0.8568
Epoch 35/500
227s - loss: 0.3901 - acc: 0.8779 - val_loss: 0.5619 - val_acc: 0.8537
Epoch 36/500
227s - loss: 0.3887 - acc: 0.8758 - val_loss: 0.5198 - val_acc: 0.8568
Epoch 37/500
227s - loss: 0.3709 - acc: 0.8784 - val_loss: 0.5475 - val_acc: 0.8621
Epoch 38/500
225s - loss: 0.3837 - acc: 0.8826 - val_loss: 0.6466 - val_acc: 0.8537
Epoch 39/500
226s - loss: 0.3709 - acc: 0.8858 - val_loss: 0.5125 - val_acc: 0.8695
Epoch 40/500
225s - loss: 0.3633 - acc: 0.8834 - val_loss: 0.5476 - val_acc: 0.8674
Epoch 41/500
227s - loss: 0.3548 - acc: 0.8824 - val_loss: 0.8057 - val_acc: 0.8347
Epoch 42/500
227s - loss: 0.3474 - acc: 0.8874 - val_loss: 0.5918 - val_acc: 0.8579
Epoch 43/500
228s - loss: 0.3539 - acc: 0.8858 - val_loss: 0.6007 - val_acc: 0.8463
Epoch 44/500
227s - loss: 0.3443 - acc: 0.8868 - val_loss: 0.5013 - val_acc: 0.8716
Epoch 45/500
226s - loss: 0.3326 - acc: 0.8932 - val_loss: 0.4843 - val_acc: 0.8716
Epoch 46/500
227s - loss: 0.3527 - acc: 0.8834 - val_loss: 0.6044 - val_acc: 0.8495
Epoch 47/500
227s - loss: 0.3317 - acc: 0.8853 - val_loss: 0.5225 - val_acc: 0.8684
Epoch 48/500
227s - loss: 0.3403 - acc: 0.8895 - val_loss: 0.5120 - val_acc: 0.8653
Epoch 49/500
