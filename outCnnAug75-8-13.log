Using TensorFlow backend.
/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
 
Did not find train data with correct size. Generating...
 
/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.
  warn("The default mode, 'constant', will be changed to 'reflect' in "
Done. Loading train images
 
 
Did not find test data with correct size. Generating...
 
Done. Loading test images
 
Augmentation data size (4750, 102) (794, 102)
/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)
(3800, 75, 75, 3) (3800,)
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_1 (InputLayer)             (None, 75, 75, 3)     0                                            
____________________________________________________________________________________________________
conv2d_1 (Conv2D)                (None, 73, 73, 16)    448         input_1[0][0]                    
____________________________________________________________________________________________________
batch_normalization_1 (BatchNorm (None, 73, 73, 16)    64          conv2d_1[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_1 (LeakyReLU)        (None, 73, 73, 16)    0           batch_normalization_1[0][0]      
____________________________________________________________________________________________________
conv2d_2 (Conv2D)                (None, 72, 72, 16)    1040        leaky_re_lu_1[0][0]              
____________________________________________________________________________________________________
batch_normalization_2 (BatchNorm (None, 72, 72, 16)    64          conv2d_2[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_2 (LeakyReLU)        (None, 72, 72, 16)    0           batch_normalization_2[0][0]      
____________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)   (None, 36, 36, 16)    0           leaky_re_lu_2[0][0]              
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 36, 36, 16)    0           max_pooling2d_1[0][0]            
____________________________________________________________________________________________________
conv2d_3 (Conv2D)                (None, 34, 34, 32)    4640        dropout_1[0][0]                  
____________________________________________________________________________________________________
batch_normalization_3 (BatchNorm (None, 34, 34, 32)    128         conv2d_3[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_3 (LeakyReLU)        (None, 34, 34, 32)    0           batch_normalization_3[0][0]      
____________________________________________________________________________________________________
conv2d_4 (Conv2D)                (None, 33, 33, 32)    4128        leaky_re_lu_3[0][0]              
____________________________________________________________________________________________________
batch_normalization_4 (BatchNorm (None, 33, 33, 32)    128         conv2d_4[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_4 (LeakyReLU)        (None, 33, 33, 32)    0           batch_normalization_4[0][0]      
____________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)   (None, 16, 16, 32)    0           leaky_re_lu_4[0][0]              
____________________________________________________________________________________________________
dropout_2 (Dropout)              (None, 16, 16, 32)    0           max_pooling2d_2[0][0]            
____________________________________________________________________________________________________
conv2d_5 (Conv2D)                (None, 14, 14, 64)    18496       dropout_2[0][0]                  
____________________________________________________________________________________________________
batch_normalization_5 (BatchNorm (None, 14, 14, 64)    256         conv2d_5[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_5 (LeakyReLU)        (None, 14, 14, 64)    0           batch_normalization_5[0][0]      
____________________________________________________________________________________________________
conv2d_6 (Conv2D)                (None, 13, 13, 64)    16448       leaky_re_lu_5[0][0]              
____________________________________________________________________________________________________
batch_normalization_6 (BatchNorm (None, 13, 13, 64)    256         conv2d_6[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_6 (LeakyReLU)        (None, 13, 13, 64)    0           batch_normalization_6[0][0]      
____________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)   (None, 6, 6, 64)      0           leaky_re_lu_6[0][0]              
____________________________________________________________________________________________________
dropout_3 (Dropout)              (None, 6, 6, 64)      0           max_pooling2d_3[0][0]            
____________________________________________________________________________________________________
flatten_1 (Flatten)              (None, 2304)          0           dropout_3[0][0]                  
____________________________________________________________________________________________________
batch_normalization_7 (BatchNorm (None, 2304)          9216        flatten_1[0][0]                  
____________________________________________________________________________________________________
input_2 (InputLayer)             (None, 102)           0                                            
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 2406)          0           batch_normalization_7[0][0]      
                                                                   input_2[0][0]                    
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 128)           308096      concatenate_1[0][0]              
____________________________________________________________________________________________________
leaky_re_lu_7 (LeakyReLU)        (None, 128)           0           dense_1[0][0]                    
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 64)            8256        leaky_re_lu_7[0][0]              
____________________________________________________________________________________________________
dropout_4 (Dropout)              (None, 64)            0           dense_2[0][0]                    
____________________________________________________________________________________________________
dense_3 (Dense)                  (None, 32)            2080        dropout_4[0][0]                  
____________________________________________________________________________________________________
dropout_5 (Dropout)              (None, 32)            0           dense_3[0][0]                    
____________________________________________________________________________________________________
dense_4 (Dense)                  (None, 12)            396         dropout_5[0][0]                  
====================================================================================================
Total params: 374,140
Trainable params: 369,084
Non-trainable params: 5,056
____________________________________________________________________________________________________
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
2018-08-13 23:35:24.244067: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-08-13 23:35:24.244425: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
233s - loss: 2.0333 - acc: 0.3103 - val_loss: 4.0931 - val_acc: 0.1411
Epoch 2/500
230s - loss: 1.5777 - acc: 0.4911 - val_loss: 3.6089 - val_acc: 0.2179
Epoch 3/500
231s - loss: 1.4371 - acc: 0.5442 - val_loss: 3.2747 - val_acc: 0.2768
Epoch 4/500
231s - loss: 1.3748 - acc: 0.5800 - val_loss: 1.2572 - val_acc: 0.6189
Epoch 5/500
231s - loss: 1.2927 - acc: 0.6026 - val_loss: 2.3330 - val_acc: 0.4126
Epoch 6/500
231s - loss: 1.2056 - acc: 0.6313 - val_loss: 2.1475 - val_acc: 0.3168
Epoch 7/500
232s - loss: 1.1687 - acc: 0.6413 - val_loss: 3.9445 - val_acc: 0.2968
Epoch 8/500
233s - loss: 1.1241 - acc: 0.6666 - val_loss: 2.3886 - val_acc: 0.4653
Epoch 9/500
232s - loss: 1.0784 - acc: 0.6861 - val_loss: 1.1356 - val_acc: 0.6989
Epoch 10/500
232s - loss: 1.0416 - acc: 0.6947 - val_loss: 1.4282 - val_acc: 0.5789
Epoch 11/500
232s - loss: 1.0254 - acc: 0.7037 - val_loss: 1.3909 - val_acc: 0.6105
Epoch 12/500
232s - loss: 0.9850 - acc: 0.7113 - val_loss: 1.8307 - val_acc: 0.5568
Epoch 13/500
230s - loss: 0.9807 - acc: 0.7237 - val_loss: 1.4668 - val_acc: 0.6926
Epoch 14/500
231s - loss: 0.9939 - acc: 0.7192 - val_loss: 1.3680 - val_acc: 0.6642
Epoch 15/500
231s - loss: 0.9504 - acc: 0.7434 - val_loss: 1.1173 - val_acc: 0.6989
Epoch 16/500
231s - loss: 0.9099 - acc: 0.7526 - val_loss: 1.0722 - val_acc: 0.7621
Epoch 17/500
230s - loss: 0.9286 - acc: 0.7574 - val_loss: 1.6660 - val_acc: 0.6674
Epoch 18/500
230s - loss: 0.9537 - acc: 0.7545 - val_loss: 0.9738 - val_acc: 0.7463
Epoch 19/500
230s - loss: 0.9609 - acc: 0.7611 - val_loss: 1.0820 - val_acc: 0.7674
Epoch 20/500
232s - loss: 0.8879 - acc: 0.7695 - val_loss: 1.4724 - val_acc: 0.7137
Epoch 21/500
231s - loss: 0.8947 - acc: 0.7676 - val_loss: 1.3205 - val_acc: 0.7158
Epoch 22/500
230s - loss: 0.9483 - acc: 0.7561 - val_loss: 1.2055 - val_acc: 0.7126
Epoch 23/500
229s - loss: 0.8658 - acc: 0.7755 - val_loss: 1.0101 - val_acc: 0.7232
Epoch 24/500
230s - loss: 0.8661 - acc: 0.7958 - val_loss: 3.6904 - val_acc: 0.4189
Epoch 25/500
229s - loss: 0.9220 - acc: 0.7766 - val_loss: 1.5236 - val_acc: 0.6947
Epoch 26/500
232s - loss: 0.9587 - acc: 0.7747 - val_loss: 1.7909 - val_acc: 0.6474
Epoch 27/500
232s - loss: 0.9773 - acc: 0.7711 - val_loss: 2.5696 - val_acc: 0.5316
Epoch 28/500

Epoch 00027: reducing learning rate to 0.010000000149.
232s - loss: 0.9135 - acc: 0.7776 - val_loss: 1.1193 - val_acc: 0.7484
Epoch 29/500
232s - loss: 0.8168 - acc: 0.7953 - val_loss: 0.8183 - val_acc: 0.8200
Epoch 30/500
233s - loss: 0.6261 - acc: 0.8287 - val_loss: 0.8249 - val_acc: 0.8274
Epoch 31/500
234s - loss: 0.5752 - acc: 0.8389 - val_loss: 0.7726 - val_acc: 0.8389
Epoch 32/500
234s - loss: 0.5550 - acc: 0.8492 - val_loss: 0.8457 - val_acc: 0.8358
Epoch 33/500
232s - loss: 0.5485 - acc: 0.8500 - val_loss: 0.8424 - val_acc: 0.8211
Epoch 34/500
230s - loss: 0.4787 - acc: 0.8589 - val_loss: 0.7693 - val_acc: 0.8453
Epoch 35/500
231s - loss: 0.5101 - acc: 0.8511 - val_loss: 0.7613 - val_acc: 0.8411
Epoch 36/500
232s - loss: 0.4792 - acc: 0.8616 - val_loss: 0.7655 - val_acc: 0.8516
Epoch 37/500
232s - loss: 0.5170 - acc: 0.8545 - val_loss: 0.7478 - val_acc: 0.8526
Epoch 38/500
231s - loss: 0.4610 - acc: 0.8629 - val_loss: 0.7865 - val_acc: 0.8411
Epoch 39/500
233s - loss: 0.4601 - acc: 0.8663 - val_loss: 0.7467 - val_acc: 0.8484
Epoch 40/500
232s - loss: 0.4496 - acc: 0.8705 - val_loss: 0.7494 - val_acc: 0.8411
Epoch 41/500
231s - loss: 0.4443 - acc: 0.8711 - val_loss: 0.7526 - val_acc: 0.8463
Epoch 42/500
230s - loss: 0.4395 - acc: 0.8745 - val_loss: 0.6990 - val_acc: 0.8589
Epoch 43/500
232s - loss: 0.4283 - acc: 0.8776 - val_loss: 0.8338 - val_acc: 0.8295
Epoch 44/500
232s - loss: 0.4024 - acc: 0.8755 - val_loss: 0.7081 - val_acc: 0.8632
Epoch 45/500
232s - loss: 0.4095 - acc: 0.8834 - val_loss: 0.7266 - val_acc: 0.8516
Epoch 46/500
230s - loss: 0.3847 - acc: 0.8855 - val_loss: 0.7078 - val_acc: 0.8611
Epoch 47/500
230s - loss: 0.4037 - acc: 0.8842 - val_loss: 0.7396 - val_acc: 0.8579
Epoch 48/500
230s - loss: 0.3388 - acc: 0.8945 - val_loss: 0.7117 - val_acc: 0.8632
Epoch 49/500
234s - loss: 0.3734 - acc: 0.8905 - val_loss: 0.8312 - val_acc: 0.8453
Epoch 50/500
231s - loss: 0.3791 - acc: 0.8863 - val_loss: 0.8050 - val_acc: 0.8453
Epoch 51/500
230s - loss: 0.3734 - acc: 0.8884 - val_loss: 0.6936 - val_acc: 0.8568
Epoch 52/500
230s - loss: 0.3542 - acc: 0.8900 - val_loss: 0.7285 - val_acc: 0.8611
Epoch 53/500

Epoch 00052: reducing learning rate to 0.000999999977648.
230s - loss: 0.3751 - acc: 0.8929 - val_loss: 0.7859 - val_acc: 0.8547
Epoch 54/500
230s - loss: 0.3340 - acc: 0.8971 - val_loss: 0.7430 - val_acc: 0.8621
Epoch 55/500
230s - loss: 0.3648 - acc: 0.8908 - val_loss: 0.7451 - val_acc: 0.8611
Epoch 56/500
230s - loss: 0.3455 - acc: 0.8971 - val_loss: 0.7447 - val_acc: 0.8589
Epoch 57/500
230s - loss: 0.3587 - acc: 0.8913 - val_loss: 0.7563 - val_acc: 0.8600
Epoch 58/500
230s - loss: 0.3509 - acc: 0.8921 - val_loss: 0.7354 - val_acc: 0.8632
Epoch 59/500
230s - loss: 0.3461 - acc: 0.8976 - val_loss: 0.7265 - val_acc: 0.8642
Epoch 60/500
232s - loss: 0.3242 - acc: 0.8953 - val_loss: 0.7239 - val_acc: 0.8621
Epoch 61/500
233s - loss: 0.3165 - acc: 0.8963 - val_loss: 0.7162 - val_acc: 0.8663
Epoch 62/500
232s - loss: 0.3254 - acc: 0.9021 - val_loss: 0.7151 - val_acc: 0.8663
Epoch 63/500
230s - loss: 0.3408 - acc: 0.8932 - val_loss: 0.7144 - val_acc: 0.8663
Epoch 64/500
230s - loss: 0.3298 - acc: 0.9013 - val_loss: 0.7104 - val_acc: 0.8653
Epoch 65/500
230s - loss: 0.3573 - acc: 0.9008 - val_loss: 0.7105 - val_acc: 0.8674
Epoch 66/500
221s - loss: 0.3486 - acc: 0.8947 - val_loss: 0.7093 - val_acc: 0.8695
Epoch 67/500
222s - loss: 0.3021 - acc: 0.9003 - val_loss: 0.7013 - val_acc: 0.8674
Epoch 68/500
220s - loss: 0.3287 - acc: 0.8987 - val_loss: 0.6906 - val_acc: 0.8674
Epoch 69/500
221s - loss: 0.3206 - acc: 0.8995 - val_loss: 0.7053 - val_acc: 0.8674
Epoch 70/500
219s - loss: 0.3377 - acc: 0.8934 - val_loss: 0.7113 - val_acc: 0.8674
Epoch 71/500
217s - loss: 0.3282 - acc: 0.8995 - val_loss: 0.6971 - val_acc: 0.8684
Epoch 72/500
217s - loss: 0.3168 - acc: 0.9000 - val_loss: 0.7145 - val_acc: 0.8663
Epoch 73/500
216s - loss: 0.3139 - acc: 0.9024 - val_loss: 0.7108 - val_acc: 0.8695
Epoch 74/500
217s - loss: 0.3010 - acc: 0.8974 - val_loss: 0.6954 - val_acc: 0.8684
Epoch 75/500

Epoch 00074: reducing learning rate to 9.99999931082e-05.
217s - loss: 0.3374 - acc: 0.9021 - val_loss: 0.7083 - val_acc: 0.8684
Epoch 76/500
217s - loss: 0.3387 - acc: 0.8961 - val_loss: 0.7030 - val_acc: 0.8705
Epoch 77/500
221s - loss: 0.3292 - acc: 0.8966 - val_loss: 0.7030 - val_acc: 0.8705
Epoch 78/500
218s - loss: 0.3153 - acc: 0.8987 - val_loss: 0.7023 - val_acc: 0.8695
Epoch 79/500
217s - loss: 0.3374 - acc: 0.9021 - val_loss: 0.7034 - val_acc: 0.8695
Epoch 80/500
217s - loss: 0.2945 - acc: 0.9018 - val_loss: 0.7091 - val_acc: 0.8705
Epoch 81/500
217s - loss: 0.3138 - acc: 0.9011 - val_loss: 0.7088 - val_acc: 0.8684
Epoch 82/500
217s - loss: 0.3475 - acc: 0.8974 - val_loss: 0.7020 - val_acc: 0.8695
Epoch 83/500
219s - loss: 0.3059 - acc: 0.9024 - val_loss: 0.7060 - val_acc: 0.8695
Epoch 84/500
216s - loss: 0.3444 - acc: 0.8982 - val_loss: 0.7054 - val_acc: 0.8695
Epoch 85/500

Epoch 00084: reducing learning rate to 9.99999901978e-06.
217s - loss: 0.3158 - acc: 0.9005 - val_loss: 0.7042 - val_acc: 0.8705
Epoch 86/500
216s - loss: 0.3203 - acc: 0.8984 - val_loss: 0.7067 - val_acc: 0.8705
Epoch 87/500
217s - loss: 0.3392 - acc: 0.8974 - val_loss: 0.7043 - val_acc: 0.8695
Epoch 88/500
217s - loss: 0.3103 - acc: 0.8989 - val_loss: 0.7036 - val_acc: 0.8705
Epoch 89/500
216s - loss: 0.3233 - acc: 0.9016 - val_loss: 0.7007 - val_acc: 0.8705
Epoch 90/500
217s - loss: 0.3008 - acc: 0.9066 - val_loss: 0.7049 - val_acc: 0.8695
Epoch 91/500
219s - loss: 0.3228 - acc: 0.8982 - val_loss: 0.6986 - val_acc: 0.8695
Epoch 92/500
217s - loss: 0.3554 - acc: 0.9003 - val_loss: 0.7032 - val_acc: 0.8695
Epoch 93/500

Epoch 00092: reducing learning rate to 1e-06.
215s - loss: 0.3261 - acc: 0.8997 - val_loss: 0.7102 - val_acc: 0.8705
Epoch 94/500
216s - loss: 0.3551 - acc: 0.8942 - val_loss: 0.7109 - val_acc: 0.8684
Epoch 95/500
217s - loss: 0.3485 - acc: 0.8950 - val_loss: 0.7016 - val_acc: 0.8705
Epoch 96/500
216s - loss: 0.3140 - acc: 0.9021 - val_loss: 0.7033 - val_acc: 0.8695
Epoch 97/500
216s - loss: 0.3179 - acc: 0.8997 - val_loss: 0.6999 - val_acc: 0.8695
Epoch 98/500
215s - loss: 0.3272 - acc: 0.8976 - val_loss: 0.7029 - val_acc: 0.8695
Epoch 99/500
218s - loss: 0.3174 - acc: 0.8997 - val_loss: 0.7066 - val_acc: 0.8684
Epoch 100/500
217s - loss: 0.3158 - acc: 0.9024 - val_loss: 0.7054 - val_acc: 0.8705
Epoch 101/500
217s - loss: 0.3218 - acc: 0.9024 - val_loss: 0.7098 - val_acc: 0.8705
Epoch 102/500
218s - loss: 0.3513 - acc: 0.8974 - val_loss: 0.7092 - val_acc: 0.8695
Epoch 103/500
217s - loss: 0.3152 - acc: 0.9021 - val_loss: 0.7050 - val_acc: 0.8705
Epoch 104/500
217s - loss: 0.3338 - acc: 0.8982 - val_loss: 0.7021 - val_acc: 0.8705
Epoch 105/500
217s - loss: 0.3018 - acc: 0.9008 - val_loss: 0.7055 - val_acc: 0.8705
Epoch 106/500
217s - loss: 0.3202 - acc: 0.9003 - val_loss: 0.7108 - val_acc: 0.8695
Epoch 107/500
217s - loss: 0.3300 - acc: 0.8984 - val_loss: 0.7036 - val_acc: 0.8705
Epoch 108/500
217s - loss: 0.2985 - acc: 0.9039 - val_loss: 0.7032 - val_acc: 0.8705
Epoch 109/500
218s - loss: 0.3276 - acc: 0.9008 - val_loss: 0.7057 - val_acc: 0.8705
Epoch 110/500
216s - loss: 0.3309 - acc: 0.8921 - val_loss: 0.7057 - val_acc: 0.8684
Epoch 111/500
217s - loss: 0.3279 - acc: 0.9008 - val_loss: 0.7075 - val_acc: 0.8674
Epoch 112/500
216s - loss: 0.3169 - acc: 0.9047 - val_loss: 0.7066 - val_acc: 0.8695
Epoch 113/500
216s - loss: 0.3149 - acc: 0.9005 - val_loss: 0.7068 - val_acc: 0.8705
Epoch 114/500
216s - loss: 0.3148 - acc: 0.9003 - val_loss: 0.7062 - val_acc: 0.8705
Epoch 115/500
216s - loss: 0.3311 - acc: 0.8995 - val_loss: 0.7063 - val_acc: 0.8705
Epoch 116/500
216s - loss: 0.3131 - acc: 0.9000 - val_loss: 0.6984 - val_acc: 0.8705
Epoch 117/500
215s - loss: 0.3157 - acc: 0.9047 - val_loss: 0.7071 - val_acc: 0.8695
Epoch 118/500
217s - loss: 0.3282 - acc: 0.9003 - val_loss: 0.7001 - val_acc: 0.8695
Epoch 119/500
216s - loss: 0.3134 - acc: 0.8987 - val_loss: 0.7051 - val_acc: 0.8705
Epoch 120/500
216s - loss: 0.3295 - acc: 0.8987 - val_loss: 0.7079 - val_acc: 0.8705
Epoch 121/500
218s - loss: 0.3197 - acc: 0.9042 - val_loss: 0.7048 - val_acc: 0.8684
Epoch 122/500
217s - loss: 0.3234 - acc: 0.8995 - val_loss: 0.7079 - val_acc: 0.8705
Epoch 123/500
216s - loss: 0.3186 - acc: 0.9003 - val_loss: 0.7077 - val_acc: 0.8705
Epoch 124/500
215s - loss: 0.3176 - acc: 0.9000 - val_loss: 0.7047 - val_acc: 0.8695
Epoch 125/500
217s - loss: 0.3567 - acc: 0.9008 - val_loss: 0.7052 - val_acc: 0.8684
Epoch 126/500
216s - loss: 0.3226 - acc: 0.9024 - val_loss: 0.7072 - val_acc: 0.8684
Epoch 127/500
216s - loss: 0.3280 - acc: 0.8976 - val_loss: 0.7011 - val_acc: 0.8695
Epoch 128/500
216s - loss: 0.3116 - acc: 0.9003 - val_loss: 0.7057 - val_acc: 0.8705
Epoch 129/500
216s - loss: 0.3113 - acc: 0.9061 - val_loss: 0.7025 - val_acc: 0.8705
Training loss for fold 0 is 0.19100658181319505 with percent 92.9210526190306
Testing loss for fold 0 is 0.7029581609525178 with percent 87.05263152875399
 
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_3 (InputLayer)             (None, 75, 75, 3)     0                                            
____________________________________________________________________________________________________
conv2d_7 (Conv2D)                (None, 73, 73, 16)    448         input_3[0][0]                    
____________________________________________________________________________________________________
batch_normalization_8 (BatchNorm (None, 73, 73, 16)    64          conv2d_7[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_8 (LeakyReLU)        (None, 73, 73, 16)    0           batch_normalization_8[0][0]      
____________________________________________________________________________________________________
conv2d_8 (Conv2D)                (None, 72, 72, 16)    1040        leaky_re_lu_8[0][0]              
____________________________________________________________________________________________________
batch_normalization_9 (BatchNorm (None, 72, 72, 16)    64          conv2d_8[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_9 (LeakyReLU)        (None, 72, 72, 16)    0           batch_normalization_9[0][0]      
____________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)   (None, 36, 36, 16)    0           leaky_re_lu_9[0][0]              
____________________________________________________________________________________________________
dropout_6 (Dropout)              (None, 36, 36, 16)    0           max_pooling2d_4[0][0]            
____________________________________________________________________________________________________
conv2d_9 (Conv2D)                (None, 34, 34, 32)    4640        dropout_6[0][0]                  
____________________________________________________________________________________________________
batch_normalization_10 (BatchNor (None, 34, 34, 32)    128         conv2d_9[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_10 (LeakyReLU)       (None, 34, 34, 32)    0           batch_normalization_10[0][0]     
____________________________________________________________________________________________________
conv2d_10 (Conv2D)               (None, 33, 33, 32)    4128        leaky_re_lu_10[0][0]             
____________________________________________________________________________________________________
batch_normalization_11 (BatchNor (None, 33, 33, 32)    128         conv2d_10[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_11 (LeakyReLU)       (None, 33, 33, 32)    0           batch_normalization_11[0][0]     
____________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)   (None, 16, 16, 32)    0           leaky_re_lu_11[0][0]             
____________________________________________________________________________________________________
dropout_7 (Dropout)              (None, 16, 16, 32)    0           max_pooling2d_5[0][0]            
____________________________________________________________________________________________________
conv2d_11 (Conv2D)               (None, 14, 14, 64)    18496       dropout_7[0][0]                  
____________________________________________________________________________________________________
batch_normalization_12 (BatchNor (None, 14, 14, 64)    256         conv2d_11[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_12 (LeakyReLU)       (None, 14, 14, 64)    0           batch_normalization_12[0][0]     
____________________________________________________________________________________________________
conv2d_12 (Conv2D)               (None, 13, 13, 64)    16448       leaky_re_lu_12[0][0]             
____________________________________________________________________________________________________
batch_normalization_13 (BatchNor (None, 13, 13, 64)    256         conv2d_12[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_13 (LeakyReLU)       (None, 13, 13, 64)    0           batch_normalization_13[0][0]     
____________________________________________________________________________________________________
max_pooling2d_6 (MaxPooling2D)   (None, 6, 6, 64)      0           leaky_re_lu_13[0][0]             
____________________________________________________________________________________________________
dropout_8 (Dropout)              (None, 6, 6, 64)      0           max_pooling2d_6[0][0]            
____________________________________________________________________________________________________
flatten_2 (Flatten)              (None, 2304)          0           dropout_8[0][0]                  
____________________________________________________________________________________________________
batch_normalization_14 (BatchNor (None, 2304)          9216        flatten_2[0][0]                  
____________________________________________________________________________________________________
input_4 (InputLayer)             (None, 102)           0                                            
____________________________________________________________________________________________________
concatenate_2 (Concatenate)      (None, 2406)          0           batch_normalization_14[0][0]     
                                                                   input_4[0][0]                    
____________________________________________________________________________________________________
dense_5 (Dense)                  (None, 128)           308096      concatenate_2[0][0]              
____________________________________________________________________________________________________
leaky_re_lu_14 (LeakyReLU)       (None, 128)           0           dense_5[0][0]                    
____________________________________________________________________________________________________
dense_6 (Dense)                  (None, 64)            8256        leaky_re_lu_14[0][0]             
____________________________________________________________________________________________________
dropout_9 (Dropout)              (None, 64)            0           dense_6[0][0]                    
____________________________________________________________________________________________________
dense_7 (Dense)                  (None, 32)            2080        dropout_9[0][0]                  
____________________________________________________________________________________________________
dropout_10 (Dropout)             (None, 32)            0           dense_7[0][0]                    
____________________________________________________________________________________________________
dense_8 (Dense)                  (None, 12)            396         dropout_10[0][0]                 
====================================================================================================
Total params: 374,140
Trainable params: 369,084
Non-trainable params: 5,056
____________________________________________________________________________________________________
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
222s - loss: 2.0718 - acc: 0.2903 - val_loss: 1.9292 - val_acc: 0.3389
Epoch 2/500
221s - loss: 1.6106 - acc: 0.4574 - val_loss: 2.6756 - val_acc: 0.2853
Epoch 3/500
227s - loss: 1.4818 - acc: 0.5208 - val_loss: 2.2756 - val_acc: 0.4221
Epoch 4/500
227s - loss: 1.3390 - acc: 0.5705 - val_loss: 2.1540 - val_acc: 0.4158
Epoch 5/500
227s - loss: 1.2830 - acc: 0.6168 - val_loss: 3.0780 - val_acc: 0.4305
Epoch 6/500
227s - loss: 1.2511 - acc: 0.6266 - val_loss: 1.9716 - val_acc: 0.5105
Epoch 7/500
228s - loss: 1.2333 - acc: 0.6374 - val_loss: 1.6646 - val_acc: 0.5684
Epoch 8/500
227s - loss: 1.1473 - acc: 0.6600 - val_loss: 3.3330 - val_acc: 0.4074
Epoch 9/500
228s - loss: 1.1490 - acc: 0.6729 - val_loss: 1.6855 - val_acc: 0.5768
Epoch 10/500
227s - loss: 1.0816 - acc: 0.6911 - val_loss: 1.1038 - val_acc: 0.7358
Epoch 11/500
227s - loss: 1.1332 - acc: 0.6871 - val_loss: 1.9774 - val_acc: 0.5274
Epoch 12/500
227s - loss: 1.1073 - acc: 0.6905 - val_loss: 1.1233 - val_acc: 0.7042
Epoch 13/500
228s - loss: 1.0535 - acc: 0.7005 - val_loss: 3.8714 - val_acc: 0.3042
Epoch 14/500
229s - loss: 1.0181 - acc: 0.7089 - val_loss: 1.4181 - val_acc: 0.6526
Epoch 15/500
230s - loss: 1.0886 - acc: 0.6979 - val_loss: 1.1994 - val_acc: 0.6653
Epoch 16/500
229s - loss: 1.0776 - acc: 0.7000 - val_loss: 2.7706 - val_acc: 0.4442
Epoch 17/500
230s - loss: 1.0601 - acc: 0.7163 - val_loss: 3.0460 - val_acc: 0.4779
Epoch 18/500
229s - loss: 1.0563 - acc: 0.7026 - val_loss: 1.0970 - val_acc: 0.6821
Epoch 19/500

Epoch 00018: reducing learning rate to 0.010000000149.
230s - loss: 1.0727 - acc: 0.7163 - val_loss: 1.5763 - val_acc: 0.6253
Epoch 20/500
229s - loss: 0.8971 - acc: 0.7489 - val_loss: 0.7874 - val_acc: 0.8095
Epoch 21/500
228s - loss: 0.7167 - acc: 0.7955 - val_loss: 0.7590 - val_acc: 0.8200
Epoch 22/500
228s - loss: 0.6871 - acc: 0.8016 - val_loss: 0.7088 - val_acc: 0.8242
Epoch 23/500
227s - loss: 0.6355 - acc: 0.8184 - val_loss: 0.7425 - val_acc: 0.8158
Epoch 24/500
227s - loss: 0.5727 - acc: 0.8245 - val_loss: 0.6990 - val_acc: 0.8274
Epoch 25/500
227s - loss: 0.5789 - acc: 0.8276 - val_loss: 0.6614 - val_acc: 0.8347
Epoch 26/500
227s - loss: 0.5357 - acc: 0.8392 - val_loss: 0.6825 - val_acc: 0.8326
Epoch 27/500
227s - loss: 0.5181 - acc: 0.8484 - val_loss: 0.6635 - val_acc: 0.8453
Epoch 28/500
227s - loss: 0.5090 - acc: 0.8437 - val_loss: 0.7061 - val_acc: 0.8316
Epoch 29/500
227s - loss: 0.4737 - acc: 0.8539 - val_loss: 0.6465 - val_acc: 0.8400
Epoch 30/500
228s - loss: 0.4739 - acc: 0.8479 - val_loss: 0.6493 - val_acc: 0.8453
Epoch 31/500
229s - loss: 0.4539 - acc: 0.8587 - val_loss: 0.6370 - val_acc: 0.8516
Epoch 32/500
227s - loss: 0.4607 - acc: 0.8526 - val_loss: 0.6691 - val_acc: 0.8347
Epoch 33/500
228s - loss: 0.4262 - acc: 0.8600 - val_loss: 0.6936 - val_acc: 0.8516
Epoch 34/500
230s - loss: 0.4554 - acc: 0.8608 - val_loss: 0.6718 - val_acc: 0.8495
Epoch 35/500
230s - loss: 0.4415 - acc: 0.8611 - val_loss: 0.6310 - val_acc: 0.8537
Epoch 36/500
227s - loss: 0.4028 - acc: 0.8684 - val_loss: 0.6960 - val_acc: 0.8463
Epoch 37/500
227s - loss: 0.4160 - acc: 0.8721 - val_loss: 0.6862 - val_acc: 0.8463
Epoch 38/500
229s - loss: 0.4157 - acc: 0.8687 - val_loss: 0.6343 - val_acc: 0.8526
Epoch 39/500
230s - loss: 0.3921 - acc: 0.8761 - val_loss: 0.6081 - val_acc: 0.8611
Epoch 40/500
228s - loss: 0.4123 - acc: 0.8684 - val_loss: 0.7394 - val_acc: 0.8337
Epoch 41/500
229s - loss: 0.3898 - acc: 0.8779 - val_loss: 0.7508 - val_acc: 0.8263
Epoch 42/500
229s - loss: 0.3937 - acc: 0.8734 - val_loss: 0.6251 - val_acc: 0.8537
Epoch 43/500
229s - loss: 0.3896 - acc: 0.8713 - val_loss: 0.6179 - val_acc: 0.8547
Epoch 44/500
229s - loss: 0.3702 - acc: 0.8787 - val_loss: 0.6160 - val_acc: 0.8642
Epoch 45/500
227s - loss: 0.3599 - acc: 0.8847 - val_loss: 0.7059 - val_acc: 0.8411
Epoch 46/500
227s - loss: 0.3341 - acc: 0.8847 - val_loss: 0.6405 - val_acc: 0.8579
Epoch 47/500
228s - loss: 0.3451 - acc: 0.8945 - val_loss: 0.5965 - val_acc: 0.8653
Epoch 48/500
227s - loss: 0.3795 - acc: 0.8834 - val_loss: 0.7149 - val_acc: 0.8474
Epoch 49/500
227s - loss: 0.3612 - acc: 0.8868 - val_loss: 0.6446 - val_acc: 0.8526
Epoch 50/500
226s - loss: 0.3427 - acc: 0.8874 - val_loss: 0.7351 - val_acc: 0.8516
Epoch 51/500
227s - loss: 0.3322 - acc: 0.8871 - val_loss: 0.6764 - val_acc: 0.8537
Epoch 52/500
228s - loss: 0.3568 - acc: 0.8924 - val_loss: 0.6402 - val_acc: 0.8516
Epoch 53/500
229s - loss: 0.3442 - acc: 0.8900 - val_loss: 0.6202 - val_acc: 0.8653
Epoch 54/500
228s - loss: 0.3192 - acc: 0.8929 - val_loss: 0.6245 - val_acc: 0.8663
Epoch 55/500
226s - loss: 0.3280 - acc: 0.8971 - val_loss: 0.6761 - val_acc: 0.8642
Epoch 56/500
228s - loss: 0.2903 - acc: 0.9005 - val_loss: 0.6496 - val_acc: 0.8579
Epoch 57/500
228s - loss: 0.3333 - acc: 0.9011 - val_loss: 0.6544 - val_acc: 0.8642
Epoch 58/500
228s - loss: 0.3191 - acc: 0.9034 - val_loss: 0.7433 - val_acc: 0.8453
Epoch 59/500
228s - loss: 0.2927 - acc: 0.9079 - val_loss: 0.6952 - val_acc: 0.8600
Epoch 60/500
228s - loss: 0.3035 - acc: 0.9103 - val_loss: 0.6043 - val_acc: 0.8674
Epoch 61/500
226s - loss: 0.2980 - acc: 0.9076 - val_loss: 0.7687 - val_acc: 0.8568
Epoch 62/500
226s - loss: 0.2983 - acc: 0.9034 - val_loss: 0.6791 - val_acc: 0.8632
Epoch 63/500
226s - loss: 0.2581 - acc: 0.9129 - val_loss: 0.6593 - val_acc: 0.8832
Epoch 64/500
227s - loss: 0.3130 - acc: 0.9071 - val_loss: 0.6618 - val_acc: 0.8684
Epoch 65/500
228s - loss: 0.3118 - acc: 0.9113 - val_loss: 0.6614 - val_acc: 0.8705
Epoch 66/500
228s - loss: 0.2811 - acc: 0.9074 - val_loss: 0.6778 - val_acc: 0.8663
Epoch 67/500
228s - loss: 0.2755 - acc: 0.9097 - val_loss: 0.7292 - val_acc: 0.8621
Epoch 68/500
229s - loss: 0.2653 - acc: 0.9158 - val_loss: 0.6647 - val_acc: 0.8789
Epoch 69/500
228s - loss: 0.2861 - acc: 0.9089 - val_loss: 0.6685 - val_acc: 0.8653
Epoch 70/500
228s - loss: 0.2911 - acc: 0.9113 - val_loss: 0.5891 - val_acc: 0.8674
Epoch 71/500
229s - loss: 0.2626 - acc: 0.9116 - val_loss: 0.6391 - val_acc: 0.8674
Epoch 72/500

Epoch 00071: reducing learning rate to 0.000999999977648.
229s - loss: 0.2418 - acc: 0.9179 - val_loss: 0.6652 - val_acc: 0.8716
Epoch 73/500
229s - loss: 0.2246 - acc: 0.9208 - val_loss: 0.6225 - val_acc: 0.8789
Epoch 74/500
229s - loss: 0.2182 - acc: 0.9287 - val_loss: 0.6130 - val_acc: 0.8789
Epoch 75/500
229s - loss: 0.2248 - acc: 0.9258 - val_loss: 0.6236 - val_acc: 0.8789
Epoch 76/500
228s - loss: 0.2228 - acc: 0.9242 - val_loss: 0.6080 - val_acc: 0.8779
Epoch 77/500
228s - loss: 0.2367 - acc: 0.9229 - val_loss: 0.6052 - val_acc: 0.8800
Epoch 78/500
228s - loss: 0.2283 - acc: 0.9245 - val_loss: 0.6040 - val_acc: 0.8789
Epoch 79/500
228s - loss: 0.2509 - acc: 0.9208 - val_loss: 0.6157 - val_acc: 0.8779
Epoch 80/500

Epoch 00079: reducing learning rate to 9.99999931082e-05.
228s - loss: 0.2283 - acc: 0.9226 - val_loss: 0.6215 - val_acc: 0.8758
Epoch 81/500
228s - loss: 0.2390 - acc: 0.9253 - val_loss: 0.6208 - val_acc: 0.8789
Epoch 82/500
229s - loss: 0.2548 - acc: 0.9245 - val_loss: 0.6196 - val_acc: 0.8800
Epoch 83/500
228s - loss: 0.2135 - acc: 0.9247 - val_loss: 0.6146 - val_acc: 0.8779
Epoch 84/500
229s - loss: 0.2140 - acc: 0.9326 - val_loss: 0.6167 - val_acc: 0.8800
Epoch 85/500
229s - loss: 0.2335 - acc: 0.9261 - val_loss: 0.6242 - val_acc: 0.8779
Epoch 86/500
229s - loss: 0.2105 - acc: 0.9289 - val_loss: 0.6185 - val_acc: 0.8800
Epoch 87/500
229s - loss: 0.2331 - acc: 0.9229 - val_loss: 0.6178 - val_acc: 0.8811
Epoch 88/500

Epoch 00087: reducing learning rate to 9.99999901978e-06.
228s - loss: 0.2397 - acc: 0.9237 - val_loss: 0.6077 - val_acc: 0.8800
Epoch 89/500
228s - loss: 0.2277 - acc: 0.9229 - val_loss: 0.6177 - val_acc: 0.8811
Epoch 90/500
228s - loss: 0.2251 - acc: 0.9263 - val_loss: 0.6177 - val_acc: 0.8800
Epoch 91/500
228s - loss: 0.2272 - acc: 0.9321 - val_loss: 0.6240 - val_acc: 0.8800
Epoch 92/500
228s - loss: 0.2248 - acc: 0.9279 - val_loss: 0.6226 - val_acc: 0.8789
Epoch 93/500
228s - loss: 0.2281 - acc: 0.9261 - val_loss: 0.6181 - val_acc: 0.8800
Epoch 94/500
228s - loss: 0.2275 - acc: 0.9218 - val_loss: 0.6163 - val_acc: 0.8800
Epoch 95/500
228s - loss: 0.2169 - acc: 0.9218 - val_loss: 0.6196 - val_acc: 0.8789
Epoch 96/500

Epoch 00095: reducing learning rate to 1e-06.
228s - loss: 0.2289 - acc: 0.9258 - val_loss: 0.6162 - val_acc: 0.8811
Epoch 97/500
229s - loss: 0.2282 - acc: 0.9287 - val_loss: 0.6145 - val_acc: 0.8811
Epoch 98/500
229s - loss: 0.2632 - acc: 0.9195 - val_loss: 0.6174 - val_acc: 0.8811
Epoch 99/500
228s - loss: 0.2445 - acc: 0.9284 - val_loss: 0.6171 - val_acc: 0.8821
Epoch 100/500
228s - loss: 0.2446 - acc: 0.9213 - val_loss: 0.6168 - val_acc: 0.8821
Epoch 101/500
228s - loss: 0.2288 - acc: 0.9263 - val_loss: 0.6170 - val_acc: 0.8821
Epoch 102/500
228s - loss: 0.2277 - acc: 0.9263 - val_loss: 0.6172 - val_acc: 0.8811
Epoch 103/500
229s - loss: 0.2459 - acc: 0.9253 - val_loss: 0.6227 - val_acc: 0.8811
Epoch 104/500
228s - loss: 0.2339 - acc: 0.9221 - val_loss: 0.6225 - val_acc: 0.8811
Epoch 105/500
228s - loss: 0.2259 - acc: 0.9208 - val_loss: 0.6146 - val_acc: 0.8811
Epoch 106/500
228s - loss: 0.2203 - acc: 0.9232 - val_loss: 0.6112 - val_acc: 0.8800
Epoch 107/500
229s - loss: 0.2350 - acc: 0.9234 - val_loss: 0.6125 - val_acc: 0.8800
Epoch 108/500
226s - loss: 0.2461 - acc: 0.9182 - val_loss: 0.6159 - val_acc: 0.8789
Epoch 109/500
227s - loss: 0.2383 - acc: 0.9242 - val_loss: 0.6202 - val_acc: 0.8811
Epoch 110/500
228s - loss: 0.2261 - acc: 0.9268 - val_loss: 0.6136 - val_acc: 0.8800
Epoch 111/500
229s - loss: 0.2213 - acc: 0.9263 - val_loss: 0.6112 - val_acc: 0.8821
Epoch 112/500
228s - loss: 0.2215 - acc: 0.9282 - val_loss: 0.6131 - val_acc: 0.8821
Epoch 113/500
229s - loss: 0.2281 - acc: 0.9274 - val_loss: 0.6178 - val_acc: 0.8832
Epoch 114/500
227s - loss: 0.2275 - acc: 0.9250 - val_loss: 0.6104 - val_acc: 0.8821
Epoch 115/500
229s - loss: 0.2343 - acc: 0.9268 - val_loss: 0.6171 - val_acc: 0.8800
Epoch 116/500
229s - loss: 0.2106 - acc: 0.9289 - val_loss: 0.6200 - val_acc: 0.8800
Epoch 117/500
229s - loss: 0.2363 - acc: 0.9218 - val_loss: 0.6185 - val_acc: 0.8832
Epoch 118/500
227s - loss: 0.2419 - acc: 0.9245 - val_loss: 0.6235 - val_acc: 0.8800
Epoch 119/500
226s - loss: 0.2086 - acc: 0.9250 - val_loss: 0.6180 - val_acc: 0.8800
Epoch 120/500
226s - loss: 0.2202 - acc: 0.9316 - val_loss: 0.6232 - val_acc: 0.8800
Epoch 121/500
226s - loss: 0.2330 - acc: 0.9268 - val_loss: 0.6211 - val_acc: 0.8811
Epoch 122/500
226s - loss: 0.2096 - acc: 0.9268 - val_loss: 0.6184 - val_acc: 0.8832
Epoch 123/500
226s - loss: 0.2204 - acc: 0.9255 - val_loss: 0.6155 - val_acc: 0.8821
Epoch 124/500
226s - loss: 0.2311 - acc: 0.9255 - val_loss: 0.6218 - val_acc: 0.8832
Epoch 125/500
226s - loss: 0.2133 - acc: 0.9300 - val_loss: 0.6160 - val_acc: 0.8811
Epoch 126/500
226s - loss: 0.2492 - acc: 0.9218 - val_loss: 0.6201 - val_acc: 0.8811
Epoch 127/500
226s - loss: 0.2134 - acc: 0.9271 - val_loss: 0.6201 - val_acc: 0.8811
Epoch 128/500
226s - loss: 0.2438 - acc: 0.9192 - val_loss: 0.6205 - val_acc: 0.8811
Epoch 129/500
226s - loss: 0.2360 - acc: 0.9263 - val_loss: 0.6165 - val_acc: 0.8800
Epoch 130/500
226s - loss: 0.2290 - acc: 0.9226 - val_loss: 0.6147 - val_acc: 0.8811
Epoch 131/500
226s - loss: 0.2093 - acc: 0.9308 - val_loss: 0.6090 - val_acc: 0.8821
Training loss for fold 1 is 0.09542406599380468 with percent 95.84210526315789
Testing loss for fold 1 is 0.6177591084179125 with percent 88.3157895238776
 
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_5 (InputLayer)             (None, 75, 75, 3)     0                                            
____________________________________________________________________________________________________
conv2d_13 (Conv2D)               (None, 73, 73, 16)    448         input_5[0][0]                    
____________________________________________________________________________________________________
batch_normalization_15 (BatchNor (None, 73, 73, 16)    64          conv2d_13[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_15 (LeakyReLU)       (None, 73, 73, 16)    0           batch_normalization_15[0][0]     
____________________________________________________________________________________________________
conv2d_14 (Conv2D)               (None, 72, 72, 16)    1040        leaky_re_lu_15[0][0]             
____________________________________________________________________________________________________
batch_normalization_16 (BatchNor (None, 72, 72, 16)    64          conv2d_14[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_16 (LeakyReLU)       (None, 72, 72, 16)    0           batch_normalization_16[0][0]     
____________________________________________________________________________________________________
max_pooling2d_7 (MaxPooling2D)   (None, 36, 36, 16)    0           leaky_re_lu_16[0][0]             
____________________________________________________________________________________________________
dropout_11 (Dropout)             (None, 36, 36, 16)    0           max_pooling2d_7[0][0]            
____________________________________________________________________________________________________
conv2d_15 (Conv2D)               (None, 34, 34, 32)    4640        dropout_11[0][0]                 
____________________________________________________________________________________________________
batch_normalization_17 (BatchNor (None, 34, 34, 32)    128         conv2d_15[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_17 (LeakyReLU)       (None, 34, 34, 32)    0           batch_normalization_17[0][0]     
____________________________________________________________________________________________________
conv2d_16 (Conv2D)               (None, 33, 33, 32)    4128        leaky_re_lu_17[0][0]             
____________________________________________________________________________________________________
batch_normalization_18 (BatchNor (None, 33, 33, 32)    128         conv2d_16[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_18 (LeakyReLU)       (None, 33, 33, 32)    0           batch_normalization_18[0][0]     
____________________________________________________________________________________________________
max_pooling2d_8 (MaxPooling2D)   (None, 16, 16, 32)    0           leaky_re_lu_18[0][0]             
____________________________________________________________________________________________________
dropout_12 (Dropout)             (None, 16, 16, 32)    0           max_pooling2d_8[0][0]            
____________________________________________________________________________________________________
conv2d_17 (Conv2D)               (None, 14, 14, 64)    18496       dropout_12[0][0]                 
____________________________________________________________________________________________________
batch_normalization_19 (BatchNor (None, 14, 14, 64)    256         conv2d_17[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_19 (LeakyReLU)       (None, 14, 14, 64)    0           batch_normalization_19[0][0]     
____________________________________________________________________________________________________
conv2d_18 (Conv2D)               (None, 13, 13, 64)    16448       leaky_re_lu_19[0][0]             
____________________________________________________________________________________________________
batch_normalization_20 (BatchNor (None, 13, 13, 64)    256         conv2d_18[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_20 (LeakyReLU)       (None, 13, 13, 64)    0           batch_normalization_20[0][0]     
____________________________________________________________________________________________________
max_pooling2d_9 (MaxPooling2D)   (None, 6, 6, 64)      0           leaky_re_lu_20[0][0]             
____________________________________________________________________________________________________
dropout_13 (Dropout)             (None, 6, 6, 64)      0           max_pooling2d_9[0][0]            
____________________________________________________________________________________________________
flatten_3 (Flatten)              (None, 2304)          0           dropout_13[0][0]                 
____________________________________________________________________________________________________
batch_normalization_21 (BatchNor (None, 2304)          9216        flatten_3[0][0]                  
____________________________________________________________________________________________________
input_6 (InputLayer)             (None, 102)           0                                            
____________________________________________________________________________________________________
concatenate_3 (Concatenate)      (None, 2406)          0           batch_normalization_21[0][0]     
                                                                   input_6[0][0]                    
____________________________________________________________________________________________________
dense_9 (Dense)                  (None, 128)           308096      concatenate_3[0][0]              
____________________________________________________________________________________________________
leaky_re_lu_21 (LeakyReLU)       (None, 128)           0           dense_9[0][0]                    
____________________________________________________________________________________________________
dense_10 (Dense)                 (None, 64)            8256        leaky_re_lu_21[0][0]             
____________________________________________________________________________________________________
dropout_14 (Dropout)             (None, 64)            0           dense_10[0][0]                   
____________________________________________________________________________________________________
dense_11 (Dense)                 (None, 32)            2080        dropout_14[0][0]                 
____________________________________________________________________________________________________
dropout_15 (Dropout)             (None, 32)            0           dense_11[0][0]                   
____________________________________________________________________________________________________
dense_12 (Dense)                 (None, 12)            396         dropout_15[0][0]                 
====================================================================================================
Total params: 374,140
Trainable params: 369,084
Non-trainable params: 5,056
____________________________________________________________________________________________________
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
232s - loss: 2.0817 - acc: 0.3074 - val_loss: 3.1848 - val_acc: 0.1400
Epoch 2/500
228s - loss: 1.6256 - acc: 0.4682 - val_loss: 1.8130 - val_acc: 0.3295
Epoch 3/500
228s - loss: 1.4254 - acc: 0.5447 - val_loss: 1.2489 - val_acc: 0.5842
Epoch 4/500
228s - loss: 1.3340 - acc: 0.5953 - val_loss: 2.5293 - val_acc: 0.2768
Epoch 5/500
226s - loss: 1.3091 - acc: 0.6055 - val_loss: 1.5714 - val_acc: 0.5400
Epoch 6/500
226s - loss: 1.2722 - acc: 0.6250 - val_loss: 2.4131 - val_acc: 0.3674
Epoch 7/500
226s - loss: 1.2321 - acc: 0.6321 - val_loss: 1.8501 - val_acc: 0.4600
Epoch 8/500
228s - loss: 1.2139 - acc: 0.6334 - val_loss: 1.2575 - val_acc: 0.5737
Epoch 9/500
229s - loss: 1.1560 - acc: 0.6637 - val_loss: 1.0731 - val_acc: 0.7284
Epoch 10/500
226s - loss: 1.1986 - acc: 0.6776 - val_loss: 1.8568 - val_acc: 0.5926
Epoch 11/500
226s - loss: 1.2535 - acc: 0.6600 - val_loss: 1.5365 - val_acc: 0.5989
Epoch 12/500
226s - loss: 1.2148 - acc: 0.6637 - val_loss: 2.6599 - val_acc: 0.5337
Epoch 13/500
228s - loss: 1.2125 - acc: 0.6763 - val_loss: 1.6091 - val_acc: 0.5442
Epoch 14/500
228s - loss: 1.1206 - acc: 0.7032 - val_loss: 1.1119 - val_acc: 0.7137
Epoch 15/500
228s - loss: 1.1858 - acc: 0.6821 - val_loss: 1.1301 - val_acc: 0.6832
Epoch 16/500
229s - loss: 1.1405 - acc: 0.7053 - val_loss: 3.6165 - val_acc: 0.2716
Epoch 17/500
228s - loss: 1.2162 - acc: 0.6816 - val_loss: 1.7927 - val_acc: 0.6232
Epoch 18/500

Epoch 00017: reducing learning rate to 0.010000000149.
229s - loss: 1.1252 - acc: 0.7037 - val_loss: 1.8644 - val_acc: 0.6126
Epoch 19/500
229s - loss: 1.0573 - acc: 0.7255 - val_loss: 1.0678 - val_acc: 0.7147
Epoch 20/500
229s - loss: 0.8607 - acc: 0.7682 - val_loss: 1.0230 - val_acc: 0.7305
Epoch 21/500
226s - loss: 0.8135 - acc: 0.7829 - val_loss: 0.8718 - val_acc: 0.7716
Epoch 22/500
226s - loss: 0.7796 - acc: 0.7861 - val_loss: 0.8698 - val_acc: 0.7779
Epoch 23/500
226s - loss: 0.7096 - acc: 0.8024 - val_loss: 0.8509 - val_acc: 0.7853
Epoch 24/500
226s - loss: 0.6851 - acc: 0.8050 - val_loss: 0.8279 - val_acc: 0.7747
Epoch 25/500
228s - loss: 0.6718 - acc: 0.8055 - val_loss: 0.8760 - val_acc: 0.7811
Epoch 26/500
229s - loss: 0.6623 - acc: 0.8100 - val_loss: 0.8582 - val_acc: 0.7832
Epoch 27/500
229s - loss: 0.6594 - acc: 0.8092 - val_loss: 0.8975 - val_acc: 0.7842
Epoch 28/500
229s - loss: 0.6658 - acc: 0.8150 - val_loss: 0.8525 - val_acc: 0.7811
Epoch 29/500
229s - loss: 0.6006 - acc: 0.8253 - val_loss: 0.7926 - val_acc: 0.8000
Epoch 30/500
226s - loss: 0.6427 - acc: 0.8155 - val_loss: 0.8817 - val_acc: 0.7874
Epoch 31/500
226s - loss: 0.6367 - acc: 0.8174 - val_loss: 0.7973 - val_acc: 0.7842
Epoch 32/500
226s - loss: 0.5742 - acc: 0.8253 - val_loss: 0.7512 - val_acc: 0.8032
Epoch 33/500
227s - loss: 0.6076 - acc: 0.8261 - val_loss: 0.7776 - val_acc: 0.7968
Epoch 34/500
227s - loss: 0.6109 - acc: 0.8221 - val_loss: 0.7248 - val_acc: 0.8063
Epoch 35/500
226s - loss: 0.5775 - acc: 0.8339 - val_loss: 0.7654 - val_acc: 0.8011
Epoch 36/500
226s - loss: 0.5535 - acc: 0.8300 - val_loss: 0.7961 - val_acc: 0.7874
Epoch 37/500
226s - loss: 0.5378 - acc: 0.8334 - val_loss: 0.7296 - val_acc: 0.8084
Epoch 38/500
227s - loss: 0.5360 - acc: 0.8358 - val_loss: 0.7632 - val_acc: 0.8084
Epoch 39/500
226s - loss: 0.5395 - acc: 0.8361 - val_loss: 0.8039 - val_acc: 0.8000
Epoch 40/500
226s - loss: 0.5221 - acc: 0.8397 - val_loss: 0.7390 - val_acc: 0.8105
Epoch 41/500
226s - loss: 0.5167 - acc: 0.8392 - val_loss: 0.7423 - val_acc: 0.8042
Epoch 42/500
226s - loss: 0.5338 - acc: 0.8389 - val_loss: 0.7230 - val_acc: 0.8095
Epoch 43/500
226s - loss: 0.5228 - acc: 0.8403 - val_loss: 0.7535 - val_acc: 0.8011
Epoch 44/500
226s - loss: 0.5262 - acc: 0.8371 - val_loss: 0.8125 - val_acc: 0.7895
Epoch 45/500
226s - loss: 0.5138 - acc: 0.8405 - val_loss: 0.7813 - val_acc: 0.8000
Epoch 46/500
226s - loss: 0.4885 - acc: 0.8450 - val_loss: 0.7572 - val_acc: 0.8095
Epoch 47/500
228s - loss: 0.5260 - acc: 0.8416 - val_loss: 0.7619 - val_acc: 0.8000
Epoch 48/500
228s - loss: 0.4967 - acc: 0.8432 - val_loss: 0.7595 - val_acc: 0.7979
Epoch 49/500

Epoch 00048: reducing learning rate to 0.000999999977648.
228s - loss: 0.4897 - acc: 0.8434 - val_loss: 0.7520 - val_acc: 0.8063
Epoch 50/500
228s - loss: 0.4804 - acc: 0.8468 - val_loss: 0.7340 - val_acc: 0.8053
Epoch 51/500
229s - loss: 0.4705 - acc: 0.8482 - val_loss: 0.7421 - val_acc: 0.8063
Epoch 52/500
228s - loss: 0.4803 - acc: 0.8468 - val_loss: 0.7435 - val_acc: 0.8042
Epoch 53/500
228s - loss: 0.4724 - acc: 0.8484 - val_loss: 0.7463 - val_acc: 0.8021
Epoch 54/500
228s - loss: 0.4944 - acc: 0.8432 - val_loss: 0.7474 - val_acc: 0.8042
Epoch 55/500
228s - loss: 0.4629 - acc: 0.8500 - val_loss: 0.7591 - val_acc: 0.7979
Epoch 56/500
228s - loss: 0.4538 - acc: 0.8518 - val_loss: 0.7477 - val_acc: 0.8063
Epoch 57/500

Epoch 00056: reducing learning rate to 9.99999931082e-05.
228s - loss: 0.4509 - acc: 0.8497 - val_loss: 0.7522 - val_acc: 0.8063
Epoch 58/500
228s - loss: 0.4628 - acc: 0.8526 - val_loss: 0.7514 - val_acc: 0.8053
Epoch 59/500
227s - loss: 0.4444 - acc: 0.8553 - val_loss: 0.7497 - val_acc: 0.8084
Epoch 60/500
228s - loss: 0.4629 - acc: 0.8534 - val_loss: 0.7472 - val_acc: 0.8084
Epoch 61/500
228s - loss: 0.4603 - acc: 0.8518 - val_loss: 0.7488 - val_acc: 0.8084
Epoch 62/500
228s - loss: 0.4574 - acc: 0.8516 - val_loss: 0.7491 - val_acc: 0.8053
Epoch 63/500
227s - loss: 0.4649 - acc: 0.8511 - val_loss: 0.7469 - val_acc: 0.8074
Epoch 64/500
226s - loss: 0.4713 - acc: 0.8511 - val_loss: 0.7443 - val_acc: 0.8105
Epoch 65/500

Epoch 00064: reducing learning rate to 9.99999901978e-06.
226s - loss: 0.4786 - acc: 0.8487 - val_loss: 0.7465 - val_acc: 0.8074
Epoch 66/500
226s - loss: 0.4850 - acc: 0.8476 - val_loss: 0.7407 - val_acc: 0.8095
Epoch 67/500
226s - loss: 0.4600 - acc: 0.8524 - val_loss: 0.7433 - val_acc: 0.8095
Epoch 68/500
226s - loss: 0.4483 - acc: 0.8511 - val_loss: 0.7440 - val_acc: 0.8084
Epoch 69/500
227s - loss: 0.4422 - acc: 0.8526 - val_loss: 0.7466 - val_acc: 0.8074
Epoch 70/500
228s - loss: 0.4495 - acc: 0.8511 - val_loss: 0.7438 - val_acc: 0.8084
Epoch 71/500
228s - loss: 0.4590 - acc: 0.8508 - val_loss: 0.7465 - val_acc: 0.8074
Epoch 72/500
227s - loss: 0.4756 - acc: 0.8487 - val_loss: 0.7465 - val_acc: 0.8084
Epoch 73/500

Epoch 00072: reducing learning rate to 1e-06.
227s - loss: 0.4589 - acc: 0.8526 - val_loss: 0.7490 - val_acc: 0.8074
Epoch 74/500
227s - loss: 0.4639 - acc: 0.8484 - val_loss: 0.7486 - val_acc: 0.8053
Epoch 75/500
228s - loss: 0.4583 - acc: 0.8516 - val_loss: 0.7490 - val_acc: 0.8074
Epoch 76/500
228s - loss: 0.4433 - acc: 0.8529 - val_loss: 0.7460 - val_acc: 0.8074
Epoch 77/500
228s - loss: 0.4799 - acc: 0.8495 - val_loss: 0.7439 - val_acc: 0.8084
Epoch 78/500
228s - loss: 0.4383 - acc: 0.8539 - val_loss: 0.7388 - val_acc: 0.8105
Epoch 79/500
228s - loss: 0.4558 - acc: 0.8526 - val_loss: 0.7367 - val_acc: 0.8095
Epoch 80/500
228s - loss: 0.4651 - acc: 0.8513 - val_loss: 0.7437 - val_acc: 0.8084
Epoch 81/500
228s - loss: 0.4645 - acc: 0.8526 - val_loss: 0.7454 - val_acc: 0.8084
Epoch 82/500
228s - loss: 0.4587 - acc: 0.8532 - val_loss: 0.7414 - val_acc: 0.8095
Epoch 83/500
228s - loss: 0.4741 - acc: 0.8497 - val_loss: 0.7466 - val_acc: 0.8063
Epoch 84/500
228s - loss: 0.4701 - acc: 0.8518 - val_loss: 0.7491 - val_acc: 0.8063
Epoch 85/500
228s - loss: 0.4476 - acc: 0.8529 - val_loss: 0.7455 - val_acc: 0.8074
Epoch 86/500
228s - loss: 0.4699 - acc: 0.8479 - val_loss: 0.7391 - val_acc: 0.8116
Epoch 87/500
226s - loss: 0.4706 - acc: 0.8463 - val_loss: 0.7449 - val_acc: 0.8095
Epoch 88/500
228s - loss: 0.4611 - acc: 0.8524 - val_loss: 0.7497 - val_acc: 0.8074
Epoch 89/500
228s - loss: 0.4552 - acc: 0.8492 - val_loss: 0.7460 - val_acc: 0.8074
Epoch 90/500
228s - loss: 0.4433 - acc: 0.8553 - val_loss: 0.7478 - val_acc: 0.8084
Epoch 91/500
228s - loss: 0.4768 - acc: 0.8492 - val_loss: 0.7424 - val_acc: 0.8095
Epoch 92/500
227s - loss: 0.4672 - acc: 0.8513 - val_loss: 0.7438 - val_acc: 0.8084
Epoch 93/500
225s - loss: 0.4774 - acc: 0.8526 - val_loss: 0.7536 - val_acc: 0.8063
Epoch 94/500
226s - loss: 0.4513 - acc: 0.8505 - val_loss: 0.7485 - val_acc: 0.8084
Epoch 95/500
227s - loss: 0.4678 - acc: 0.8497 - val_loss: 0.7458 - val_acc: 0.8084
Epoch 96/500
228s - loss: 0.4440 - acc: 0.8547 - val_loss: 0.7425 - val_acc: 0.8095
Epoch 97/500
228s - loss: 0.4490 - acc: 0.8524 - val_loss: 0.7510 - val_acc: 0.8074
Epoch 98/500
229s - loss: 0.4707 - acc: 0.8503 - val_loss: 0.7561 - val_acc: 0.8032
Epoch 99/500
229s - loss: 0.4373 - acc: 0.8537 - val_loss: 0.7500 - val_acc: 0.8063
Epoch 100/500
228s - loss: 0.4610 - acc: 0.8524 - val_loss: 0.7488 - val_acc: 0.8084
Epoch 101/500
226s - loss: 0.4435 - acc: 0.8539 - val_loss: 0.7520 - val_acc: 0.8074
Epoch 102/500
226s - loss: 0.4567 - acc: 0.8539 - val_loss: 0.7452 - val_acc: 0.8074
Epoch 103/500
226s - loss: 0.4705 - acc: 0.8497 - val_loss: 0.7439 - val_acc: 0.8084
Training loss for fold 2 is 0.32164157616464717 with percent 87.71052631578947
Testing loss for fold 2 is 0.7391309532993718 with percent 81.15789476193879
 
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_7 (InputLayer)             (None, 75, 75, 3)     0                                            
____________________________________________________________________________________________________
conv2d_19 (Conv2D)               (None, 73, 73, 16)    448         input_7[0][0]                    
____________________________________________________________________________________________________
batch_normalization_22 (BatchNor (None, 73, 73, 16)    64          conv2d_19[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_22 (LeakyReLU)       (None, 73, 73, 16)    0           batch_normalization_22[0][0]     
____________________________________________________________________________________________________
conv2d_20 (Conv2D)               (None, 72, 72, 16)    1040        leaky_re_lu_22[0][0]             
____________________________________________________________________________________________________
batch_normalization_23 (BatchNor (None, 72, 72, 16)    64          conv2d_20[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_23 (LeakyReLU)       (None, 72, 72, 16)    0           batch_normalization_23[0][0]     
____________________________________________________________________________________________________
max_pooling2d_10 (MaxPooling2D)  (None, 36, 36, 16)    0           leaky_re_lu_23[0][0]             
____________________________________________________________________________________________________
dropout_16 (Dropout)             (None, 36, 36, 16)    0           max_pooling2d_10[0][0]           
____________________________________________________________________________________________________
conv2d_21 (Conv2D)               (None, 34, 34, 32)    4640        dropout_16[0][0]                 
____________________________________________________________________________________________________
batch_normalization_24 (BatchNor (None, 34, 34, 32)    128         conv2d_21[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_24 (LeakyReLU)       (None, 34, 34, 32)    0           batch_normalization_24[0][0]     
____________________________________________________________________________________________________
conv2d_22 (Conv2D)               (None, 33, 33, 32)    4128        leaky_re_lu_24[0][0]             
____________________________________________________________________________________________________
batch_normalization_25 (BatchNor (None, 33, 33, 32)    128         conv2d_22[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_25 (LeakyReLU)       (None, 33, 33, 32)    0           batch_normalization_25[0][0]     
____________________________________________________________________________________________________
max_pooling2d_11 (MaxPooling2D)  (None, 16, 16, 32)    0           leaky_re_lu_25[0][0]             
____________________________________________________________________________________________________
dropout_17 (Dropout)             (None, 16, 16, 32)    0           max_pooling2d_11[0][0]           
____________________________________________________________________________________________________
conv2d_23 (Conv2D)               (None, 14, 14, 64)    18496       dropout_17[0][0]                 
____________________________________________________________________________________________________
batch_normalization_26 (BatchNor (None, 14, 14, 64)    256         conv2d_23[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_26 (LeakyReLU)       (None, 14, 14, 64)    0           batch_normalization_26[0][0]     
____________________________________________________________________________________________________
conv2d_24 (Conv2D)               (None, 13, 13, 64)    16448       leaky_re_lu_26[0][0]             
____________________________________________________________________________________________________
batch_normalization_27 (BatchNor (None, 13, 13, 64)    256         conv2d_24[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_27 (LeakyReLU)       (None, 13, 13, 64)    0           batch_normalization_27[0][0]     
____________________________________________________________________________________________________
max_pooling2d_12 (MaxPooling2D)  (None, 6, 6, 64)      0           leaky_re_lu_27[0][0]             
____________________________________________________________________________________________________
dropout_18 (Dropout)             (None, 6, 6, 64)      0           max_pooling2d_12[0][0]           
____________________________________________________________________________________________________
flatten_4 (Flatten)              (None, 2304)          0           dropout_18[0][0]                 
____________________________________________________________________________________________________
batch_normalization_28 (BatchNor (None, 2304)          9216        flatten_4[0][0]                  
____________________________________________________________________________________________________
input_8 (InputLayer)             (None, 102)           0                                            
____________________________________________________________________________________________________
concatenate_4 (Concatenate)      (None, 2406)          0           batch_normalization_28[0][0]     
                                                                   input_8[0][0]                    
____________________________________________________________________________________________________
dense_13 (Dense)                 (None, 128)           308096      concatenate_4[0][0]              
____________________________________________________________________________________________________
leaky_re_lu_28 (LeakyReLU)       (None, 128)           0           dense_13[0][0]                   
____________________________________________________________________________________________________
dense_14 (Dense)                 (None, 64)            8256        leaky_re_lu_28[0][0]             
____________________________________________________________________________________________________
dropout_19 (Dropout)             (None, 64)            0           dense_14[0][0]                   
____________________________________________________________________________________________________
dense_15 (Dense)                 (None, 32)            2080        dropout_19[0][0]                 
____________________________________________________________________________________________________
dropout_20 (Dropout)             (None, 32)            0           dense_15[0][0]                   
____________________________________________________________________________________________________
dense_16 (Dense)                 (None, 12)            396         dropout_20[0][0]                 
====================================================================================================
Total params: 374,140
Trainable params: 369,084
Non-trainable params: 5,056
____________________________________________________________________________________________________
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
232s - loss: 2.0592 - acc: 0.3050 - val_loss: 3.5972 - val_acc: 0.1768
Epoch 2/500
227s - loss: 1.6424 - acc: 0.4497 - val_loss: 2.7220 - val_acc: 0.4232
Epoch 3/500
228s - loss: 1.5307 - acc: 0.5024 - val_loss: 1.2135 - val_acc: 0.5495
Epoch 4/500
227s - loss: 1.4135 - acc: 0.5447 - val_loss: 1.6297 - val_acc: 0.4968
Epoch 5/500
228s - loss: 1.3599 - acc: 0.5687 - val_loss: 1.5671 - val_acc: 0.5611
Epoch 6/500
227s - loss: 1.3668 - acc: 0.5884 - val_loss: 1.7101 - val_acc: 0.5074
Epoch 7/500
227s - loss: 1.2971 - acc: 0.6053 - val_loss: 1.0657 - val_acc: 0.6779
Epoch 8/500
228s - loss: 1.2777 - acc: 0.6111 - val_loss: 1.4339 - val_acc: 0.6063
Epoch 9/500
229s - loss: 1.1982 - acc: 0.6471 - val_loss: 2.2292 - val_acc: 0.4979
Epoch 10/500
229s - loss: 1.1817 - acc: 0.6547 - val_loss: 1.0593 - val_acc: 0.6632
Epoch 11/500
229s - loss: 1.1124 - acc: 0.6668 - val_loss: 1.6791 - val_acc: 0.5505
Epoch 12/500
229s - loss: 1.0957 - acc: 0.6837 - val_loss: 1.7426 - val_acc: 0.5337
Epoch 13/500
229s - loss: 1.1017 - acc: 0.6926 - val_loss: 1.4923 - val_acc: 0.5937
Epoch 14/500
229s - loss: 1.1434 - acc: 0.6832 - val_loss: 3.4521 - val_acc: 0.1863
Epoch 15/500
230s - loss: 1.1084 - acc: 0.6853 - val_loss: 1.8602 - val_acc: 0.4705
Epoch 16/500

Epoch 00015: reducing learning rate to 0.010000000149.
230s - loss: 1.0885 - acc: 0.6953 - val_loss: 2.2455 - val_acc: 0.5421
Epoch 17/500
229s - loss: 0.8988 - acc: 0.7326 - val_loss: 0.8617 - val_acc: 0.7516
Epoch 18/500
227s - loss: 0.7507 - acc: 0.7732 - val_loss: 0.7489 - val_acc: 0.7926
Epoch 19/500
226s - loss: 0.6908 - acc: 0.8063 - val_loss: 0.7965 - val_acc: 0.8042
Epoch 20/500
226s - loss: 0.6358 - acc: 0.8126 - val_loss: 0.7142 - val_acc: 0.8105
Epoch 21/500
226s - loss: 0.6718 - acc: 0.8084 - val_loss: 0.6790 - val_acc: 0.8168
Epoch 22/500
226s - loss: 0.6185 - acc: 0.8139 - val_loss: 0.8084 - val_acc: 0.7968
Epoch 23/500
227s - loss: 0.5918 - acc: 0.8287 - val_loss: 0.6857 - val_acc: 0.8116
Epoch 24/500
228s - loss: 0.5421 - acc: 0.8405 - val_loss: 0.6746 - val_acc: 0.8263
Epoch 25/500
227s - loss: 0.5876 - acc: 0.8318 - val_loss: 0.7047 - val_acc: 0.8232
Epoch 26/500
228s - loss: 0.5356 - acc: 0.8453 - val_loss: 0.6295 - val_acc: 0.8358
Epoch 27/500
228s - loss: 0.5423 - acc: 0.8421 - val_loss: 0.7525 - val_acc: 0.8211
Epoch 28/500
228s - loss: 0.5294 - acc: 0.8447 - val_loss: 0.7096 - val_acc: 0.8168
Epoch 29/500
228s - loss: 0.4801 - acc: 0.8505 - val_loss: 0.5913 - val_acc: 0.8474
Epoch 30/500
226s - loss: 0.5242 - acc: 0.8476 - val_loss: 0.9063 - val_acc: 0.8032
Epoch 31/500
227s - loss: 0.5257 - acc: 0.8495 - val_loss: 0.5964 - val_acc: 0.8400
Epoch 32/500
228s - loss: 0.4901 - acc: 0.8561 - val_loss: 0.7404 - val_acc: 0.8358
Epoch 33/500
228s - loss: 0.4957 - acc: 0.8537 - val_loss: 0.5618 - val_acc: 0.8516
Epoch 34/500
226s - loss: 0.4680 - acc: 0.8624 - val_loss: 0.6001 - val_acc: 0.8421
Epoch 35/500
227s - loss: 0.4619 - acc: 0.8605 - val_loss: 0.5986 - val_acc: 0.8421
Epoch 36/500
228s - loss: 0.4859 - acc: 0.8539 - val_loss: 0.5870 - val_acc: 0.8463
Epoch 37/500
227s - loss: 0.4570 - acc: 0.8629 - val_loss: 0.5617 - val_acc: 0.8411
Epoch 38/500
228s - loss: 0.4482 - acc: 0.8637 - val_loss: 0.5746 - val_acc: 0.8526
Epoch 39/500
226s - loss: 0.4467 - acc: 0.8574 - val_loss: 0.5622 - val_acc: 0.8537
Epoch 40/500
226s - loss: 0.4309 - acc: 0.8703 - val_loss: 0.6058 - val_acc: 0.8526
Epoch 41/500
226s - loss: 0.4140 - acc: 0.8721 - val_loss: 0.5313 - val_acc: 0.8547
Epoch 42/500
226s - loss: 0.4290 - acc: 0.8742 - val_loss: 0.6222 - val_acc: 0.8421
Epoch 43/500
226s - loss: 0.3905 - acc: 0.8795 - val_loss: 0.5417 - val_acc: 0.8526
Epoch 44/500
226s - loss: 0.3972 - acc: 0.8779 - val_loss: 0.5950 - val_acc: 0.8453
Epoch 45/500
226s - loss: 0.3990 - acc: 0.8808 - val_loss: 0.5499 - val_acc: 0.8558
Epoch 46/500
226s - loss: 0.4218 - acc: 0.8721 - val_loss: 0.6195 - val_acc: 0.8516
Epoch 47/500
226s - loss: 0.3778 - acc: 0.8842 - val_loss: 0.5967 - val_acc: 0.8600
Epoch 48/500
225s - loss: 0.4060 - acc: 0.8761 - val_loss: 0.5484 - val_acc: 0.8484
Epoch 49/500
226s - loss: 0.3792 - acc: 0.8784 - val_loss: 0.5399 - val_acc: 0.8568
Epoch 50/500
226s - loss: 0.3893 - acc: 0.8813 - val_loss: 0.5089 - val_acc: 0.8705
Epoch 51/500
226s - loss: 0.3899 - acc: 0.8850 - val_loss: 0.5596 - val_acc: 0.8558
Epoch 52/500
227s - loss: 0.3651 - acc: 0.8874 - val_loss: 0.6049 - val_acc: 0.8453
Epoch 53/500
228s - loss: 0.3685 - acc: 0.8918 - val_loss: 0.5593 - val_acc: 0.8505
Epoch 54/500
228s - loss: 0.3461 - acc: 0.8858 - val_loss: 0.6702 - val_acc: 0.8168
Epoch 55/500
228s - loss: 0.3512 - acc: 0.8895 - val_loss: 0.5309 - val_acc: 0.8737
Epoch 56/500
226s - loss: 0.3306 - acc: 0.8924 - val_loss: 0.6624 - val_acc: 0.8568
Epoch 57/500
226s - loss: 0.3139 - acc: 0.8950 - val_loss: 0.5523 - val_acc: 0.8705
Epoch 58/500
226s - loss: 0.3642 - acc: 0.8863 - val_loss: 0.6981 - val_acc: 0.8474
Epoch 59/500
226s - loss: 0.3492 - acc: 0.8908 - val_loss: 0.5630 - val_acc: 0.8663
Epoch 60/500
228s - loss: 0.3526 - acc: 0.8939 - val_loss: 0.6174 - val_acc: 0.8495
Epoch 61/500
228s - loss: 0.3548 - acc: 0.8905 - val_loss: 0.5990 - val_acc: 0.8558
Epoch 62/500
228s - loss: 0.3484 - acc: 0.8974 - val_loss: 0.6215 - val_acc: 0.8453
Epoch 63/500
228s - loss: 0.3344 - acc: 0.8950 - val_loss: 0.5600 - val_acc: 0.8684
Epoch 64/500
228s - loss: 0.3403 - acc: 0.8987 - val_loss: 0.5359 - val_acc: 0.8821
Epoch 65/500
226s - loss: 0.3220 - acc: 0.9011 - val_loss: 0.5690 - val_acc: 0.8653
Epoch 66/500
228s - loss: 0.3352 - acc: 0.9050 - val_loss: 0.5951 - val_acc: 0.8516
Epoch 67/500
228s - loss: 0.3108 - acc: 0.9018 - val_loss: 0.5514 - val_acc: 0.8716
Epoch 68/500
229s - loss: 0.3469 - acc: 0.8995 - val_loss: 0.5153 - val_acc: 0.8779
Epoch 69/500
229s - loss: 0.2968 - acc: 0.9095 - val_loss: 0.5083 - val_acc: 0.8800
Epoch 70/500
229s - loss: 0.3332 - acc: 0.8992 - val_loss: 0.5699 - val_acc: 0.8621
Epoch 71/500
228s - loss: 0.3068 - acc: 0.9095 - val_loss: 0.5529 - val_acc: 0.8747
Epoch 72/500
229s - loss: 0.3108 - acc: 0.9068 - val_loss: 0.6876 - val_acc: 0.8400
Epoch 73/500

Epoch 00072: reducing learning rate to 0.000999999977648.
228s - loss: 0.2978 - acc: 0.9092 - val_loss: 0.5289 - val_acc: 0.8789
Epoch 74/500
228s - loss: 0.3138 - acc: 0.9055 - val_loss: 0.5214 - val_acc: 0.8779
Epoch 75/500
228s - loss: 0.2979 - acc: 0.9118 - val_loss: 0.5039 - val_acc: 0.8811
Epoch 76/500
228s - loss: 0.2997 - acc: 0.9118 - val_loss: 0.4951 - val_acc: 0.8842
Epoch 77/500
226s - loss: 0.2639 - acc: 0.9176 - val_loss: 0.4998 - val_acc: 0.8800
Epoch 78/500
226s - loss: 0.3142 - acc: 0.9050 - val_loss: 0.4915 - val_acc: 0.8821
Epoch 79/500
226s - loss: 0.2594 - acc: 0.9137 - val_loss: 0.4930 - val_acc: 0.8821
Epoch 80/500
227s - loss: 0.2701 - acc: 0.9124 - val_loss: 0.4869 - val_acc: 0.8821
Epoch 81/500
228s - loss: 0.2611 - acc: 0.9189 - val_loss: 0.4969 - val_acc: 0.8800
Epoch 82/500
228s - loss: 0.2736 - acc: 0.9137 - val_loss: 0.4906 - val_acc: 0.8832
Epoch 83/500
228s - loss: 0.2774 - acc: 0.9166 - val_loss: 0.4903 - val_acc: 0.8811
Epoch 84/500
228s - loss: 0.2624 - acc: 0.9174 - val_loss: 0.4898 - val_acc: 0.8832
Epoch 85/500

Epoch 00084: reducing learning rate to 9.99999931082e-05.
228s - loss: 0.2726 - acc: 0.9153 - val_loss: 0.4981 - val_acc: 0.8832
Epoch 86/500
229s - loss: 0.2598 - acc: 0.9168 - val_loss: 0.4934 - val_acc: 0.8853
Epoch 87/500
226s - loss: 0.2714 - acc: 0.9166 - val_loss: 0.4923 - val_acc: 0.8842
Epoch 88/500
226s - loss: 0.2754 - acc: 0.9142 - val_loss: 0.4916 - val_acc: 0.8832
Epoch 89/500
227s - loss: 0.2623 - acc: 0.9203 - val_loss: 0.4940 - val_acc: 0.8853
Epoch 90/500
229s - loss: 0.2520 - acc: 0.9253 - val_loss: 0.4937 - val_acc: 0.8853
Epoch 91/500
228s - loss: 0.3021 - acc: 0.9166 - val_loss: 0.4917 - val_acc: 0.8853
Epoch 92/500
227s - loss: 0.2872 - acc: 0.9145 - val_loss: 0.4909 - val_acc: 0.8853
Epoch 93/500
228s - loss: 0.2842 - acc: 0.9158 - val_loss: 0.4923 - val_acc: 0.8832
Epoch 94/500
229s - loss: 0.2641 - acc: 0.9195 - val_loss: 0.4892 - val_acc: 0.8853
Epoch 95/500

Epoch 00094: reducing learning rate to 9.99999901978e-06.
228s - loss: 0.2609 - acc: 0.9189 - val_loss: 0.4937 - val_acc: 0.8853
Epoch 96/500
228s - loss: 0.2863 - acc: 0.9184 - val_loss: 0.4936 - val_acc: 0.8842
Epoch 97/500
228s - loss: 0.2859 - acc: 0.9192 - val_loss: 0.4914 - val_acc: 0.8853
Epoch 98/500
229s - loss: 0.2418 - acc: 0.9232 - val_loss: 0.4911 - val_acc: 0.8853
Epoch 99/500
229s - loss: 0.2540 - acc: 0.9211 - val_loss: 0.4879 - val_acc: 0.8853
Epoch 100/500
228s - loss: 0.2552 - acc: 0.9226 - val_loss: 0.4901 - val_acc: 0.8874
Epoch 101/500
226s - loss: 0.2633 - acc: 0.9161 - val_loss: 0.4904 - val_acc: 0.8853
Epoch 102/500
227s - loss: 0.2739 - acc: 0.9189 - val_loss: 0.4920 - val_acc: 0.8853
Epoch 103/500
228s - loss: 0.2665 - acc: 0.9182 - val_loss: 0.4910 - val_acc: 0.8853
Epoch 104/500
228s - loss: 0.2774 - acc: 0.9147 - val_loss: 0.4879 - val_acc: 0.8853
Epoch 105/500
228s - loss: 0.2735 - acc: 0.9155 - val_loss: 0.4912 - val_acc: 0.8853
Epoch 106/500
228s - loss: 0.2614 - acc: 0.9168 - val_loss: 0.4912 - val_acc: 0.8842
Epoch 107/500
226s - loss: 0.2788 - acc: 0.9192 - val_loss: 0.4940 - val_acc: 0.8863
Epoch 108/500
226s - loss: 0.2623 - acc: 0.9226 - val_loss: 0.4906 - val_acc: 0.8863
Epoch 109/500

Epoch 00108: reducing learning rate to 1e-06.
226s - loss: 0.2556 - acc: 0.9247 - val_loss: 0.4928 - val_acc: 0.8842
Epoch 110/500
226s - loss: 0.2508 - acc: 0.9195 - val_loss: 0.4911 - val_acc: 0.8853
Epoch 111/500
226s - loss: 0.2758 - acc: 0.9108 - val_loss: 0.4929 - val_acc: 0.8853
Epoch 112/500
227s - loss: 0.2734 - acc: 0.9211 - val_loss: 0.4919 - val_acc: 0.8853
Epoch 113/500
227s - loss: 0.2860 - acc: 0.9111 - val_loss: 0.4885 - val_acc: 0.8853
Epoch 114/500
227s - loss: 0.2581 - acc: 0.9224 - val_loss: 0.4904 - val_acc: 0.8863
Epoch 115/500
228s - loss: 0.2498 - acc: 0.9237 - val_loss: 0.4950 - val_acc: 0.8842
Epoch 116/500
227s - loss: 0.2544 - acc: 0.9203 - val_loss: 0.4939 - val_acc: 0.8853
Epoch 117/500
226s - loss: 0.2740 - acc: 0.9134 - val_loss: 0.4929 - val_acc: 0.8863
Epoch 118/500
226s - loss: 0.2779 - acc: 0.9218 - val_loss: 0.4904 - val_acc: 0.8874
Epoch 119/500
226s - loss: 0.2633 - acc: 0.9224 - val_loss: 0.4881 - val_acc: 0.8853
Epoch 120/500
226s - loss: 0.2555 - acc: 0.9271 - val_loss: 0.4911 - val_acc: 0.8853
Epoch 121/500
226s - loss: 0.2590 - acc: 0.9229 - val_loss: 0.4892 - val_acc: 0.8853
Epoch 122/500
226s - loss: 0.2712 - acc: 0.9166 - val_loss: 0.4913 - val_acc: 0.8853
Epoch 123/500
226s - loss: 0.2774 - acc: 0.9197 - val_loss: 0.4921 - val_acc: 0.8863
Epoch 124/500
226s - loss: 0.2778 - acc: 0.9203 - val_loss: 0.4934 - val_acc: 0.8853
Epoch 125/500
226s - loss: 0.2519 - acc: 0.9187 - val_loss: 0.4908 - val_acc: 0.8853
Epoch 126/500
226s - loss: 0.2475 - acc: 0.9247 - val_loss: 0.4916 - val_acc: 0.8874
Epoch 127/500
226s - loss: 0.2769 - acc: 0.9171 - val_loss: 0.4904 - val_acc: 0.8853
Epoch 128/500
226s - loss: 0.2661 - acc: 0.9158 - val_loss: 0.4893 - val_acc: 0.8863
Epoch 129/500
226s - loss: 0.2536 - acc: 0.9211 - val_loss: 0.4924 - val_acc: 0.8842
Epoch 130/500
226s - loss: 0.2751 - acc: 0.9184 - val_loss: 0.4895 - val_acc: 0.8853
Epoch 131/500
226s - loss: 0.2745 - acc: 0.9187 - val_loss: 0.4909 - val_acc: 0.8853
Epoch 132/500
226s - loss: 0.2485 - acc: 0.9242 - val_loss: 0.4892 - val_acc: 0.8853
Epoch 133/500
226s - loss: 0.2587 - acc: 0.9208 - val_loss: 0.4913 - val_acc: 0.8853
Epoch 134/500
226s - loss: 0.2768 - acc: 0.9147 - val_loss: 0.4906 - val_acc: 0.8853
Epoch 135/500
226s - loss: 0.2734 - acc: 0.9176 - val_loss: 0.4910 - val_acc: 0.8853
Epoch 136/500
226s - loss: 0.2668 - acc: 0.9182 - val_loss: 0.4891 - val_acc: 0.8853
Epoch 137/500
226s - loss: 0.2606 - acc: 0.9229 - val_loss: 0.4897 - val_acc: 0.8853
Epoch 138/500
226s - loss: 0.2447 - acc: 0.9218 - val_loss: 0.4914 - val_acc: 0.8853
Epoch 139/500
226s - loss: 0.2389 - acc: 0.9245 - val_loss: 0.4910 - val_acc: 0.8863
Epoch 140/500
227s - loss: 0.2794 - acc: 0.9126 - val_loss: 0.4902 - val_acc: 0.8853
Epoch 141/500
227s - loss: 0.2393 - acc: 0.9229 - val_loss: 0.4905 - val_acc: 0.8863
Training loss for fold 3 is 0.13731587958963293 with percent 94.63157894736842
Testing loss for fold 3 is 0.490133952968999 with percent 88.73684209271481
 
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_9 (InputLayer)             (None, 75, 75, 3)     0                                            
____________________________________________________________________________________________________
conv2d_25 (Conv2D)               (None, 73, 73, 16)    448         input_9[0][0]                    
____________________________________________________________________________________________________
batch_normalization_29 (BatchNor (None, 73, 73, 16)    64          conv2d_25[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_29 (LeakyReLU)       (None, 73, 73, 16)    0           batch_normalization_29[0][0]     
____________________________________________________________________________________________________
conv2d_26 (Conv2D)               (None, 72, 72, 16)    1040        leaky_re_lu_29[0][0]             
____________________________________________________________________________________________________
batch_normalization_30 (BatchNor (None, 72, 72, 16)    64          conv2d_26[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_30 (LeakyReLU)       (None, 72, 72, 16)    0           batch_normalization_30[0][0]     
____________________________________________________________________________________________________
max_pooling2d_13 (MaxPooling2D)  (None, 36, 36, 16)    0           leaky_re_lu_30[0][0]             
____________________________________________________________________________________________________
dropout_21 (Dropout)             (None, 36, 36, 16)    0           max_pooling2d_13[0][0]           
____________________________________________________________________________________________________
conv2d_27 (Conv2D)               (None, 34, 34, 32)    4640        dropout_21[0][0]                 
____________________________________________________________________________________________________
batch_normalization_31 (BatchNor (None, 34, 34, 32)    128         conv2d_27[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_31 (LeakyReLU)       (None, 34, 34, 32)    0           batch_normalization_31[0][0]     
____________________________________________________________________________________________________
conv2d_28 (Conv2D)               (None, 33, 33, 32)    4128        leaky_re_lu_31[0][0]             
____________________________________________________________________________________________________
batch_normalization_32 (BatchNor (None, 33, 33, 32)    128         conv2d_28[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_32 (LeakyReLU)       (None, 33, 33, 32)    0           batch_normalization_32[0][0]     
____________________________________________________________________________________________________
max_pooling2d_14 (MaxPooling2D)  (None, 16, 16, 32)    0           leaky_re_lu_32[0][0]             
____________________________________________________________________________________________________
dropout_22 (Dropout)             (None, 16, 16, 32)    0           max_pooling2d_14[0][0]           
____________________________________________________________________________________________________
conv2d_29 (Conv2D)               (None, 14, 14, 64)    18496       dropout_22[0][0]                 
____________________________________________________________________________________________________
batch_normalization_33 (BatchNor (None, 14, 14, 64)    256         conv2d_29[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_33 (LeakyReLU)       (None, 14, 14, 64)    0           batch_normalization_33[0][0]     
____________________________________________________________________________________________________
conv2d_30 (Conv2D)               (None, 13, 13, 64)    16448       leaky_re_lu_33[0][0]             
____________________________________________________________________________________________________
batch_normalization_34 (BatchNor (None, 13, 13, 64)    256         conv2d_30[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_34 (LeakyReLU)       (None, 13, 13, 64)    0           batch_normalization_34[0][0]     
____________________________________________________________________________________________________
max_pooling2d_15 (MaxPooling2D)  (None, 6, 6, 64)      0           leaky_re_lu_34[0][0]             
____________________________________________________________________________________________________
dropout_23 (Dropout)             (None, 6, 6, 64)      0           max_pooling2d_15[0][0]           
____________________________________________________________________________________________________
flatten_5 (Flatten)              (None, 2304)          0           dropout_23[0][0]                 
____________________________________________________________________________________________________
batch_normalization_35 (BatchNor (None, 2304)          9216        flatten_5[0][0]                  
____________________________________________________________________________________________________
input_10 (InputLayer)            (None, 102)           0                                            
____________________________________________________________________________________________________
concatenate_5 (Concatenate)      (None, 2406)          0           batch_normalization_35[0][0]     
                                                                   input_10[0][0]                   
____________________________________________________________________________________________________
dense_17 (Dense)                 (None, 128)           308096      concatenate_5[0][0]              
____________________________________________________________________________________________________
leaky_re_lu_35 (LeakyReLU)       (None, 128)           0           dense_17[0][0]                   
____________________________________________________________________________________________________
dense_18 (Dense)                 (None, 64)            8256        leaky_re_lu_35[0][0]             
____________________________________________________________________________________________________
dropout_24 (Dropout)             (None, 64)            0           dense_18[0][0]                   
____________________________________________________________________________________________________
dense_19 (Dense)                 (None, 32)            2080        dropout_24[0][0]                 
____________________________________________________________________________________________________
dropout_25 (Dropout)             (None, 32)            0           dense_19[0][0]                   
____________________________________________________________________________________________________
dense_20 (Dense)                 (None, 12)            396         dropout_25[0][0]                 
====================================================================================================
Total params: 374,140
Trainable params: 369,084
Non-trainable params: 5,056
____________________________________________________________________________________________________
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
230s - loss: 2.0100 - acc: 0.3174 - val_loss: 1.7304 - val_acc: 0.3884
Epoch 2/500
226s - loss: 1.6092 - acc: 0.4711 - val_loss: 2.0919 - val_acc: 0.4042
Epoch 3/500
227s - loss: 1.4953 - acc: 0.5105 - val_loss: 1.7575 - val_acc: 0.5274
Epoch 4/500
226s - loss: 1.4421 - acc: 0.5484 - val_loss: 1.3346 - val_acc: 0.5642
Epoch 5/500
226s - loss: 1.3141 - acc: 0.5903 - val_loss: 1.7032 - val_acc: 0.4368
Epoch 6/500
228s - loss: 1.2340 - acc: 0.6321 - val_loss: 6.1313 - val_acc: 0.2116
Epoch 7/500
228s - loss: 1.1925 - acc: 0.6432 - val_loss: 1.1454 - val_acc: 0.6274
Epoch 8/500
226s - loss: 1.1673 - acc: 0.6695 - val_loss: 1.6418 - val_acc: 0.5126
Epoch 9/500
226s - loss: 1.1188 - acc: 0.6787 - val_loss: 2.0298 - val_acc: 0.4863
Epoch 10/500
226s - loss: 1.0748 - acc: 0.6955 - val_loss: 1.4112 - val_acc: 0.5958
Epoch 11/500
227s - loss: 1.0611 - acc: 0.7103 - val_loss: 1.2613 - val_acc: 0.7179
Epoch 12/500
226s - loss: 0.9875 - acc: 0.7268 - val_loss: 1.3980 - val_acc: 0.6705
Epoch 13/500
226s - loss: 1.0229 - acc: 0.7139 - val_loss: 0.9477 - val_acc: 0.7284
Epoch 14/500
226s - loss: 0.9277 - acc: 0.7468 - val_loss: 0.8976 - val_acc: 0.7211
Epoch 15/500
226s - loss: 0.9798 - acc: 0.7347 - val_loss: 1.6026 - val_acc: 0.5200
Epoch 16/500
228s - loss: 0.9685 - acc: 0.7395 - val_loss: 2.5237 - val_acc: 0.4663
Epoch 17/500
228s - loss: 0.9297 - acc: 0.7492 - val_loss: 1.5452 - val_acc: 0.6737
Epoch 18/500
228s - loss: 0.9453 - acc: 0.7461 - val_loss: 1.4790 - val_acc: 0.7253
Epoch 19/500
228s - loss: 0.9901 - acc: 0.7426 - val_loss: 1.8627 - val_acc: 0.6432
Epoch 20/500
228s - loss: 0.8993 - acc: 0.7618 - val_loss: 1.9362 - val_acc: 0.6189
Epoch 21/500
228s - loss: 0.9032 - acc: 0.7589 - val_loss: 2.8413 - val_acc: 0.4842
Epoch 22/500

Epoch 00021: reducing learning rate to 0.010000000149.
228s - loss: 0.9977 - acc: 0.7526 - val_loss: 1.6350 - val_acc: 0.7116
Epoch 23/500
228s - loss: 0.8276 - acc: 0.7916 - val_loss: 0.9113 - val_acc: 0.8021
Epoch 24/500
225s - loss: 0.6175 - acc: 0.8332 - val_loss: 0.8131 - val_acc: 0.8147
Epoch 25/500
225s - loss: 0.5781 - acc: 0.8432 - val_loss: 0.6692 - val_acc: 0.8411
Epoch 26/500
225s - loss: 0.5345 - acc: 0.8474 - val_loss: 0.6436 - val_acc: 0.8411
Epoch 27/500
226s - loss: 0.5057 - acc: 0.8563 - val_loss: 0.6528 - val_acc: 0.8400
Epoch 28/500
227s - loss: 0.5225 - acc: 0.8521 - val_loss: 0.6323 - val_acc: 0.8379
Epoch 29/500
227s - loss: 0.4873 - acc: 0.8566 - val_loss: 0.5834 - val_acc: 0.8484
Epoch 30/500
225s - loss: 0.4494 - acc: 0.8668 - val_loss: 0.5909 - val_acc: 0.8474
Epoch 31/500
225s - loss: 0.4208 - acc: 0.8666 - val_loss: 0.5174 - val_acc: 0.8621
Epoch 32/500
226s - loss: 0.4042 - acc: 0.8729 - val_loss: 0.5914 - val_acc: 0.8516
Epoch 33/500
226s - loss: 0.4038 - acc: 0.8721 - val_loss: 0.5551 - val_acc: 0.8547
Epoch 34/500
226s - loss: 0.3932 - acc: 0.8739 - val_loss: 0.5551 - val_acc: 0.8568
Epoch 35/500
227s - loss: 0.3901 - acc: 0.8779 - val_loss: 0.5619 - val_acc: 0.8537
Epoch 36/500
227s - loss: 0.3887 - acc: 0.8758 - val_loss: 0.5198 - val_acc: 0.8568
Epoch 37/500
227s - loss: 0.3709 - acc: 0.8784 - val_loss: 0.5475 - val_acc: 0.8621
Epoch 38/500
225s - loss: 0.3837 - acc: 0.8826 - val_loss: 0.6466 - val_acc: 0.8537
Epoch 39/500
226s - loss: 0.3709 - acc: 0.8858 - val_loss: 0.5125 - val_acc: 0.8695
Epoch 40/500
225s - loss: 0.3633 - acc: 0.8834 - val_loss: 0.5476 - val_acc: 0.8674
Epoch 41/500
227s - loss: 0.3548 - acc: 0.8824 - val_loss: 0.8057 - val_acc: 0.8347
Epoch 42/500
227s - loss: 0.3474 - acc: 0.8874 - val_loss: 0.5918 - val_acc: 0.8579
Epoch 43/500
228s - loss: 0.3539 - acc: 0.8858 - val_loss: 0.6007 - val_acc: 0.8463
Epoch 44/500
227s - loss: 0.3443 - acc: 0.8868 - val_loss: 0.5013 - val_acc: 0.8716
Epoch 45/500
226s - loss: 0.3326 - acc: 0.8932 - val_loss: 0.4843 - val_acc: 0.8716
Epoch 46/500
227s - loss: 0.3527 - acc: 0.8834 - val_loss: 0.6044 - val_acc: 0.8495
Epoch 47/500
227s - loss: 0.3317 - acc: 0.8853 - val_loss: 0.5225 - val_acc: 0.8684
Epoch 48/500
227s - loss: 0.3403 - acc: 0.8895 - val_loss: 0.5120 - val_acc: 0.8653
Epoch 49/500
227s - loss: 0.3434 - acc: 0.8945 - val_loss: 0.5194 - val_acc: 0.8663
Epoch 50/500
227s - loss: 0.3117 - acc: 0.8945 - val_loss: 0.5180 - val_acc: 0.8747
Epoch 51/500
225s - loss: 0.3036 - acc: 0.9016 - val_loss: 0.5181 - val_acc: 0.8663
Epoch 52/500
225s - loss: 0.2892 - acc: 0.8984 - val_loss: 0.5429 - val_acc: 0.8758
Epoch 53/500
225s - loss: 0.2975 - acc: 0.9032 - val_loss: 0.5020 - val_acc: 0.8726
Epoch 54/500
225s - loss: 0.3226 - acc: 0.8939 - val_loss: 0.5555 - val_acc: 0.8632
Epoch 55/500
226s - loss: 0.3146 - acc: 0.8932 - val_loss: 0.5174 - val_acc: 0.8705
Epoch 56/500
227s - loss: 0.2869 - acc: 0.9003 - val_loss: 0.4953 - val_acc: 0.8779
Epoch 57/500
225s - loss: 0.3049 - acc: 0.8995 - val_loss: 0.5066 - val_acc: 0.8811
Epoch 58/500
225s - loss: 0.2855 - acc: 0.9111 - val_loss: 0.5238 - val_acc: 0.8758
Epoch 59/500
225s - loss: 0.2744 - acc: 0.9071 - val_loss: 0.5875 - val_acc: 0.8684
Epoch 60/500
225s - loss: 0.2852 - acc: 0.9097 - val_loss: 0.4581 - val_acc: 0.8811
Epoch 61/500
226s - loss: 0.2919 - acc: 0.9058 - val_loss: 0.5290 - val_acc: 0.8642
Epoch 62/500
227s - loss: 0.2962 - acc: 0.9103 - val_loss: 0.5129 - val_acc: 0.8705
Epoch 63/500
227s - loss: 0.2932 - acc: 0.9092 - val_loss: 0.5671 - val_acc: 0.8653
Epoch 64/500
227s - loss: 0.2773 - acc: 0.9079 - val_loss: 0.5492 - val_acc: 0.8716
Epoch 65/500
227s - loss: 0.2625 - acc: 0.9163 - val_loss: 0.6258 - val_acc: 0.8547
Epoch 66/500

Epoch 00065: reducing learning rate to 0.000999999977648.
227s - loss: 0.2564 - acc: 0.9129 - val_loss: 0.4978 - val_acc: 0.8726
Epoch 67/500
227s - loss: 0.2406 - acc: 0.9158 - val_loss: 0.5310 - val_acc: 0.8684
Epoch 68/500
227s - loss: 0.2408 - acc: 0.9211 - val_loss: 0.5214 - val_acc: 0.8705
Epoch 69/500
227s - loss: 0.2516 - acc: 0.9189 - val_loss: 0.5199 - val_acc: 0.8726
Epoch 70/500
227s - loss: 0.2299 - acc: 0.9189 - val_loss: 0.5236 - val_acc: 0.8674
Epoch 71/500
227s - loss: 0.2310 - acc: 0.9221 - val_loss: 0.5127 - val_acc: 0.8737
Epoch 72/500
227s - loss: 0.2228 - acc: 0.9208 - val_loss: 0.5168 - val_acc: 0.8737
Epoch 73/500
227s - loss: 0.2633 - acc: 0.9182 - val_loss: 0.5296 - val_acc: 0.8684
Epoch 74/500

Epoch 00073: reducing learning rate to 9.99999931082e-05.
227s - loss: 0.2343 - acc: 0.9174 - val_loss: 0.5100 - val_acc: 0.8705
Epoch 75/500
227s - loss: 0.2410 - acc: 0.9216 - val_loss: 0.5092 - val_acc: 0.8726
Epoch 76/500
227s - loss: 0.2314 - acc: 0.9239 - val_loss: 0.5104 - val_acc: 0.8716
Epoch 77/500
228s - loss: 0.2347 - acc: 0.9245 - val_loss: 0.5139 - val_acc: 0.8737
Epoch 78/500
227s - loss: 0.2180 - acc: 0.9192 - val_loss: 0.5098 - val_acc: 0.8737
Epoch 79/500
227s - loss: 0.2425 - acc: 0.9221 - val_loss: 0.5176 - val_acc: 0.8726
Epoch 80/500
227s - loss: 0.2354 - acc: 0.9216 - val_loss: 0.5142 - val_acc: 0.8737
Epoch 81/500
228s - loss: 0.2385 - acc: 0.9239 - val_loss: 0.5070 - val_acc: 0.8737
Epoch 82/500

Epoch 00081: reducing learning rate to 9.99999901978e-06.
228s - loss: 0.2385 - acc: 0.9192 - val_loss: 0.5089 - val_acc: 0.8747
Epoch 83/500
227s - loss: 0.2383 - acc: 0.9213 - val_loss: 0.5116 - val_acc: 0.8737
Epoch 84/500
226s - loss: 0.2380 - acc: 0.9192 - val_loss: 0.5123 - val_acc: 0.8737
Epoch 85/500
225s - loss: 0.2418 - acc: 0.9184 - val_loss: 0.5064 - val_acc: 0.8747
Epoch 86/500
225s - loss: 0.2402 - acc: 0.9211 - val_loss: 0.5127 - val_acc: 0.8747
Epoch 87/500
227s - loss: 0.2277 - acc: 0.9239 - val_loss: 0.5140 - val_acc: 0.8716
Epoch 88/500
227s - loss: 0.2411 - acc: 0.9211 - val_loss: 0.5064 - val_acc: 0.8747
Epoch 89/500
227s - loss: 0.2346 - acc: 0.9224 - val_loss: 0.5106 - val_acc: 0.8737
Epoch 90/500

Epoch 00089: reducing learning rate to 1e-06.
226s - loss: 0.2408 - acc: 0.9232 - val_loss: 0.5142 - val_acc: 0.8737
Epoch 91/500
227s - loss: 0.2274 - acc: 0.9216 - val_loss: 0.5189 - val_acc: 0.8726
Epoch 92/500
226s - loss: 0.2368 - acc: 0.9237 - val_loss: 0.5088 - val_acc: 0.8737
Epoch 93/500
227s - loss: 0.2480 - acc: 0.9176 - val_loss: 0.5076 - val_acc: 0.8737
Epoch 94/500
227s - loss: 0.2417 - acc: 0.9208 - val_loss: 0.5172 - val_acc: 0.8737
Epoch 95/500
227s - loss: 0.2311 - acc: 0.9187 - val_loss: 0.5123 - val_acc: 0.8726
Epoch 96/500
227s - loss: 0.2248 - acc: 0.9184 - val_loss: 0.5125 - val_acc: 0.8737
Epoch 97/500
227s - loss: 0.2255 - acc: 0.9268 - val_loss: 0.5088 - val_acc: 0.8747
Epoch 98/500
227s - loss: 0.2346 - acc: 0.9216 - val_loss: 0.5102 - val_acc: 0.8737
Epoch 99/500
227s - loss: 0.2412 - acc: 0.9218 - val_loss: 0.5136 - val_acc: 0.8737
Epoch 100/500
228s - loss: 0.2495 - acc: 0.9184 - val_loss: 0.5121 - val_acc: 0.8716
Epoch 101/500
229s - loss: 0.2237 - acc: 0.9229 - val_loss: 0.5173 - val_acc: 0.8737
Epoch 102/500
228s - loss: 0.2389 - acc: 0.9205 - val_loss: 0.5168 - val_acc: 0.8747
Epoch 103/500
227s - loss: 0.2520 - acc: 0.9171 - val_loss: 0.5079 - val_acc: 0.8737
Epoch 104/500
227s - loss: 0.2104 - acc: 0.9266 - val_loss: 0.5134 - val_acc: 0.8716
Epoch 105/500
228s - loss: 0.2433 - acc: 0.9171 - val_loss: 0.5104 - val_acc: 0.8726
Epoch 106/500
228s - loss: 0.2500 - acc: 0.9168 - val_loss: 0.5101 - val_acc: 0.8737
Epoch 107/500
227s - loss: 0.2343 - acc: 0.9232 - val_loss: 0.5097 - val_acc: 0.8726
Epoch 108/500
227s - loss: 0.2221 - acc: 0.9237 - val_loss: 0.5107 - val_acc: 0.8716
Epoch 109/500
228s - loss: 0.2360 - acc: 0.9232 - val_loss: 0.5137 - val_acc: 0.8737
Epoch 110/500
228s - loss: 0.2455 - acc: 0.9195 - val_loss: 0.5074 - val_acc: 0.8747
Epoch 111/500
228s - loss: 0.2445 - acc: 0.9182 - val_loss: 0.5100 - val_acc: 0.8747
Epoch 112/500
227s - loss: 0.2701 - acc: 0.9179 - val_loss: 0.5064 - val_acc: 0.8768
Epoch 113/500
226s - loss: 0.2231 - acc: 0.9274 - val_loss: 0.5136 - val_acc: 0.8726
Epoch 114/500
225s - loss: 0.2447 - acc: 0.9174 - val_loss: 0.5104 - val_acc: 0.8737
Epoch 115/500
225s - loss: 0.2249 - acc: 0.9232 - val_loss: 0.5137 - val_acc: 0.8726
Epoch 116/500
225s - loss: 0.2359 - acc: 0.9208 - val_loss: 0.5115 - val_acc: 0.8758
Epoch 117/500
226s - loss: 0.2279 - acc: 0.9229 - val_loss: 0.5136 - val_acc: 0.8737
Epoch 118/500
227s - loss: 0.2429 - acc: 0.9179 - val_loss: 0.5096 - val_acc: 0.8716
Epoch 119/500
226s - loss: 0.2383 - acc: 0.9174 - val_loss: 0.5148 - val_acc: 0.8716
Epoch 120/500
226s - loss: 0.2145 - acc: 0.9274 - val_loss: 0.5136 - val_acc: 0.8726
Epoch 121/500
227s - loss: 0.2512 - acc: 0.9189 - val_loss: 0.5097 - val_acc: 0.8726
Training loss for fold 4 is 0.16075858787486427 with percent 93.39473684210526
Testing loss for fold 4 is 0.5066385729689347 with percent 88.1052631202497
 
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_11 (InputLayer)            (None, 75, 75, 3)     0                                            
____________________________________________________________________________________________________
conv2d_31 (Conv2D)               (None, 73, 73, 16)    448         input_11[0][0]                   
____________________________________________________________________________________________________
batch_normalization_36 (BatchNor (None, 73, 73, 16)    64          conv2d_31[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_36 (LeakyReLU)       (None, 73, 73, 16)    0           batch_normalization_36[0][0]     
____________________________________________________________________________________________________
conv2d_32 (Conv2D)               (None, 72, 72, 16)    1040        leaky_re_lu_36[0][0]             
____________________________________________________________________________________________________
batch_normalization_37 (BatchNor (None, 72, 72, 16)    64          conv2d_32[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_37 (LeakyReLU)       (None, 72, 72, 16)    0           batch_normalization_37[0][0]     
____________________________________________________________________________________________________
max_pooling2d_16 (MaxPooling2D)  (None, 36, 36, 16)    0           leaky_re_lu_37[0][0]             
____________________________________________________________________________________________________
dropout_26 (Dropout)             (None, 36, 36, 16)    0           max_pooling2d_16[0][0]           
____________________________________________________________________________________________________
conv2d_33 (Conv2D)               (None, 34, 34, 32)    4640        dropout_26[0][0]                 
____________________________________________________________________________________________________
batch_normalization_38 (BatchNor (None, 34, 34, 32)    128         conv2d_33[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_38 (LeakyReLU)       (None, 34, 34, 32)    0           batch_normalization_38[0][0]     
____________________________________________________________________________________________________
conv2d_34 (Conv2D)               (None, 33, 33, 32)    4128        leaky_re_lu_38[0][0]             
____________________________________________________________________________________________________
batch_normalization_39 (BatchNor (None, 33, 33, 32)    128         conv2d_34[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_39 (LeakyReLU)       (None, 33, 33, 32)    0           batch_normalization_39[0][0]     
____________________________________________________________________________________________________
max_pooling2d_17 (MaxPooling2D)  (None, 16, 16, 32)    0           leaky_re_lu_39[0][0]             
____________________________________________________________________________________________________
dropout_27 (Dropout)             (None, 16, 16, 32)    0           max_pooling2d_17[0][0]           
____________________________________________________________________________________________________
conv2d_35 (Conv2D)               (None, 14, 14, 64)    18496       dropout_27[0][0]                 
____________________________________________________________________________________________________
batch_normalization_40 (BatchNor (None, 14, 14, 64)    256         conv2d_35[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_40 (LeakyReLU)       (None, 14, 14, 64)    0           batch_normalization_40[0][0]     
____________________________________________________________________________________________________
conv2d_36 (Conv2D)               (None, 13, 13, 64)    16448       leaky_re_lu_40[0][0]             
____________________________________________________________________________________________________
batch_normalization_41 (BatchNor (None, 13, 13, 64)    256         conv2d_36[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_41 (LeakyReLU)       (None, 13, 13, 64)    0           batch_normalization_41[0][0]     
____________________________________________________________________________________________________
max_pooling2d_18 (MaxPooling2D)  (None, 6, 6, 64)      0           leaky_re_lu_41[0][0]             
____________________________________________________________________________________________________
dropout_28 (Dropout)             (None, 6, 6, 64)      0           max_pooling2d_18[0][0]           
____________________________________________________________________________________________________
flatten_6 (Flatten)              (None, 2304)          0           dropout_28[0][0]                 
____________________________________________________________________________________________________
batch_normalization_42 (BatchNor (None, 2304)          9216        flatten_6[0][0]                  
____________________________________________________________________________________________________
input_12 (InputLayer)            (None, 102)           0                                            
____________________________________________________________________________________________________
concatenate_6 (Concatenate)      (None, 2406)          0           batch_normalization_42[0][0]     
                                                                   input_12[0][0]                   
____________________________________________________________________________________________________
dense_21 (Dense)                 (None, 128)           308096      concatenate_6[0][0]              
____________________________________________________________________________________________________
leaky_re_lu_42 (LeakyReLU)       (None, 128)           0           dense_21[0][0]                   
____________________________________________________________________________________________________
dense_22 (Dense)                 (None, 64)            8256        leaky_re_lu_42[0][0]             
____________________________________________________________________________________________________
dropout_29 (Dropout)             (None, 64)            0           dense_22[0][0]                   
____________________________________________________________________________________________________
dense_23 (Dense)                 (None, 32)            2080        dropout_29[0][0]                 
____________________________________________________________________________________________________
dropout_30 (Dropout)             (None, 32)            0           dense_23[0][0]                   
____________________________________________________________________________________________________
dense_24 (Dense)                 (None, 12)            396         dropout_30[0][0]                 
====================================================================================================
Total params: 374,140
Trainable params: 369,084
Non-trainable params: 5,056
____________________________________________________________________________________________________
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
232s - loss: 2.0194 - acc: 0.3092 - val_loss: 5.7595 - val_acc: 0.1379
Epoch 2/500
227s - loss: 1.6109 - acc: 0.4597 - val_loss: 2.9382 - val_acc: 0.3063
Epoch 3/500
229s - loss: 1.4855 - acc: 0.5079 - val_loss: 2.1066 - val_acc: 0.4716
Epoch 4/500
229s - loss: 1.3884 - acc: 0.5537 - val_loss: 6.0126 - val_acc: 0.2442
Epoch 5/500
230s - loss: 1.3672 - acc: 0.5589 - val_loss: 1.9193 - val_acc: 0.4674
Epoch 6/500
230s - loss: 1.3071 - acc: 0.5913 - val_loss: 1.5107 - val_acc: 0.5547
Epoch 7/500
227s - loss: 1.2702 - acc: 0.6161 - val_loss: 5.4511 - val_acc: 0.2347
Epoch 8/500
228s - loss: 1.2716 - acc: 0.6171 - val_loss: 1.9824 - val_acc: 0.4295
Epoch 9/500
230s - loss: 1.2142 - acc: 0.6434 - val_loss: 1.7722 - val_acc: 0.4863
Epoch 10/500
230s - loss: 1.1885 - acc: 0.6645 - val_loss: 1.1820 - val_acc: 0.6800
Epoch 11/500
227s - loss: 1.1543 - acc: 0.6684 - val_loss: 2.0235 - val_acc: 0.5789
Epoch 12/500
228s - loss: 1.1686 - acc: 0.6534 - val_loss: 1.5928 - val_acc: 0.6147
Epoch 13/500
228s - loss: 1.0898 - acc: 0.6792 - val_loss: 3.0633 - val_acc: 0.2947
Epoch 14/500
229s - loss: 1.1480 - acc: 0.6916 - val_loss: 0.9915 - val_acc: 0.7253
Epoch 15/500
227s - loss: 1.1549 - acc: 0.6726 - val_loss: 1.0551 - val_acc: 0.7274
Epoch 16/500
229s - loss: 1.1076 - acc: 0.6845 - val_loss: 3.1195 - val_acc: 0.2895
Epoch 17/500
229s - loss: 1.0982 - acc: 0.6868 - val_loss: 1.8594 - val_acc: 0.6284
Epoch 18/500
230s - loss: 1.1328 - acc: 0.6903 - val_loss: 1.6387 - val_acc: 0.6137
Epoch 19/500
229s - loss: 1.0947 - acc: 0.6963 - val_loss: 1.0485 - val_acc: 0.7463
Epoch 20/500
229s - loss: 1.0772 - acc: 0.7113 - val_loss: 2.5207 - val_acc: 0.4821
Epoch 21/500
227s - loss: 1.0584 - acc: 0.7118 - val_loss: 1.6777 - val_acc: 0.6495
Epoch 22/500
228s - loss: 1.1320 - acc: 0.6976 - val_loss: 1.3207 - val_acc: 0.6853
Epoch 23/500
228s - loss: 1.0332 - acc: 0.7213 - val_loss: 2.5505 - val_acc: 0.5442
Epoch 24/500
227s - loss: 1.0600 - acc: 0.7324 - val_loss: 1.2147 - val_acc: 0.7011
Epoch 25/500
229s - loss: 1.0889 - acc: 0.7132 - val_loss: 0.9968 - val_acc: 0.7358
Epoch 26/500
228s - loss: 1.0191 - acc: 0.7329 - val_loss: 3.9406 - val_acc: 0.4032
Epoch 27/500
228s - loss: 1.1242 - acc: 0.7400 - val_loss: 2.4825 - val_acc: 0.6021
Epoch 28/500

Epoch 00027: reducing learning rate to 0.010000000149.
229s - loss: 1.1948 - acc: 0.7182 - val_loss: 1.3975 - val_acc: 0.6768
Epoch 29/500
228s - loss: 0.9340 - acc: 0.7566 - val_loss: 1.0180 - val_acc: 0.7653
Epoch 30/500
226s - loss: 0.7485 - acc: 0.7908 - val_loss: 0.9268 - val_acc: 0.7842
Epoch 31/500
226s - loss: 0.7095 - acc: 0.8026 - val_loss: 0.8854 - val_acc: 0.7779
Epoch 32/500
227s - loss: 0.6525 - acc: 0.8145 - val_loss: 0.9523 - val_acc: 0.7800
Epoch 33/500
227s - loss: 0.6576 - acc: 0.8095 - val_loss: 0.8499 - val_acc: 0.7895
Epoch 34/500
226s - loss: 0.5874 - acc: 0.8171 - val_loss: 0.8325 - val_acc: 0.7884
Epoch 35/500
226s - loss: 0.5829 - acc: 0.8192 - val_loss: 0.8615 - val_acc: 0.8158
Epoch 36/500
225s - loss: 0.5691 - acc: 0.8224 - val_loss: 0.8200 - val_acc: 0.8116
Epoch 37/500
227s - loss: 0.5793 - acc: 0.8200 - val_loss: 0.8072 - val_acc: 0.7926
Epoch 38/500
227s - loss: 0.5623 - acc: 0.8313 - val_loss: 0.8389 - val_acc: 0.7832
Epoch 39/500
227s - loss: 0.4940 - acc: 0.8392 - val_loss: 0.8653 - val_acc: 0.8242
Epoch 40/500
226s - loss: 0.5111 - acc: 0.8395 - val_loss: 0.7910 - val_acc: 0.8379
Epoch 41/500
227s - loss: 0.4662 - acc: 0.8487 - val_loss: 0.7911 - val_acc: 0.8347
Epoch 42/500
227s - loss: 0.5140 - acc: 0.8389 - val_loss: 0.7707 - val_acc: 0.8337
Epoch 43/500
228s - loss: 0.5049 - acc: 0.8450 - val_loss: 0.7563 - val_acc: 0.8389
Epoch 44/500
225s - loss: 0.4739 - acc: 0.8479 - val_loss: 0.7882 - val_acc: 0.8347
Epoch 45/500
227s - loss: 0.4885 - acc: 0.8432 - val_loss: 0.8074 - val_acc: 0.8347
Epoch 46/500
228s - loss: 0.4652 - acc: 0.8403 - val_loss: 0.8386 - val_acc: 0.8316
Epoch 47/500
229s - loss: 0.4464 - acc: 0.8497 - val_loss: 0.8531 - val_acc: 0.8432
Epoch 48/500
227s - loss: 0.4298 - acc: 0.8505 - val_loss: 0.7459 - val_acc: 0.8505
Epoch 49/500
226s - loss: 0.4439 - acc: 0.8471 - val_loss: 0.7859 - val_acc: 0.8432
Epoch 50/500
228s - loss: 0.4239 - acc: 0.8563 - val_loss: 0.7930 - val_acc: 0.8474
Epoch 51/500
227s - loss: 0.4212 - acc: 0.8597 - val_loss: 0.8162 - val_acc: 0.8368
Epoch 52/500
225s - loss: 0.4533 - acc: 0.8487 - val_loss: 0.7871 - val_acc: 0.8442
Epoch 53/500
227s - loss: 0.4104 - acc: 0.8592 - val_loss: 0.7878 - val_acc: 0.8421
Epoch 54/500
228s - loss: 0.3963 - acc: 0.8597 - val_loss: 0.7649 - val_acc: 0.8442
Epoch 55/500
227s - loss: 0.3994 - acc: 0.8592 - val_loss: 0.7303 - val_acc: 0.8516
Epoch 56/500
225s - loss: 0.3852 - acc: 0.8724 - val_loss: 0.9572 - val_acc: 0.8253
Epoch 57/500
226s - loss: 0.3671 - acc: 0.8663 - val_loss: 0.8012 - val_acc: 0.8463
Epoch 58/500
227s - loss: 0.3780 - acc: 0.8658 - val_loss: 0.8904 - val_acc: 0.8484
Epoch 59/500
228s - loss: 0.3857 - acc: 0.8695 - val_loss: 0.8391 - val_acc: 0.8568
Epoch 60/500
227s - loss: 0.3747 - acc: 0.8692 - val_loss: 0.8408 - val_acc: 0.8400
Epoch 61/500
228s - loss: 0.3880 - acc: 0.8721 - val_loss: 0.8940 - val_acc: 0.8411
Epoch 62/500
228s - loss: 0.3539 - acc: 0.8684 - val_loss: 0.7991 - val_acc: 0.8516
Epoch 63/500
229s - loss: 0.3778 - acc: 0.8689 - val_loss: 0.7901 - val_acc: 0.8495
Epoch 64/500
228s - loss: 0.3912 - acc: 0.8647 - val_loss: 0.8899 - val_acc: 0.8368
Epoch 65/500
228s - loss: 0.3718 - acc: 0.8708 - val_loss: 0.7533 - val_acc: 0.8463
Epoch 66/500
228s - loss: 0.3652 - acc: 0.8637 - val_loss: 0.7510 - val_acc: 0.8547
Epoch 67/500
228s - loss: 0.3667 - acc: 0.8732 - val_loss: 0.8352 - val_acc: 0.8463
Epoch 68/500

Epoch 00067: reducing learning rate to 0.000999999977648.
228s - loss: 0.3713 - acc: 0.8739 - val_loss: 1.0065 - val_acc: 0.8305
Epoch 69/500
228s - loss: 0.3733 - acc: 0.8726 - val_loss: 0.8589 - val_acc: 0.8474
Epoch 70/500
229s - loss: 0.3587 - acc: 0.8734 - val_loss: 0.8324 - val_acc: 0.8463
Epoch 71/500
228s - loss: 0.3275 - acc: 0.8800 - val_loss: 0.8224 - val_acc: 0.8463
Epoch 72/500
228s - loss: 0.3585 - acc: 0.8829 - val_loss: 0.8208 - val_acc: 0.8495
Epoch 73/500
229s - loss: 0.3536 - acc: 0.8758 - val_loss: 0.8231 - val_acc: 0.8537
Epoch 74/500
228s - loss: 0.3393 - acc: 0.8761 - val_loss: 0.8126 - val_acc: 0.8537
Epoch 75/500
228s - loss: 0.3279 - acc: 0.8739 - val_loss: 0.8165 - val_acc: 0.8537
Epoch 76/500

Epoch 00075: reducing learning rate to 9.99999931082e-05.
228s - loss: 0.3518 - acc: 0.8779 - val_loss: 0.8134 - val_acc: 0.8537
Epoch 77/500
228s - loss: 0.3326 - acc: 0.8803 - val_loss: 0.8116 - val_acc: 0.8526
Epoch 78/500
228s - loss: 0.3386 - acc: 0.8787 - val_loss: 0.8122 - val_acc: 0.8537
Epoch 79/500
229s - loss: 0.3338 - acc: 0.8821 - val_loss: 0.8113 - val_acc: 0.8558
Epoch 80/500
226s - loss: 0.3555 - acc: 0.8776 - val_loss: 0.8120 - val_acc: 0.8558
Epoch 81/500
226s - loss: 0.3599 - acc: 0.8758 - val_loss: 0.8127 - val_acc: 0.8547
Epoch 82/500
227s - loss: 0.3167 - acc: 0.8787 - val_loss: 0.8164 - val_acc: 0.8537
Epoch 83/500
227s - loss: 0.3368 - acc: 0.8779 - val_loss: 0.8135 - val_acc: 0.8526
Epoch 84/500

Epoch 00083: reducing learning rate to 9.99999901978e-06.
227s - loss: 0.3581 - acc: 0.8776 - val_loss: 0.8118 - val_acc: 0.8526
Epoch 85/500
228s - loss: 0.3531 - acc: 0.8763 - val_loss: 0.8164 - val_acc: 0.8526
Epoch 86/500
228s - loss: 0.3206 - acc: 0.8824 - val_loss: 0.8175 - val_acc: 0.8526
Epoch 87/500
226s - loss: 0.3486 - acc: 0.8816 - val_loss: 0.8170 - val_acc: 0.8526
Epoch 88/500
228s - loss: 0.3377 - acc: 0.8821 - val_loss: 0.8248 - val_acc: 0.8526
Epoch 89/500
228s - loss: 0.3346 - acc: 0.8755 - val_loss: 0.8180 - val_acc: 0.8526
Epoch 90/500
228s - loss: 0.3325 - acc: 0.8808 - val_loss: 0.8187 - val_acc: 0.8526
Epoch 91/500
228s - loss: 0.3403 - acc: 0.8755 - val_loss: 0.8182 - val_acc: 0.8526
Epoch 92/500

Epoch 00091: reducing learning rate to 1e-06.
228s - loss: 0.3236 - acc: 0.8818 - val_loss: 0.8231 - val_acc: 0.8526
Epoch 93/500
228s - loss: 0.3316 - acc: 0.8855 - val_loss: 0.8178 - val_acc: 0.8537
Epoch 94/500
228s - loss: 0.3351 - acc: 0.8758 - val_loss: 0.8147 - val_acc: 0.8516
Epoch 95/500
228s - loss: 0.3193 - acc: 0.8842 - val_loss: 0.8148 - val_acc: 0.8526
Epoch 96/500
228s - loss: 0.3275 - acc: 0.8782 - val_loss: 0.8143 - val_acc: 0.8526
Epoch 97/500
228s - loss: 0.3493 - acc: 0.8766 - val_loss: 0.8202 - val_acc: 0.8526
Epoch 98/500
228s - loss: 0.3538 - acc: 0.8766 - val_loss: 0.8128 - val_acc: 0.8516
Epoch 99/500
228s - loss: 0.3618 - acc: 0.8753 - val_loss: 0.8095 - val_acc: 0.8516
Epoch 100/500
228s - loss: 0.3442 - acc: 0.8771 - val_loss: 0.8094 - val_acc: 0.8537
Epoch 101/500
228s - loss: 0.3417 - acc: 0.8766 - val_loss: 0.8167 - val_acc: 0.8558
Epoch 102/500
227s - loss: 0.3431 - acc: 0.8668 - val_loss: 0.8138 - val_acc: 0.8547
Epoch 103/500
228s - loss: 0.3516 - acc: 0.8724 - val_loss: 0.8197 - val_acc: 0.8526
Epoch 104/500
228s - loss: 0.3645 - acc: 0.8708 - val_loss: 0.8212 - val_acc: 0.8516
Epoch 105/500
228s - loss: 0.3284 - acc: 0.8797 - val_loss: 0.8173 - val_acc: 0.8526
Epoch 106/500
228s - loss: 0.3382 - acc: 0.8771 - val_loss: 0.8051 - val_acc: 0.8537
Epoch 107/500
228s - loss: 0.3257 - acc: 0.8742 - val_loss: 0.8129 - val_acc: 0.8516
Epoch 108/500
228s - loss: 0.3297 - acc: 0.8821 - val_loss: 0.8157 - val_acc: 0.8537
Epoch 109/500
227s - loss: 0.3532 - acc: 0.8758 - val_loss: 0.8159 - val_acc: 0.8516
Epoch 110/500
227s - loss: 0.3288 - acc: 0.8813 - val_loss: 0.8152 - val_acc: 0.8526
Epoch 111/500
228s - loss: 0.3397 - acc: 0.8779 - val_loss: 0.8081 - val_acc: 0.8537
Epoch 112/500
228s - loss: 0.3560 - acc: 0.8745 - val_loss: 0.8099 - val_acc: 0.8537
Epoch 113/500
228s - loss: 0.3431 - acc: 0.8724 - val_loss: 0.8178 - val_acc: 0.8526
Epoch 114/500
228s - loss: 0.3126 - acc: 0.8847 - val_loss: 0.8109 - val_acc: 0.8537
Epoch 115/500
228s - loss: 0.3367 - acc: 0.8837 - val_loss: 0.8133 - val_acc: 0.8516
Epoch 116/500
228s - loss: 0.3350 - acc: 0.8705 - val_loss: 0.8095 - val_acc: 0.8516
Training loss for fold 5 is 0.20572622738386456 with percent 93.3157894736842
Testing loss for fold 5 is 0.8390570461122613 with percent 85.68421058905751
 
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_13 (InputLayer)            (None, 75, 75, 3)     0                                            
____________________________________________________________________________________________________
conv2d_37 (Conv2D)               (None, 73, 73, 16)    448         input_13[0][0]                   
____________________________________________________________________________________________________
batch_normalization_43 (BatchNor (None, 73, 73, 16)    64          conv2d_37[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_43 (LeakyReLU)       (None, 73, 73, 16)    0           batch_normalization_43[0][0]     
____________________________________________________________________________________________________
conv2d_38 (Conv2D)               (None, 72, 72, 16)    1040        leaky_re_lu_43[0][0]             
____________________________________________________________________________________________________
batch_normalization_44 (BatchNor (None, 72, 72, 16)    64          conv2d_38[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_44 (LeakyReLU)       (None, 72, 72, 16)    0           batch_normalization_44[0][0]     
____________________________________________________________________________________________________
max_pooling2d_19 (MaxPooling2D)  (None, 36, 36, 16)    0           leaky_re_lu_44[0][0]             
____________________________________________________________________________________________________
dropout_31 (Dropout)             (None, 36, 36, 16)    0           max_pooling2d_19[0][0]           
____________________________________________________________________________________________________
conv2d_39 (Conv2D)               (None, 34, 34, 32)    4640        dropout_31[0][0]                 
____________________________________________________________________________________________________
batch_normalization_45 (BatchNor (None, 34, 34, 32)    128         conv2d_39[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_45 (LeakyReLU)       (None, 34, 34, 32)    0           batch_normalization_45[0][0]     
____________________________________________________________________________________________________
conv2d_40 (Conv2D)               (None, 33, 33, 32)    4128        leaky_re_lu_45[0][0]             
____________________________________________________________________________________________________
batch_normalization_46 (BatchNor (None, 33, 33, 32)    128         conv2d_40[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_46 (LeakyReLU)       (None, 33, 33, 32)    0           batch_normalization_46[0][0]     
____________________________________________________________________________________________________
max_pooling2d_20 (MaxPooling2D)  (None, 16, 16, 32)    0           leaky_re_lu_46[0][0]             
____________________________________________________________________________________________________
dropout_32 (Dropout)             (None, 16, 16, 32)    0           max_pooling2d_20[0][0]           
____________________________________________________________________________________________________
conv2d_41 (Conv2D)               (None, 14, 14, 64)    18496       dropout_32[0][0]                 
____________________________________________________________________________________________________
batch_normalization_47 (BatchNor (None, 14, 14, 64)    256         conv2d_41[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_47 (LeakyReLU)       (None, 14, 14, 64)    0           batch_normalization_47[0][0]     
____________________________________________________________________________________________________
conv2d_42 (Conv2D)               (None, 13, 13, 64)    16448       leaky_re_lu_47[0][0]             
____________________________________________________________________________________________________
batch_normalization_48 (BatchNor (None, 13, 13, 64)    256         conv2d_42[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_48 (LeakyReLU)       (None, 13, 13, 64)    0           batch_normalization_48[0][0]     
____________________________________________________________________________________________________
max_pooling2d_21 (MaxPooling2D)  (None, 6, 6, 64)      0           leaky_re_lu_48[0][0]             
____________________________________________________________________________________________________
dropout_33 (Dropout)             (None, 6, 6, 64)      0           max_pooling2d_21[0][0]           
____________________________________________________________________________________________________
flatten_7 (Flatten)              (None, 2304)          0           dropout_33[0][0]                 
____________________________________________________________________________________________________
batch_normalization_49 (BatchNor (None, 2304)          9216        flatten_7[0][0]                  
____________________________________________________________________________________________________
input_14 (InputLayer)            (None, 102)           0                                            
____________________________________________________________________________________________________
concatenate_7 (Concatenate)      (None, 2406)          0           batch_normalization_49[0][0]     
                                                                   input_14[0][0]                   
____________________________________________________________________________________________________
dense_25 (Dense)                 (None, 128)           308096      concatenate_7[0][0]              
____________________________________________________________________________________________________
leaky_re_lu_49 (LeakyReLU)       (None, 128)           0           dense_25[0][0]                   
____________________________________________________________________________________________________
dense_26 (Dense)                 (None, 64)            8256        leaky_re_lu_49[0][0]             
____________________________________________________________________________________________________
dropout_34 (Dropout)             (None, 64)            0           dense_26[0][0]                   
____________________________________________________________________________________________________
dense_27 (Dense)                 (None, 32)            2080        dropout_34[0][0]                 
____________________________________________________________________________________________________
dropout_35 (Dropout)             (None, 32)            0           dense_27[0][0]                   
____________________________________________________________________________________________________
dense_28 (Dense)                 (None, 12)            396         dropout_35[0][0]                 
====================================================================================================
Total params: 374,140
Trainable params: 369,084
Non-trainable params: 5,056
____________________________________________________________________________________________________
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
231s - loss: 1.9473 - acc: 0.3424 - val_loss: 2.4630 - val_acc: 0.2116
Epoch 2/500
226s - loss: 1.3975 - acc: 0.5442 - val_loss: 3.3401 - val_acc: 0.2800
Epoch 3/500
226s - loss: 1.2970 - acc: 0.6000 - val_loss: 1.5437 - val_acc: 0.5305
Epoch 4/500
227s - loss: 1.1696 - acc: 0.6424 - val_loss: 1.4805 - val_acc: 0.5758
Epoch 5/500
228s - loss: 1.0806 - acc: 0.6821 - val_loss: 2.2323 - val_acc: 0.4716
Epoch 6/500
229s - loss: 1.0454 - acc: 0.6850 - val_loss: 3.6292 - val_acc: 0.2937
Epoch 7/500
229s - loss: 1.0608 - acc: 0.7008 - val_loss: 2.1754 - val_acc: 0.4726
Epoch 8/500
228s - loss: 1.0348 - acc: 0.7053 - val_loss: 1.4381 - val_acc: 0.6484
Epoch 9/500
226s - loss: 0.9965 - acc: 0.7297 - val_loss: 1.0948 - val_acc: 0.7221
Epoch 10/500
229s - loss: 0.9760 - acc: 0.7308 - val_loss: 1.5591 - val_acc: 0.6432
Epoch 11/500
229s - loss: 0.9444 - acc: 0.7416 - val_loss: 1.4450 - val_acc: 0.7105
Epoch 12/500
229s - loss: 0.9544 - acc: 0.7500 - val_loss: 1.2690 - val_acc: 0.7126
Epoch 13/500
226s - loss: 0.9652 - acc: 0.7400 - val_loss: 2.7230 - val_acc: 0.4232
Epoch 14/500
226s - loss: 0.9225 - acc: 0.7632 - val_loss: 2.2934 - val_acc: 0.5379
Epoch 15/500
226s - loss: 0.8968 - acc: 0.7721 - val_loss: 1.6187 - val_acc: 0.6800
Epoch 16/500
226s - loss: 0.8875 - acc: 0.7705 - val_loss: 1.2038 - val_acc: 0.7379
Epoch 17/500
227s - loss: 0.9486 - acc: 0.7732 - val_loss: 1.1785 - val_acc: 0.7495
Epoch 18/500
226s - loss: 0.9265 - acc: 0.7703 - val_loss: 1.0269 - val_acc: 0.7695
Epoch 19/500
226s - loss: 1.0008 - acc: 0.7905 - val_loss: 1.3557 - val_acc: 0.7347
Epoch 20/500
227s - loss: 0.9314 - acc: 0.7939 - val_loss: 1.6063 - val_acc: 0.6726
Epoch 21/500
228s - loss: 0.8694 - acc: 0.8034 - val_loss: 1.5799 - val_acc: 0.7084
Epoch 22/500
227s - loss: 0.9199 - acc: 0.7887 - val_loss: 1.2282 - val_acc: 0.7674
Epoch 23/500
225s - loss: 0.9794 - acc: 0.7955 - val_loss: 1.4196 - val_acc: 0.7211
Epoch 24/500
225s - loss: 0.9161 - acc: 0.7884 - val_loss: 1.6331 - val_acc: 0.6642
Epoch 25/500
226s - loss: 0.9408 - acc: 0.7913 - val_loss: 2.0811 - val_acc: 0.6884
Epoch 26/500
227s - loss: 0.8876 - acc: 0.8050 - val_loss: 0.9199 - val_acc: 0.8084
Epoch 27/500
225s - loss: 0.9332 - acc: 0.7758 - val_loss: 2.0170 - val_acc: 0.6032
Epoch 28/500
225s - loss: 0.9699 - acc: 0.7816 - val_loss: 2.4703 - val_acc: 0.5558
Epoch 29/500
226s - loss: 0.9772 - acc: 0.7789 - val_loss: 1.3138 - val_acc: 0.7832
Epoch 30/500
227s - loss: 0.9174 - acc: 0.8021 - val_loss: 2.1211 - val_acc: 0.6832
Epoch 31/500
227s - loss: 1.0653 - acc: 0.7718 - val_loss: 1.6156 - val_acc: 0.7632
Epoch 32/500
227s - loss: 1.1824 - acc: 0.7376 - val_loss: 6.5011 - val_acc: 0.3432
Epoch 33/500
227s - loss: 1.0596 - acc: 0.7737 - val_loss: 2.1969 - val_acc: 0.5147
Epoch 34/500
228s - loss: 1.0776 - acc: 0.7800 - val_loss: 4.2711 - val_acc: 0.3411
Epoch 35/500

Epoch 00034: reducing learning rate to 0.010000000149.
228s - loss: 1.0386 - acc: 0.7708 - val_loss: 1.6816 - val_acc: 0.7168
Epoch 36/500
227s - loss: 0.9763 - acc: 0.8050 - val_loss: 0.9924 - val_acc: 0.8284
Epoch 37/500
228s - loss: 0.7060 - acc: 0.8361 - val_loss: 0.8623 - val_acc: 0.8389
Epoch 38/500
225s - loss: 0.6570 - acc: 0.8387 - val_loss: 0.9190 - val_acc: 0.8337
Epoch 39/500
226s - loss: 0.5929 - acc: 0.8482 - val_loss: 0.8761 - val_acc: 0.8284
Epoch 40/500
227s - loss: 0.5873 - acc: 0.8503 - val_loss: 0.8492 - val_acc: 0.8432
Epoch 41/500
226s - loss: 0.5914 - acc: 0.8582 - val_loss: 0.7961 - val_acc: 0.8505
Epoch 42/500
226s - loss: 0.5238 - acc: 0.8632 - val_loss: 0.7956 - val_acc: 0.8474
Epoch 43/500
227s - loss: 0.5096 - acc: 0.8603 - val_loss: 0.7790 - val_acc: 0.8463
Epoch 44/500
228s - loss: 0.5106 - acc: 0.8647 - val_loss: 0.8481 - val_acc: 0.8442
Epoch 45/500
227s - loss: 0.4926 - acc: 0.8650 - val_loss: 0.7825 - val_acc: 0.8568
Epoch 46/500
226s - loss: 0.5219 - acc: 0.8679 - val_loss: 0.7933 - val_acc: 0.8505
Epoch 47/500
227s - loss: 0.4949 - acc: 0.8721 - val_loss: 0.7209 - val_acc: 0.8589
Epoch 48/500
225s - loss: 0.5011 - acc: 0.8776 - val_loss: 0.7692 - val_acc: 0.8505
Epoch 49/500
227s - loss: 0.4465 - acc: 0.8818 - val_loss: 0.7671 - val_acc: 0.8537
Epoch 50/500
228s - loss: 0.4571 - acc: 0.8829 - val_loss: 0.7759 - val_acc: 0.8547
Epoch 51/500
227s - loss: 0.4421 - acc: 0.8826 - val_loss: 0.7917 - val_acc: 0.8579
Epoch 52/500
227s - loss: 0.4397 - acc: 0.8803 - val_loss: 0.7460 - val_acc: 0.8611
Epoch 53/500
225s - loss: 0.4478 - acc: 0.8805 - val_loss: 0.7586 - val_acc: 0.8568
Epoch 54/500
228s - loss: 0.3934 - acc: 0.8900 - val_loss: 0.7944 - val_acc: 0.8579
Epoch 55/500
229s - loss: 0.4176 - acc: 0.8887 - val_loss: 0.8054 - val_acc: 0.8600
Epoch 56/500
228s - loss: 0.4153 - acc: 0.8887 - val_loss: 0.7335 - val_acc: 0.8621
Epoch 57/500
225s - loss: 0.4072 - acc: 0.8905 - val_loss: 0.7183 - val_acc: 0.8589
Epoch 58/500
226s - loss: 0.4505 - acc: 0.8834 - val_loss: 0.7397 - val_acc: 0.8589
Epoch 59/500
228s - loss: 0.4072 - acc: 0.8847 - val_loss: 0.6812 - val_acc: 0.8653
Epoch 60/500
225s - loss: 0.3973 - acc: 0.8924 - val_loss: 0.7726 - val_acc: 0.8558
Epoch 61/500
225s - loss: 0.4121 - acc: 0.8921 - val_loss: 0.7263 - val_acc: 0.8621
Epoch 62/500
225s - loss: 0.3935 - acc: 0.8913 - val_loss: 0.6954 - val_acc: 0.8642
Epoch 63/500
226s - loss: 0.3854 - acc: 0.8950 - val_loss: 0.6745 - val_acc: 0.8653
Epoch 64/500
225s - loss: 0.3853 - acc: 0.8932 - val_loss: 0.7214 - val_acc: 0.8600
Epoch 65/500
225s - loss: 0.3889 - acc: 0.8892 - val_loss: 0.6710 - val_acc: 0.8632
Epoch 66/500
225s - loss: 0.3634 - acc: 0.8984 - val_loss: 0.6871 - val_acc: 0.8684
Epoch 67/500
227s - loss: 0.3309 - acc: 0.9021 - val_loss: 0.6710 - val_acc: 0.8726
Epoch 68/500
225s - loss: 0.3896 - acc: 0.8911 - val_loss: 0.6462 - val_acc: 0.8674
Epoch 69/500
225s - loss: 0.3759 - acc: 0.8947 - val_loss: 0.6560 - val_acc: 0.8726
Epoch 70/500
226s - loss: 0.3657 - acc: 0.9003 - val_loss: 0.7206 - val_acc: 0.8632
Epoch 71/500
227s - loss: 0.3661 - acc: 0.8929 - val_loss: 0.6973 - val_acc: 0.8674
Epoch 72/500
227s - loss: 0.3838 - acc: 0.8966 - val_loss: 0.6537 - val_acc: 0.8737
Epoch 73/500
225s - loss: 0.3365 - acc: 0.8963 - val_loss: 0.6823 - val_acc: 0.8611
Epoch 74/500
225s - loss: 0.3758 - acc: 0.8929 - val_loss: 0.6871 - val_acc: 0.8611
Epoch 75/500
227s - loss: 0.3418 - acc: 0.8987 - val_loss: 0.6204 - val_acc: 0.8726
Epoch 76/500
227s - loss: 0.3236 - acc: 0.9013 - val_loss: 0.5800 - val_acc: 0.8800
Epoch 77/500
225s - loss: 0.3249 - acc: 0.9063 - val_loss: 0.5936 - val_acc: 0.8821
Epoch 78/500
225s - loss: 0.3297 - acc: 0.9050 - val_loss: 0.6216 - val_acc: 0.8726
Epoch 79/500
225s - loss: 0.3340 - acc: 0.9068 - val_loss: 0.6269 - val_acc: 0.8737
Epoch 80/500
226s - loss: 0.3368 - acc: 0.9021 - val_loss: 0.6454 - val_acc: 0.8768
Epoch 81/500
225s - loss: 0.3123 - acc: 0.9050 - val_loss: 0.5954 - val_acc: 0.8768
Epoch 82/500
225s - loss: 0.3124 - acc: 0.9039 - val_loss: 0.6069 - val_acc: 0.8716
Epoch 83/500
225s - loss: 0.3593 - acc: 0.8997 - val_loss: 0.5993 - val_acc: 0.8737
Epoch 84/500
226s - loss: 0.3336 - acc: 0.9032 - val_loss: 0.6567 - val_acc: 0.8674
Epoch 85/500
227s - loss: 0.3438 - acc: 0.9076 - val_loss: 0.6548 - val_acc: 0.8684
Epoch 86/500

Epoch 00085: reducing learning rate to 0.000999999977648.
227s - loss: 0.2947 - acc: 0.9082 - val_loss: 0.6547 - val_acc: 0.8716
Epoch 87/500
227s - loss: 0.3064 - acc: 0.9063 - val_loss: 0.6432 - val_acc: 0.8716
Epoch 88/500
226s - loss: 0.2975 - acc: 0.9087 - val_loss: 0.6298 - val_acc: 0.8726
Epoch 89/500
221s - loss: 0.2994 - acc: 0.9068 - val_loss: 0.6246 - val_acc: 0.8716
Epoch 90/500
219s - loss: 0.3394 - acc: 0.9013 - val_loss: 0.6184 - val_acc: 0.8716
Epoch 91/500
219s - loss: 0.3136 - acc: 0.9089 - val_loss: 0.6256 - val_acc: 0.8716
Epoch 92/500
219s - loss: 0.3024 - acc: 0.9071 - val_loss: 0.6328 - val_acc: 0.8716
Epoch 93/500
219s - loss: 0.3223 - acc: 0.9095 - val_loss: 0.6207 - val_acc: 0.8737
Epoch 94/500

Epoch 00093: reducing learning rate to 9.99999931082e-05.
217s - loss: 0.3209 - acc: 0.9068 - val_loss: 0.6169 - val_acc: 0.8758
Epoch 95/500
216s - loss: 0.3163 - acc: 0.9084 - val_loss: 0.6172 - val_acc: 0.8768
Epoch 96/500
216s - loss: 0.3099 - acc: 0.9095 - val_loss: 0.6190 - val_acc: 0.8758
Epoch 97/500
216s - loss: 0.3330 - acc: 0.9032 - val_loss: 0.6171 - val_acc: 0.8758
Epoch 98/500
216s - loss: 0.2730 - acc: 0.9118 - val_loss: 0.6285 - val_acc: 0.8768
Epoch 99/500
216s - loss: 0.2907 - acc: 0.9097 - val_loss: 0.6260 - val_acc: 0.8758
Epoch 100/500
216s - loss: 0.2840 - acc: 0.9108 - val_loss: 0.6250 - val_acc: 0.8747
Epoch 101/500
216s - loss: 0.2998 - acc: 0.9084 - val_loss: 0.6268 - val_acc: 0.8747
Epoch 102/500

Epoch 00101: reducing learning rate to 9.99999901978e-06.
216s - loss: 0.3096 - acc: 0.9079 - val_loss: 0.6259 - val_acc: 0.8768
Epoch 103/500
216s - loss: 0.3067 - acc: 0.9063 - val_loss: 0.6255 - val_acc: 0.8768
Epoch 104/500
216s - loss: 0.3175 - acc: 0.9095 - val_loss: 0.6157 - val_acc: 0.8758
Epoch 105/500
216s - loss: 0.3035 - acc: 0.9097 - val_loss: 0.6251 - val_acc: 0.8779
Epoch 106/500
215s - loss: 0.3086 - acc: 0.9100 - val_loss: 0.6202 - val_acc: 0.8779
Epoch 107/500
216s - loss: 0.3045 - acc: 0.9045 - val_loss: 0.6158 - val_acc: 0.8779
Epoch 108/500
216s - loss: 0.2924 - acc: 0.9071 - val_loss: 0.6193 - val_acc: 0.8768
Epoch 109/500
216s - loss: 0.3050 - acc: 0.9055 - val_loss: 0.6263 - val_acc: 0.8768
Epoch 110/500

Epoch 00109: reducing learning rate to 1e-06.
216s - loss: 0.3246 - acc: 0.9045 - val_loss: 0.6164 - val_acc: 0.8779
Epoch 111/500
216s - loss: 0.3093 - acc: 0.9079 - val_loss: 0.6221 - val_acc: 0.8747
Epoch 112/500
216s - loss: 0.3014 - acc: 0.9103 - val_loss: 0.6170 - val_acc: 0.8758
Epoch 113/500
216s - loss: 0.2916 - acc: 0.9087 - val_loss: 0.6170 - val_acc: 0.8758
Epoch 114/500
217s - loss: 0.3034 - acc: 0.9103 - val_loss: 0.6236 - val_acc: 0.8758
Epoch 115/500
216s - loss: 0.3131 - acc: 0.9089 - val_loss: 0.6238 - val_acc: 0.8747
Epoch 116/500
215s - loss: 0.3005 - acc: 0.9068 - val_loss: 0.6283 - val_acc: 0.8747
Epoch 117/500
216s - loss: 0.2926 - acc: 0.9092 - val_loss: 0.6195 - val_acc: 0.8758
Epoch 118/500
216s - loss: 0.3042 - acc: 0.9145 - val_loss: 0.6192 - val_acc: 0.8758
Epoch 119/500
216s - loss: 0.2910 - acc: 0.9087 - val_loss: 0.6080 - val_acc: 0.8768
Epoch 120/500
216s - loss: 0.3099 - acc: 0.9092 - val_loss: 0.6092 - val_acc: 0.8779
Epoch 121/500
216s - loss: 0.2980 - acc: 0.9116 - val_loss: 0.6177 - val_acc: 0.8758
Epoch 122/500
216s - loss: 0.3224 - acc: 0.9079 - val_loss: 0.6190 - val_acc: 0.8768
Epoch 123/500
216s - loss: 0.2950 - acc: 0.9084 - val_loss: 0.6198 - val_acc: 0.8768
Epoch 124/500
218s - loss: 0.2871 - acc: 0.9121 - val_loss: 0.6126 - val_acc: 0.8768
Epoch 125/500
225s - loss: 0.2870 - acc: 0.9116 - val_loss: 0.6200 - val_acc: 0.8758
Epoch 126/500
229s - loss: 0.3029 - acc: 0.9092 - val_loss: 0.6242 - val_acc: 0.8758
Epoch 127/500
229s - loss: 0.2888 - acc: 0.9118 - val_loss: 0.6160 - val_acc: 0.8758
Epoch 128/500
229s - loss: 0.3298 - acc: 0.9053 - val_loss: 0.6193 - val_acc: 0.8758
Epoch 129/500
229s - loss: 0.3015 - acc: 0.9097 - val_loss: 0.6200 - val_acc: 0.8768
Epoch 130/500
229s - loss: 0.3103 - acc: 0.9079 - val_loss: 0.6181 - val_acc: 0.8768
Epoch 131/500
229s - loss: 0.2734 - acc: 0.9126 - val_loss: 0.6163 - val_acc: 0.8758
Epoch 132/500
229s - loss: 0.3122 - acc: 0.9097 - val_loss: 0.6342 - val_acc: 0.8758
Epoch 133/500
229s - loss: 0.2925 - acc: 0.9132 - val_loss: 0.6266 - val_acc: 0.8779
Epoch 134/500
229s - loss: 0.2970 - acc: 0.9074 - val_loss: 0.6225 - val_acc: 0.8768
Epoch 135/500
229s - loss: 0.3030 - acc: 0.9092 - val_loss: 0.6243 - val_acc: 0.8758
Epoch 136/500
229s - loss: 0.2858 - acc: 0.9113 - val_loss: 0.6087 - val_acc: 0.8789
Epoch 137/500
229s - loss: 0.3030 - acc: 0.9068 - val_loss: 0.6260 - val_acc: 0.8779
Training loss for fold 6 is 0.20158538485828198 with percent 93.15789474939045
Testing loss for fold 6 is 0.5935808142862822 with percent 88.21052637853121
 
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_15 (InputLayer)            (None, 75, 75, 3)     0                                            
____________________________________________________________________________________________________
conv2d_43 (Conv2D)               (None, 73, 73, 16)    448         input_15[0][0]                   
____________________________________________________________________________________________________
batch_normalization_50 (BatchNor (None, 73, 73, 16)    64          conv2d_43[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_50 (LeakyReLU)       (None, 73, 73, 16)    0           batch_normalization_50[0][0]     
____________________________________________________________________________________________________
conv2d_44 (Conv2D)               (None, 72, 72, 16)    1040        leaky_re_lu_50[0][0]             
____________________________________________________________________________________________________
batch_normalization_51 (BatchNor (None, 72, 72, 16)    64          conv2d_44[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_51 (LeakyReLU)       (None, 72, 72, 16)    0           batch_normalization_51[0][0]     
____________________________________________________________________________________________________
max_pooling2d_22 (MaxPooling2D)  (None, 36, 36, 16)    0           leaky_re_lu_51[0][0]             
____________________________________________________________________________________________________
dropout_36 (Dropout)             (None, 36, 36, 16)    0           max_pooling2d_22[0][0]           
____________________________________________________________________________________________________
conv2d_45 (Conv2D)               (None, 34, 34, 32)    4640        dropout_36[0][0]                 
____________________________________________________________________________________________________
batch_normalization_52 (BatchNor (None, 34, 34, 32)    128         conv2d_45[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_52 (LeakyReLU)       (None, 34, 34, 32)    0           batch_normalization_52[0][0]     
____________________________________________________________________________________________________
conv2d_46 (Conv2D)               (None, 33, 33, 32)    4128        leaky_re_lu_52[0][0]             
____________________________________________________________________________________________________
batch_normalization_53 (BatchNor (None, 33, 33, 32)    128         conv2d_46[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_53 (LeakyReLU)       (None, 33, 33, 32)    0           batch_normalization_53[0][0]     
____________________________________________________________________________________________________
max_pooling2d_23 (MaxPooling2D)  (None, 16, 16, 32)    0           leaky_re_lu_53[0][0]             
____________________________________________________________________________________________________
dropout_37 (Dropout)             (None, 16, 16, 32)    0           max_pooling2d_23[0][0]           
____________________________________________________________________________________________________
conv2d_47 (Conv2D)               (None, 14, 14, 64)    18496       dropout_37[0][0]                 
____________________________________________________________________________________________________
batch_normalization_54 (BatchNor (None, 14, 14, 64)    256         conv2d_47[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_54 (LeakyReLU)       (None, 14, 14, 64)    0           batch_normalization_54[0][0]     
____________________________________________________________________________________________________
conv2d_48 (Conv2D)               (None, 13, 13, 64)    16448       leaky_re_lu_54[0][0]             
____________________________________________________________________________________________________
batch_normalization_55 (BatchNor (None, 13, 13, 64)    256         conv2d_48[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_55 (LeakyReLU)       (None, 13, 13, 64)    0           batch_normalization_55[0][0]     
____________________________________________________________________________________________________
max_pooling2d_24 (MaxPooling2D)  (None, 6, 6, 64)      0           leaky_re_lu_55[0][0]             
____________________________________________________________________________________________________
dropout_38 (Dropout)             (None, 6, 6, 64)      0           max_pooling2d_24[0][0]           
____________________________________________________________________________________________________
flatten_8 (Flatten)              (None, 2304)          0           dropout_38[0][0]                 
____________________________________________________________________________________________________
batch_normalization_56 (BatchNor (None, 2304)          9216        flatten_8[0][0]                  
____________________________________________________________________________________________________
input_16 (InputLayer)            (None, 102)           0                                            
____________________________________________________________________________________________________
concatenate_8 (Concatenate)      (None, 2406)          0           batch_normalization_56[0][0]     
                                                                   input_16[0][0]                   
____________________________________________________________________________________________________
dense_29 (Dense)                 (None, 128)           308096      concatenate_8[0][0]              
____________________________________________________________________________________________________
leaky_re_lu_56 (LeakyReLU)       (None, 128)           0           dense_29[0][0]                   
____________________________________________________________________________________________________
dense_30 (Dense)                 (None, 64)            8256        leaky_re_lu_56[0][0]             
____________________________________________________________________________________________________
dropout_39 (Dropout)             (None, 64)            0           dense_30[0][0]                   
____________________________________________________________________________________________________
dense_31 (Dense)                 (None, 32)            2080        dropout_39[0][0]                 
____________________________________________________________________________________________________
dropout_40 (Dropout)             (None, 32)            0           dense_31[0][0]                   
____________________________________________________________________________________________________
dense_32 (Dense)                 (None, 12)            396         dropout_40[0][0]                 
====================================================================================================
Total params: 374,140
Trainable params: 369,084
Non-trainable params: 5,056
____________________________________________________________________________________________________
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
236s - loss: 1.9175 - acc: 0.3597 - val_loss: 4.9396 - val_acc: 0.1379
Epoch 2/500
233s - loss: 1.5178 - acc: 0.4995 - val_loss: 2.5529 - val_acc: 0.4263
Epoch 3/500
231s - loss: 1.3507 - acc: 0.5721 - val_loss: 1.4307 - val_acc: 0.5558
Epoch 4/500
232s - loss: 1.2869 - acc: 0.6013 - val_loss: 1.7486 - val_acc: 0.4716
Epoch 5/500
232s - loss: 1.1979 - acc: 0.6347 - val_loss: 2.3872 - val_acc: 0.4116
Epoch 6/500
230s - loss: 1.1747 - acc: 0.6497 - val_loss: 1.2917 - val_acc: 0.5895
Epoch 7/500
232s - loss: 1.1239 - acc: 0.6768 - val_loss: 1.0762 - val_acc: 0.6916
Epoch 8/500
230s - loss: 1.1049 - acc: 0.6718 - val_loss: 3.4658 - val_acc: 0.4253
Epoch 9/500
230s - loss: 1.0822 - acc: 0.6882 - val_loss: 1.1299 - val_acc: 0.6484
Epoch 10/500
233s - loss: 1.0463 - acc: 0.7063 - val_loss: 1.2546 - val_acc: 0.6937
Epoch 11/500
230s - loss: 1.0130 - acc: 0.7129 - val_loss: 1.8308 - val_acc: 0.5632
Epoch 12/500
231s - loss: 0.9526 - acc: 0.7308 - val_loss: 1.8972 - val_acc: 0.5042
Epoch 13/500
233s - loss: 0.9718 - acc: 0.7300 - val_loss: 3.1962 - val_acc: 0.4905
Epoch 14/500
232s - loss: 0.9314 - acc: 0.7511 - val_loss: 1.5329 - val_acc: 0.6684
Epoch 15/500
232s - loss: 0.9956 - acc: 0.7379 - val_loss: 2.8034 - val_acc: 0.4716
Epoch 16/500
233s - loss: 0.9989 - acc: 0.7400 - val_loss: 1.1777 - val_acc: 0.7084
Epoch 17/500
233s - loss: 1.0038 - acc: 0.7553 - val_loss: 1.8636 - val_acc: 0.6084
Epoch 18/500
233s - loss: 0.9898 - acc: 0.7574 - val_loss: 1.1117 - val_acc: 0.7126
Epoch 19/500
231s - loss: 1.0728 - acc: 0.7384 - val_loss: 1.5748 - val_acc: 0.6832
Epoch 20/500
233s - loss: 1.0436 - acc: 0.7497 - val_loss: 2.2768 - val_acc: 0.4758
Epoch 21/500
233s - loss: 1.0366 - acc: 0.7505 - val_loss: 1.2589 - val_acc: 0.7021
Epoch 22/500
233s - loss: 0.9973 - acc: 0.7624 - val_loss: 1.4232 - val_acc: 0.6821
Epoch 23/500
232s - loss: 1.0535 - acc: 0.7461 - val_loss: 1.0542 - val_acc: 0.8053
Epoch 24/500
232s - loss: 1.0287 - acc: 0.7624 - val_loss: 1.2448 - val_acc: 0.7621
Epoch 25/500
233s - loss: 1.0170 - acc: 0.7529 - val_loss: 1.6688 - val_acc: 0.6832
Epoch 26/500
233s - loss: 1.0012 - acc: 0.7668 - val_loss: 1.5823 - val_acc: 0.6726
Epoch 27/500
233s - loss: 0.9304 - acc: 0.7871 - val_loss: 1.3015 - val_acc: 0.7053
Epoch 28/500
233s - loss: 0.9293 - acc: 0.7874 - val_loss: 1.4006 - val_acc: 0.7632
Epoch 29/500
233s - loss: 0.9953 - acc: 0.7874 - val_loss: 1.3472 - val_acc: 0.7516
Epoch 30/500
233s - loss: 1.1043 - acc: 0.7529 - val_loss: 3.2545 - val_acc: 0.6400
Epoch 31/500
234s - loss: 1.1335 - acc: 0.7724 - val_loss: 1.8903 - val_acc: 0.6695
Epoch 32/500

Epoch 00031: reducing learning rate to 0.010000000149.
234s - loss: 1.1940 - acc: 0.7542 - val_loss: 1.4234 - val_acc: 0.7389
Epoch 33/500
231s - loss: 1.0311 - acc: 0.7711 - val_loss: 0.9722 - val_acc: 0.7874
Epoch 34/500
231s - loss: 0.8339 - acc: 0.8100 - val_loss: 0.8632 - val_acc: 0.8137
Epoch 35/500
230s - loss: 0.7703 - acc: 0.8203 - val_loss: 0.7745 - val_acc: 0.8284
Epoch 36/500
231s - loss: 0.6862 - acc: 0.8305 - val_loss: 0.7855 - val_acc: 0.8337
Epoch 37/500
230s - loss: 0.7125 - acc: 0.8305 - val_loss: 0.8202 - val_acc: 0.8326
Epoch 38/500
230s - loss: 0.6780 - acc: 0.8303 - val_loss: 0.8401 - val_acc: 0.8326
Epoch 39/500
231s - loss: 0.6122 - acc: 0.8421 - val_loss: 0.8313 - val_acc: 0.8295
Epoch 40/500
232s - loss: 0.6120 - acc: 0.8474 - val_loss: 0.7852 - val_acc: 0.8463
Epoch 41/500
233s - loss: 0.5513 - acc: 0.8571 - val_loss: 0.7901 - val_acc: 0.8389
Epoch 42/500
233s - loss: 0.5546 - acc: 0.8600 - val_loss: 0.6798 - val_acc: 0.8547
Epoch 43/500
231s - loss: 0.5807 - acc: 0.8561 - val_loss: 0.7176 - val_acc: 0.8558
Epoch 44/500
231s - loss: 0.5716 - acc: 0.8568 - val_loss: 0.7382 - val_acc: 0.8589
Epoch 45/500
230s - loss: 0.5459 - acc: 0.8637 - val_loss: 0.8233 - val_acc: 0.8463
Epoch 46/500
231s - loss: 0.5045 - acc: 0.8642 - val_loss: 0.7356 - val_acc: 0.8558
Epoch 47/500
233s - loss: 0.5190 - acc: 0.8611 - val_loss: 0.7519 - val_acc: 0.8579
Epoch 48/500
232s - loss: 0.5228 - acc: 0.8676 - val_loss: 0.8194 - val_acc: 0.8537
Epoch 49/500
232s - loss: 0.5019 - acc: 0.8647 - val_loss: 0.7162 - val_acc: 0.8484
Epoch 50/500
231s - loss: 0.5097 - acc: 0.8674 - val_loss: 0.7907 - val_acc: 0.8495
Epoch 51/500
231s - loss: 0.4963 - acc: 0.8716 - val_loss: 0.7871 - val_acc: 0.8484
Epoch 52/500
231s - loss: 0.4829 - acc: 0.8726 - val_loss: 0.7100 - val_acc: 0.8611
Epoch 53/500
230s - loss: 0.4602 - acc: 0.8758 - val_loss: 0.7488 - val_acc: 0.8611
Epoch 54/500
232s - loss: 0.4589 - acc: 0.8753 - val_loss: 0.6938 - val_acc: 0.8579
Epoch 55/500
233s - loss: 0.4522 - acc: 0.8742 - val_loss: 0.6836 - val_acc: 0.8589
Epoch 56/500
234s - loss: 0.4571 - acc: 0.8763 - val_loss: 0.6419 - val_acc: 0.8632
Epoch 57/500
233s - loss: 0.4119 - acc: 0.8826 - val_loss: 0.6289 - val_acc: 0.8621
Epoch 58/500
234s - loss: 0.4918 - acc: 0.8711 - val_loss: 0.6668 - val_acc: 0.8579
Epoch 59/500
233s - loss: 0.4327 - acc: 0.8774 - val_loss: 0.6253 - val_acc: 0.8663
Epoch 60/500
230s - loss: 0.4330 - acc: 0.8795 - val_loss: 0.6534 - val_acc: 0.8642
Epoch 61/500
231s - loss: 0.4171 - acc: 0.8792 - val_loss: 0.6277 - val_acc: 0.8695
Epoch 62/500
230s - loss: 0.4328 - acc: 0.8808 - val_loss: 0.6623 - val_acc: 0.8621
Epoch 63/500
230s - loss: 0.4149 - acc: 0.8808 - val_loss: 0.7380 - val_acc: 0.8558
Epoch 64/500
231s - loss: 0.4003 - acc: 0.8868 - val_loss: 0.6064 - val_acc: 0.8642
Epoch 65/500
233s - loss: 0.4006 - acc: 0.8853 - val_loss: 0.6144 - val_acc: 0.8653
Epoch 66/500
234s - loss: 0.4265 - acc: 0.8789 - val_loss: 0.7122 - val_acc: 0.8526
Epoch 67/500
232s - loss: 0.3998 - acc: 0.8889 - val_loss: 0.6754 - val_acc: 0.8579
Epoch 68/500
232s - loss: 0.4020 - acc: 0.8905 - val_loss: 0.6621 - val_acc: 0.8663
Epoch 69/500
233s - loss: 0.3996 - acc: 0.8916 - val_loss: 0.6481 - val_acc: 0.8642
Epoch 70/500

Epoch 00069: reducing learning rate to 0.000999999977648.
233s - loss: 0.3610 - acc: 0.8979 - val_loss: 0.6697 - val_acc: 0.8684
Epoch 71/500
233s - loss: 0.3692 - acc: 0.8939 - val_loss: 0.6715 - val_acc: 0.8674
Epoch 72/500
231s - loss: 0.3941 - acc: 0.8974 - val_loss: 0.6672 - val_acc: 0.8695
Epoch 73/500
230s - loss: 0.3595 - acc: 0.9018 - val_loss: 0.6532 - val_acc: 0.8642
Epoch 74/500
232s - loss: 0.3992 - acc: 0.8934 - val_loss: 0.6560 - val_acc: 0.8653
Epoch 75/500
233s - loss: 0.3406 - acc: 0.9000 - val_loss: 0.6517 - val_acc: 0.8642
Epoch 76/500
233s - loss: 0.3586 - acc: 0.8945 - val_loss: 0.6448 - val_acc: 0.8642
Epoch 77/500
233s - loss: 0.3302 - acc: 0.9005 - val_loss: 0.6429 - val_acc: 0.8621
Epoch 78/500

Epoch 00077: reducing learning rate to 9.99999931082e-05.
233s - loss: 0.3595 - acc: 0.8989 - val_loss: 0.6395 - val_acc: 0.8642
Epoch 79/500
233s - loss: 0.3612 - acc: 0.8953 - val_loss: 0.6260 - val_acc: 0.8632
Epoch 80/500
234s - loss: 0.3746 - acc: 0.8939 - val_loss: 0.6287 - val_acc: 0.8632
Epoch 81/500
233s - loss: 0.3548 - acc: 0.8989 - val_loss: 0.6281 - val_acc: 0.8642
Epoch 82/500
233s - loss: 0.3665 - acc: 0.8963 - val_loss: 0.6239 - val_acc: 0.8653
Epoch 83/500
234s - loss: 0.3604 - acc: 0.8939 - val_loss: 0.6250 - val_acc: 0.8632
Epoch 84/500
233s - loss: 0.3386 - acc: 0.8997 - val_loss: 0.6246 - val_acc: 0.8621
Epoch 85/500
233s - loss: 0.3506 - acc: 0.9005 - val_loss: 0.6261 - val_acc: 0.8642
Epoch 86/500

Epoch 00085: reducing learning rate to 9.99999901978e-06.
233s - loss: 0.3808 - acc: 0.8921 - val_loss: 0.6202 - val_acc: 0.8632
Epoch 87/500
233s - loss: 0.3407 - acc: 0.8968 - val_loss: 0.6201 - val_acc: 0.8642
Epoch 88/500
235s - loss: 0.3627 - acc: 0.8979 - val_loss: 0.6252 - val_acc: 0.8642
Epoch 89/500
233s - loss: 0.3352 - acc: 0.8979 - val_loss: 0.6231 - val_acc: 0.8653
Epoch 90/500
233s - loss: 0.3389 - acc: 0.8995 - val_loss: 0.6263 - val_acc: 0.8632
Epoch 91/500
233s - loss: 0.3479 - acc: 0.8995 - val_loss: 0.6277 - val_acc: 0.8621
Epoch 92/500
234s - loss: 0.3593 - acc: 0.8934 - val_loss: 0.6206 - val_acc: 0.8632
Epoch 93/500
233s - loss: 0.3328 - acc: 0.9003 - val_loss: 0.6214 - val_acc: 0.8632
Epoch 94/500

Epoch 00093: reducing learning rate to 1e-06.
234s - loss: 0.3506 - acc: 0.8979 - val_loss: 0.6241 - val_acc: 0.8632
Epoch 95/500
233s - loss: 0.3775 - acc: 0.8937 - val_loss: 0.6255 - val_acc: 0.8653
Epoch 96/500
234s - loss: 0.3860 - acc: 0.8913 - val_loss: 0.6317 - val_acc: 0.8663
Epoch 97/500
233s - loss: 0.3252 - acc: 0.9016 - val_loss: 0.6298 - val_acc: 0.8674
Epoch 98/500
233s - loss: 0.3674 - acc: 0.8979 - val_loss: 0.6211 - val_acc: 0.8642
Epoch 99/500
232s - loss: 0.3404 - acc: 0.8958 - val_loss: 0.6250 - val_acc: 0.8632
Epoch 100/500
233s - loss: 0.3446 - acc: 0.8992 - val_loss: 0.6255 - val_acc: 0.8632
Epoch 101/500
234s - loss: 0.3387 - acc: 0.8976 - val_loss: 0.6216 - val_acc: 0.8653
Epoch 102/500
233s - loss: 0.3475 - acc: 0.8987 - val_loss: 0.6241 - val_acc: 0.8653
Epoch 103/500
233s - loss: 0.3238 - acc: 0.9045 - val_loss: 0.6242 - val_acc: 0.8642
Epoch 104/500
233s - loss: 0.3455 - acc: 0.9016 - val_loss: 0.6249 - val_acc: 0.8653
Epoch 105/500
233s - loss: 0.3626 - acc: 0.8929 - val_loss: 0.6261 - val_acc: 0.8663
Epoch 106/500
233s - loss: 0.3492 - acc: 0.8997 - val_loss: 0.6300 - val_acc: 0.8642
Epoch 107/500
233s - loss: 0.3522 - acc: 0.8979 - val_loss: 0.6227 - val_acc: 0.8642
Epoch 108/500
234s - loss: 0.3550 - acc: 0.8963 - val_loss: 0.6204 - val_acc: 0.8632
Epoch 109/500
233s - loss: 0.3584 - acc: 0.8974 - val_loss: 0.6184 - val_acc: 0.8642
Epoch 110/500
233s - loss: 0.3640 - acc: 0.8987 - val_loss: 0.6251 - val_acc: 0.8663
Epoch 111/500
233s - loss: 0.3467 - acc: 0.9005 - val_loss: 0.6265 - val_acc: 0.8663
Epoch 112/500
233s - loss: 0.3778 - acc: 0.8942 - val_loss: 0.6219 - val_acc: 0.8663
Epoch 113/500
233s - loss: 0.3505 - acc: 0.8979 - val_loss: 0.6214 - val_acc: 0.8663
Epoch 114/500
233s - loss: 0.3363 - acc: 0.9003 - val_loss: 0.6240 - val_acc: 0.8653
Epoch 115/500
234s - loss: 0.3531 - acc: 0.9000 - val_loss: 0.6230 - val_acc: 0.8653
Epoch 116/500
233s - loss: 0.3864 - acc: 0.8958 - val_loss: 0.6214 - val_acc: 0.8653
Epoch 117/500
233s - loss: 0.3819 - acc: 0.8932 - val_loss: 0.6249 - val_acc: 0.8653
Epoch 118/500
233s - loss: 0.3485 - acc: 0.8976 - val_loss: 0.6285 - val_acc: 0.8642
Epoch 119/500
233s - loss: 0.3546 - acc: 0.8989 - val_loss: 0.6202 - val_acc: 0.8663
Epoch 120/500
233s - loss: 0.3503 - acc: 0.8961 - val_loss: 0.6218 - val_acc: 0.8663
Epoch 121/500
233s - loss: 0.3520 - acc: 0.8961 - val_loss: 0.6201 - val_acc: 0.8663
Epoch 122/500
233s - loss: 0.3362 - acc: 0.8987 - val_loss: 0.6246 - val_acc: 0.8653
Epoch 123/500
233s - loss: 0.3343 - acc: 0.9013 - val_loss: 0.6216 - val_acc: 0.8642
Epoch 124/500
233s - loss: 0.3469 - acc: 0.9008 - val_loss: 0.6235 - val_acc: 0.8653
Epoch 125/500
232s - loss: 0.3444 - acc: 0.8992 - val_loss: 0.6202 - val_acc: 0.8653
Training loss for fold 7 is 0.20711251592949817 with percent 92.52631577692534
Testing loss for fold 7 is 0.6672151819028352 with percent 86.94736840850429
 
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_17 (InputLayer)            (None, 75, 75, 3)     0                                            
____________________________________________________________________________________________________
conv2d_49 (Conv2D)               (None, 73, 73, 16)    448         input_17[0][0]                   
____________________________________________________________________________________________________
batch_normalization_57 (BatchNor (None, 73, 73, 16)    64          conv2d_49[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_57 (LeakyReLU)       (None, 73, 73, 16)    0           batch_normalization_57[0][0]     
____________________________________________________________________________________________________
conv2d_50 (Conv2D)               (None, 72, 72, 16)    1040        leaky_re_lu_57[0][0]             
____________________________________________________________________________________________________
batch_normalization_58 (BatchNor (None, 72, 72, 16)    64          conv2d_50[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_58 (LeakyReLU)       (None, 72, 72, 16)    0           batch_normalization_58[0][0]     
____________________________________________________________________________________________________
max_pooling2d_25 (MaxPooling2D)  (None, 36, 36, 16)    0           leaky_re_lu_58[0][0]             
____________________________________________________________________________________________________
dropout_41 (Dropout)             (None, 36, 36, 16)    0           max_pooling2d_25[0][0]           
____________________________________________________________________________________________________
conv2d_51 (Conv2D)               (None, 34, 34, 32)    4640        dropout_41[0][0]                 
____________________________________________________________________________________________________
batch_normalization_59 (BatchNor (None, 34, 34, 32)    128         conv2d_51[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_59 (LeakyReLU)       (None, 34, 34, 32)    0           batch_normalization_59[0][0]     
____________________________________________________________________________________________________
conv2d_52 (Conv2D)               (None, 33, 33, 32)    4128        leaky_re_lu_59[0][0]             
____________________________________________________________________________________________________
batch_normalization_60 (BatchNor (None, 33, 33, 32)    128         conv2d_52[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_60 (LeakyReLU)       (None, 33, 33, 32)    0           batch_normalization_60[0][0]     
____________________________________________________________________________________________________
max_pooling2d_26 (MaxPooling2D)  (None, 16, 16, 32)    0           leaky_re_lu_60[0][0]             
____________________________________________________________________________________________________
dropout_42 (Dropout)             (None, 16, 16, 32)    0           max_pooling2d_26[0][0]           
____________________________________________________________________________________________________
conv2d_53 (Conv2D)               (None, 14, 14, 64)    18496       dropout_42[0][0]                 
____________________________________________________________________________________________________
batch_normalization_61 (BatchNor (None, 14, 14, 64)    256         conv2d_53[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_61 (LeakyReLU)       (None, 14, 14, 64)    0           batch_normalization_61[0][0]     
____________________________________________________________________________________________________
conv2d_54 (Conv2D)               (None, 13, 13, 64)    16448       leaky_re_lu_61[0][0]             
____________________________________________________________________________________________________
batch_normalization_62 (BatchNor (None, 13, 13, 64)    256         conv2d_54[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_62 (LeakyReLU)       (None, 13, 13, 64)    0           batch_normalization_62[0][0]     
____________________________________________________________________________________________________
max_pooling2d_27 (MaxPooling2D)  (None, 6, 6, 64)      0           leaky_re_lu_62[0][0]             
____________________________________________________________________________________________________
dropout_43 (Dropout)             (None, 6, 6, 64)      0           max_pooling2d_27[0][0]           
____________________________________________________________________________________________________
flatten_9 (Flatten)              (None, 2304)          0           dropout_43[0][0]                 
____________________________________________________________________________________________________
batch_normalization_63 (BatchNor (None, 2304)          9216        flatten_9[0][0]                  
____________________________________________________________________________________________________
input_18 (InputLayer)            (None, 102)           0                                            
____________________________________________________________________________________________________
concatenate_9 (Concatenate)      (None, 2406)          0           batch_normalization_63[0][0]     
                                                                   input_18[0][0]                   
____________________________________________________________________________________________________
dense_33 (Dense)                 (None, 128)           308096      concatenate_9[0][0]              
____________________________________________________________________________________________________
leaky_re_lu_63 (LeakyReLU)       (None, 128)           0           dense_33[0][0]                   
____________________________________________________________________________________________________
dense_34 (Dense)                 (None, 64)            8256        leaky_re_lu_63[0][0]             
____________________________________________________________________________________________________
dropout_44 (Dropout)             (None, 64)            0           dense_34[0][0]                   
____________________________________________________________________________________________________
dense_35 (Dense)                 (None, 32)            2080        dropout_44[0][0]                 
____________________________________________________________________________________________________
dropout_45 (Dropout)             (None, 32)            0           dense_35[0][0]                   
____________________________________________________________________________________________________
dense_36 (Dense)                 (None, 12)            396         dropout_45[0][0]                 
====================================================================================================
Total params: 374,140
Trainable params: 369,084
Non-trainable params: 5,056
____________________________________________________________________________________________________
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
235s - loss: 1.9950 - acc: 0.3284 - val_loss: 2.9560 - val_acc: 0.2632
Epoch 2/500
232s - loss: 1.5357 - acc: 0.4929 - val_loss: 1.6584 - val_acc: 0.4042
Epoch 3/500
229s - loss: 1.4994 - acc: 0.5311 - val_loss: 4.2775 - val_acc: 0.1716
Epoch 4/500
230s - loss: 1.3971 - acc: 0.5679 - val_loss: 1.7744 - val_acc: 0.5421
Epoch 5/500
230s - loss: 1.3405 - acc: 0.5850 - val_loss: 1.6196 - val_acc: 0.4789
Epoch 6/500
230s - loss: 1.2667 - acc: 0.6195 - val_loss: 1.8786 - val_acc: 0.4653
Epoch 7/500
230s - loss: 1.1742 - acc: 0.6524 - val_loss: 1.9761 - val_acc: 0.4558
Epoch 8/500
230s - loss: 1.1683 - acc: 0.6616 - val_loss: 2.6680 - val_acc: 0.4968
Epoch 9/500
230s - loss: 1.1213 - acc: 0.6784 - val_loss: 2.3134 - val_acc: 0.3526
Epoch 10/500
230s - loss: 1.1376 - acc: 0.6747 - val_loss: 1.3311 - val_acc: 0.6084
Epoch 11/500
230s - loss: 1.0865 - acc: 0.6855 - val_loss: 2.9136 - val_acc: 0.2768
Epoch 12/500
230s - loss: 1.0748 - acc: 0.7021 - val_loss: 1.5322 - val_acc: 0.5642
Epoch 13/500
232s - loss: 1.0558 - acc: 0.7053 - val_loss: 2.5184 - val_acc: 0.3337
Epoch 14/500
232s - loss: 0.9998 - acc: 0.7187 - val_loss: 1.7536 - val_acc: 0.4621
Epoch 15/500
232s - loss: 1.0198 - acc: 0.7145 - val_loss: 1.4425 - val_acc: 0.6516
Epoch 16/500
230s - loss: 0.9464 - acc: 0.7416 - val_loss: 1.7444 - val_acc: 0.5705
Epoch 17/500
230s - loss: 0.9731 - acc: 0.7318 - val_loss: 1.7683 - val_acc: 0.5968
Epoch 18/500
233s - loss: 0.8807 - acc: 0.7561 - val_loss: 3.6317 - val_acc: 0.2653
Epoch 19/500
232s - loss: 0.9582 - acc: 0.7534 - val_loss: 1.1739 - val_acc: 0.6326
Epoch 20/500
233s - loss: 0.9494 - acc: 0.7453 - val_loss: 3.4033 - val_acc: 0.3389
Epoch 21/500
232s - loss: 0.9695 - acc: 0.7458 - val_loss: 2.2036 - val_acc: 0.6484
Epoch 22/500
232s - loss: 0.9628 - acc: 0.7534 - val_loss: 1.6935 - val_acc: 0.6337
Epoch 23/500
232s - loss: 1.0023 - acc: 0.7418 - val_loss: 1.5746 - val_acc: 0.6916
Epoch 24/500
229s - loss: 0.9406 - acc: 0.7571 - val_loss: 1.7483 - val_acc: 0.6558
Epoch 25/500
231s - loss: 0.9225 - acc: 0.7605 - val_loss: 1.2643 - val_acc: 0.7253
Epoch 26/500
229s - loss: 0.9350 - acc: 0.7611 - val_loss: 2.3380 - val_acc: 0.6274
Epoch 27/500
230s - loss: 0.9958 - acc: 0.7734 - val_loss: 1.9592 - val_acc: 0.5126
Epoch 28/500
232s - loss: 0.9169 - acc: 0.7703 - val_loss: 1.9826 - val_acc: 0.5979
Epoch 29/500
232s - loss: 0.9403 - acc: 0.7634 - val_loss: 1.5510 - val_acc: 0.5821
Epoch 30/500
232s - loss: 0.9398 - acc: 0.7782 - val_loss: 2.1949 - val_acc: 0.4905
Epoch 31/500
232s - loss: 0.9320 - acc: 0.7755 - val_loss: 1.2026 - val_acc: 0.7684
Epoch 32/500
229s - loss: 0.9646 - acc: 0.7571 - val_loss: 2.5334 - val_acc: 0.5558
Epoch 33/500
230s - loss: 1.0022 - acc: 0.7687 - val_loss: 1.1569 - val_acc: 0.7947
Epoch 34/500
231s - loss: 1.1091 - acc: 0.7411 - val_loss: 3.0855 - val_acc: 0.5747
Epoch 35/500
232s - loss: 1.0271 - acc: 0.7529 - val_loss: 3.1966 - val_acc: 0.5705
Epoch 36/500
232s - loss: 1.0646 - acc: 0.7421 - val_loss: 3.4155 - val_acc: 0.2189
Epoch 37/500
231s - loss: 1.0445 - acc: 0.7645 - val_loss: 1.4571 - val_acc: 0.7084
Epoch 38/500
233s - loss: 1.0756 - acc: 0.7771 - val_loss: 3.3197 - val_acc: 0.5505
Epoch 39/500
232s - loss: 1.1176 - acc: 0.7471 - val_loss: 1.6271 - val_acc: 0.7442
Epoch 40/500
232s - loss: 1.0769 - acc: 0.7626 - val_loss: 1.6429 - val_acc: 0.6800
Epoch 41/500
232s - loss: 1.0896 - acc: 0.7537 - val_loss: 1.7654 - val_acc: 0.6884
Epoch 42/500

Epoch 00041: reducing learning rate to 0.010000000149.
233s - loss: 1.0951 - acc: 0.7355 - val_loss: 1.3145 - val_acc: 0.7484
Epoch 43/500
232s - loss: 0.9415 - acc: 0.7521 - val_loss: 1.0240 - val_acc: 0.7674
Epoch 44/500
232s - loss: 0.7721 - acc: 0.7937 - val_loss: 0.9701 - val_acc: 0.7884
Epoch 45/500
233s - loss: 0.7413 - acc: 0.7984 - val_loss: 0.8662 - val_acc: 0.8042
Epoch 46/500
229s - loss: 0.7140 - acc: 0.8097 - val_loss: 0.9366 - val_acc: 0.8053
Epoch 47/500
231s - loss: 0.6670 - acc: 0.8261 - val_loss: 0.9142 - val_acc: 0.8189
Epoch 48/500
232s - loss: 0.6527 - acc: 0.8355 - val_loss: 0.8945 - val_acc: 0.8347
Epoch 49/500
230s - loss: 0.6398 - acc: 0.8389 - val_loss: 0.8929 - val_acc: 0.8274
Epoch 50/500
233s - loss: 0.6243 - acc: 0.8434 - val_loss: 0.7632 - val_acc: 0.8558
Epoch 51/500
231s - loss: 0.5680 - acc: 0.8476 - val_loss: 0.8176 - val_acc: 0.8526
Epoch 52/500
231s - loss: 0.5484 - acc: 0.8568 - val_loss: 0.8173 - val_acc: 0.8547
Epoch 53/500
229s - loss: 0.5150 - acc: 0.8671 - val_loss: 0.8081 - val_acc: 0.8484
Epoch 54/500
229s - loss: 0.5608 - acc: 0.8571 - val_loss: 0.7846 - val_acc: 0.8453
Epoch 55/500
229s - loss: 0.5150 - acc: 0.8603 - val_loss: 0.8170 - val_acc: 0.8421
Epoch 56/500
229s - loss: 0.5222 - acc: 0.8653 - val_loss: 0.8483 - val_acc: 0.8526
Epoch 57/500
229s - loss: 0.5086 - acc: 0.8679 - val_loss: 0.7928 - val_acc: 0.8526
Epoch 58/500
229s - loss: 0.4926 - acc: 0.8687 - val_loss: 0.8097 - val_acc: 0.8537
Epoch 59/500

Epoch 00058: reducing learning rate to 0.000999999977648.
229s - loss: 0.5095 - acc: 0.8663 - val_loss: 0.7718 - val_acc: 0.8537
Epoch 60/500
229s - loss: 0.4828 - acc: 0.8692 - val_loss: 0.7792 - val_acc: 0.8589
Epoch 61/500
229s - loss: 0.5189 - acc: 0.8668 - val_loss: 0.7727 - val_acc: 0.8558
Epoch 62/500
230s - loss: 0.4613 - acc: 0.8739 - val_loss: 0.7707 - val_acc: 0.8568
Epoch 63/500
232s - loss: 0.5029 - acc: 0.8650 - val_loss: 0.7725 - val_acc: 0.8568
Epoch 64/500
232s - loss: 0.4779 - acc: 0.8729 - val_loss: 0.7615 - val_acc: 0.8579
Epoch 65/500
232s - loss: 0.5228 - acc: 0.8650 - val_loss: 0.7530 - val_acc: 0.8611
Epoch 66/500
229s - loss: 0.4879 - acc: 0.8708 - val_loss: 0.7527 - val_acc: 0.8589
Epoch 67/500
229s - loss: 0.4990 - acc: 0.8716 - val_loss: 0.7555 - val_acc: 0.8589
Epoch 68/500
229s - loss: 0.4873 - acc: 0.8724 - val_loss: 0.7622 - val_acc: 0.8589
Epoch 69/500
231s - loss: 0.4810 - acc: 0.8689 - val_loss: 0.7642 - val_acc: 0.8589
Epoch 70/500
231s - loss: 0.4950 - acc: 0.8711 - val_loss: 0.7571 - val_acc: 0.8568
Epoch 71/500
231s - loss: 0.4658 - acc: 0.8737 - val_loss: 0.7543 - val_acc: 0.8579
Epoch 72/500
232s - loss: 0.4699 - acc: 0.8747 - val_loss: 0.7504 - val_acc: 0.8621
Epoch 73/500
229s - loss: 0.4683 - acc: 0.8737 - val_loss: 0.7515 - val_acc: 0.8632
Epoch 74/500
229s - loss: 0.4479 - acc: 0.8761 - val_loss: 0.7519 - val_acc: 0.8600
Epoch 75/500
230s - loss: 0.4684 - acc: 0.8718 - val_loss: 0.7444 - val_acc: 0.8632
Epoch 76/500
231s - loss: 0.4270 - acc: 0.8734 - val_loss: 0.7469 - val_acc: 0.8621
Epoch 77/500
231s - loss: 0.4465 - acc: 0.8755 - val_loss: 0.7415 - val_acc: 0.8642
Epoch 78/500
229s - loss: 0.4629 - acc: 0.8724 - val_loss: 0.7435 - val_acc: 0.8653
Epoch 79/500
229s - loss: 0.4592 - acc: 0.8718 - val_loss: 0.7437 - val_acc: 0.8663
Epoch 80/500
229s - loss: 0.4611 - acc: 0.8768 - val_loss: 0.7455 - val_acc: 0.8611
Epoch 81/500
229s - loss: 0.4510 - acc: 0.8758 - val_loss: 0.7359 - val_acc: 0.8663
Epoch 82/500
229s - loss: 0.4724 - acc: 0.8739 - val_loss: 0.7435 - val_acc: 0.8611
Epoch 83/500
231s - loss: 0.4613 - acc: 0.8766 - val_loss: 0.7463 - val_acc: 0.8621
Epoch 84/500
231s - loss: 0.4713 - acc: 0.8700 - val_loss: 0.7379 - val_acc: 0.8632
Epoch 85/500
231s - loss: 0.4507 - acc: 0.8774 - val_loss: 0.7343 - val_acc: 0.8663
Epoch 86/500
230s - loss: 0.4645 - acc: 0.8713 - val_loss: 0.7379 - val_acc: 0.8663
Epoch 87/500
230s - loss: 0.4345 - acc: 0.8768 - val_loss: 0.7422 - val_acc: 0.8653
Epoch 88/500
230s - loss: 0.4283 - acc: 0.8847 - val_loss: 0.7462 - val_acc: 0.8674
Epoch 89/500
229s - loss: 0.4418 - acc: 0.8766 - val_loss: 0.7465 - val_acc: 0.8642
Epoch 90/500
230s - loss: 0.4293 - acc: 0.8774 - val_loss: 0.7440 - val_acc: 0.8653
Epoch 91/500
230s - loss: 0.4538 - acc: 0.8766 - val_loss: 0.7366 - val_acc: 0.8621
Epoch 92/500
230s - loss: 0.4364 - acc: 0.8792 - val_loss: 0.7422 - val_acc: 0.8653
Epoch 93/500
229s - loss: 0.4483 - acc: 0.8784 - val_loss: 0.7432 - val_acc: 0.8653
Epoch 94/500
229s - loss: 0.4239 - acc: 0.8821 - val_loss: 0.7403 - val_acc: 0.8653
Epoch 95/500
229s - loss: 0.4738 - acc: 0.8718 - val_loss: 0.7408 - val_acc: 0.8653
Epoch 96/500
229s - loss: 0.4472 - acc: 0.8774 - val_loss: 0.7431 - val_acc: 0.8632
Epoch 97/500

Epoch 00096: reducing learning rate to 9.99999931082e-05.
229s - loss: 0.4488 - acc: 0.8805 - val_loss: 0.7413 - val_acc: 0.8642
Epoch 98/500
229s - loss: 0.4142 - acc: 0.8768 - val_loss: 0.7448 - val_acc: 0.8642
Epoch 99/500
229s - loss: 0.4662 - acc: 0.8784 - val_loss: 0.7452 - val_acc: 0.8663
Epoch 100/500
229s - loss: 0.4197 - acc: 0.8808 - val_loss: 0.7477 - val_acc: 0.8642
Epoch 101/500
229s - loss: 0.4262 - acc: 0.8811 - val_loss: 0.7412 - val_acc: 0.8663
Epoch 102/500
229s - loss: 0.4480 - acc: 0.8821 - val_loss: 0.7390 - val_acc: 0.8663
Epoch 103/500
229s - loss: 0.4373 - acc: 0.8800 - val_loss: 0.7478 - val_acc: 0.8653
Epoch 104/500
229s - loss: 0.4293 - acc: 0.8805 - val_loss: 0.7494 - val_acc: 0.8663
Epoch 105/500

Epoch 00104: reducing learning rate to 9.99999901978e-06.
229s - loss: 0.4384 - acc: 0.8771 - val_loss: 0.7437 - val_acc: 0.8653
Epoch 106/500
229s - loss: 0.4319 - acc: 0.8826 - val_loss: 0.7488 - val_acc: 0.8642
Epoch 107/500
230s - loss: 0.4324 - acc: 0.8821 - val_loss: 0.7491 - val_acc: 0.8653
Epoch 108/500
229s - loss: 0.4403 - acc: 0.8763 - val_loss: 0.7431 - val_acc: 0.8642
Epoch 109/500
229s - loss: 0.4165 - acc: 0.8839 - val_loss: 0.7449 - val_acc: 0.8642
Epoch 110/500
230s - loss: 0.4384 - acc: 0.8779 - val_loss: 0.7477 - val_acc: 0.8653
Epoch 111/500
229s - loss: 0.4039 - acc: 0.8847 - val_loss: 0.7435 - val_acc: 0.8632
Epoch 112/500
229s - loss: 0.4297 - acc: 0.8776 - val_loss: 0.7475 - val_acc: 0.8642
Epoch 113/500

Epoch 00112: reducing learning rate to 1e-06.
229s - loss: 0.4085 - acc: 0.8837 - val_loss: 0.7468 - val_acc: 0.8642
Epoch 114/500
230s - loss: 0.4611 - acc: 0.8711 - val_loss: 0.7441 - val_acc: 0.8653
Epoch 115/500
230s - loss: 0.4781 - acc: 0.8813 - val_loss: 0.7495 - val_acc: 0.8653
Epoch 116/500
229s - loss: 0.4628 - acc: 0.8745 - val_loss: 0.7460 - val_acc: 0.8653
Epoch 117/500
229s - loss: 0.4287 - acc: 0.8816 - val_loss: 0.7489 - val_acc: 0.8632
Epoch 118/500
230s - loss: 0.4319 - acc: 0.8808 - val_loss: 0.7473 - val_acc: 0.8653
Epoch 119/500
229s - loss: 0.4717 - acc: 0.8792 - val_loss: 0.7480 - val_acc: 0.8642
Epoch 120/500
230s - loss: 0.4527 - acc: 0.8747 - val_loss: 0.7414 - val_acc: 0.8642
Epoch 121/500
230s - loss: 0.4547 - acc: 0.8787 - val_loss: 0.7478 - val_acc: 0.8642
Epoch 122/500
230s - loss: 0.3953 - acc: 0.8845 - val_loss: 0.7506 - val_acc: 0.8642
Epoch 123/500
230s - loss: 0.4385 - acc: 0.8771 - val_loss: 0.7467 - val_acc: 0.8642
Epoch 124/500
230s - loss: 0.4827 - acc: 0.8766 - val_loss: 0.7430 - val_acc: 0.8642
Epoch 125/500
229s - loss: 0.4404 - acc: 0.8737 - val_loss: 0.7447 - val_acc: 0.8653
Epoch 126/500
230s - loss: 0.4408 - acc: 0.8797 - val_loss: 0.7387 - val_acc: 0.8653
Epoch 127/500
229s - loss: 0.4211 - acc: 0.8826 - val_loss: 0.7425 - val_acc: 0.8642
Epoch 128/500
229s - loss: 0.4339 - acc: 0.8787 - val_loss: 0.7449 - val_acc: 0.8642
Epoch 129/500
230s - loss: 0.4510 - acc: 0.8792 - val_loss: 0.7427 - val_acc: 0.8642
Epoch 130/500
230s - loss: 0.4312 - acc: 0.8776 - val_loss: 0.7471 - val_acc: 0.8642
Epoch 131/500
230s - loss: 0.4719 - acc: 0.8739 - val_loss: 0.7489 - val_acc: 0.8632
Epoch 132/500
229s - loss: 0.4421 - acc: 0.8734 - val_loss: 0.7481 - val_acc: 0.8632
Epoch 133/500
229s - loss: 0.4406 - acc: 0.8803 - val_loss: 0.7443 - val_acc: 0.8642
Epoch 134/500
229s - loss: 0.4457 - acc: 0.8784 - val_loss: 0.7452 - val_acc: 0.8632
Epoch 135/500
230s - loss: 0.4384 - acc: 0.8826 - val_loss: 0.7412 - val_acc: 0.8632
Epoch 136/500
229s - loss: 0.4633 - acc: 0.8789 - val_loss: 0.7426 - val_acc: 0.8663
Epoch 137/500
229s - loss: 0.4402 - acc: 0.8818 - val_loss: 0.7513 - val_acc: 0.8642
Epoch 138/500
229s - loss: 0.4452 - acc: 0.8779 - val_loss: 0.7469 - val_acc: 0.8642
Epoch 139/500
229s - loss: 0.4426 - acc: 0.8742 - val_loss: 0.7404 - val_acc: 0.8684
Epoch 140/500
229s - loss: 0.4169 - acc: 0.8834 - val_loss: 0.7440 - val_acc: 0.8653
Epoch 141/500
231s - loss: 0.4475 - acc: 0.8826 - val_loss: 0.7505 - val_acc: 0.8642
Epoch 142/500
231s - loss: 0.4382 - acc: 0.8795 - val_loss: 0.7469 - val_acc: 0.8653
Epoch 143/500
230s - loss: 0.4595 - acc: 0.8766 - val_loss: 0.7485 - val_acc: 0.8632
Epoch 144/500
229s - loss: 0.4273 - acc: 0.8808 - val_loss: 0.7425 - val_acc: 0.8632
Epoch 145/500
229s - loss: 0.4597 - acc: 0.8750 - val_loss: 0.7468 - val_acc: 0.8642
Epoch 146/500
220s - loss: 0.4260 - acc: 0.8824 - val_loss: 0.7441 - val_acc: 0.8642
Training loss for fold 8 is 0.2717653535698589 with percent 91.05263156639903
Testing loss for fold 8 is 0.7403679915478355 with percent 86.84210527570625
 
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_19 (InputLayer)            (None, 75, 75, 3)     0                                            
____________________________________________________________________________________________________
conv2d_55 (Conv2D)               (None, 73, 73, 16)    448         input_19[0][0]                   
____________________________________________________________________________________________________
batch_normalization_64 (BatchNor (None, 73, 73, 16)    64          conv2d_55[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_64 (LeakyReLU)       (None, 73, 73, 16)    0           batch_normalization_64[0][0]     
____________________________________________________________________________________________________
conv2d_56 (Conv2D)               (None, 72, 72, 16)    1040        leaky_re_lu_64[0][0]             
____________________________________________________________________________________________________
batch_normalization_65 (BatchNor (None, 72, 72, 16)    64          conv2d_56[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_65 (LeakyReLU)       (None, 72, 72, 16)    0           batch_normalization_65[0][0]     
____________________________________________________________________________________________________
max_pooling2d_28 (MaxPooling2D)  (None, 36, 36, 16)    0           leaky_re_lu_65[0][0]             
____________________________________________________________________________________________________
dropout_46 (Dropout)             (None, 36, 36, 16)    0           max_pooling2d_28[0][0]           
____________________________________________________________________________________________________
conv2d_57 (Conv2D)               (None, 34, 34, 32)    4640        dropout_46[0][0]                 
____________________________________________________________________________________________________
batch_normalization_66 (BatchNor (None, 34, 34, 32)    128         conv2d_57[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_66 (LeakyReLU)       (None, 34, 34, 32)    0           batch_normalization_66[0][0]     
____________________________________________________________________________________________________
conv2d_58 (Conv2D)               (None, 33, 33, 32)    4128        leaky_re_lu_66[0][0]             
____________________________________________________________________________________________________
batch_normalization_67 (BatchNor (None, 33, 33, 32)    128         conv2d_58[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_67 (LeakyReLU)       (None, 33, 33, 32)    0           batch_normalization_67[0][0]     
____________________________________________________________________________________________________
max_pooling2d_29 (MaxPooling2D)  (None, 16, 16, 32)    0           leaky_re_lu_67[0][0]             
____________________________________________________________________________________________________
dropout_47 (Dropout)             (None, 16, 16, 32)    0           max_pooling2d_29[0][0]           
____________________________________________________________________________________________________
conv2d_59 (Conv2D)               (None, 14, 14, 64)    18496       dropout_47[0][0]                 
____________________________________________________________________________________________________
batch_normalization_68 (BatchNor (None, 14, 14, 64)    256         conv2d_59[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_68 (LeakyReLU)       (None, 14, 14, 64)    0           batch_normalization_68[0][0]     
____________________________________________________________________________________________________
conv2d_60 (Conv2D)               (None, 13, 13, 64)    16448       leaky_re_lu_68[0][0]             
____________________________________________________________________________________________________
batch_normalization_69 (BatchNor (None, 13, 13, 64)    256         conv2d_60[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_69 (LeakyReLU)       (None, 13, 13, 64)    0           batch_normalization_69[0][0]     
____________________________________________________________________________________________________
max_pooling2d_30 (MaxPooling2D)  (None, 6, 6, 64)      0           leaky_re_lu_69[0][0]             
____________________________________________________________________________________________________
dropout_48 (Dropout)             (None, 6, 6, 64)      0           max_pooling2d_30[0][0]           
____________________________________________________________________________________________________
flatten_10 (Flatten)             (None, 2304)          0           dropout_48[0][0]                 
____________________________________________________________________________________________________
batch_normalization_70 (BatchNor (None, 2304)          9216        flatten_10[0][0]                 
____________________________________________________________________________________________________
input_20 (InputLayer)            (None, 102)           0                                            
____________________________________________________________________________________________________
concatenate_10 (Concatenate)     (None, 2406)          0           batch_normalization_70[0][0]     
                                                                   input_20[0][0]                   
____________________________________________________________________________________________________
dense_37 (Dense)                 (None, 128)           308096      concatenate_10[0][0]             
____________________________________________________________________________________________________
leaky_re_lu_70 (LeakyReLU)       (None, 128)           0           dense_37[0][0]                   
____________________________________________________________________________________________________
dense_38 (Dense)                 (None, 64)            8256        leaky_re_lu_70[0][0]             
____________________________________________________________________________________________________
dropout_49 (Dropout)             (None, 64)            0           dense_38[0][0]                   
____________________________________________________________________________________________________
dense_39 (Dense)                 (None, 32)            2080        dropout_49[0][0]                 
____________________________________________________________________________________________________
dropout_50 (Dropout)             (None, 32)            0           dense_39[0][0]                   
____________________________________________________________________________________________________
dense_40 (Dense)                 (None, 12)            396         dropout_50[0][0]                 
====================================================================================================
Total params: 374,140
Trainable params: 369,084
Non-trainable params: 5,056
____________________________________________________________________________________________________
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
229s - loss: 2.0375 - acc: 0.3055 - val_loss: 3.7475 - val_acc: 0.0874
Epoch 2/500
219s - loss: 1.4882 - acc: 0.5103 - val_loss: 3.2620 - val_acc: 0.2221
Epoch 3/500
219s - loss: 1.4500 - acc: 0.5258 - val_loss: 3.6816 - val_acc: 0.2263
Epoch 4/500
219s - loss: 1.3065 - acc: 0.5966 - val_loss: 1.7046 - val_acc: 0.5705
Epoch 5/500
219s - loss: 1.2746 - acc: 0.6187 - val_loss: 1.6508 - val_acc: 0.5305
Epoch 6/500
218s - loss: 1.2273 - acc: 0.6334 - val_loss: 1.5014 - val_acc: 0.5747
Epoch 7/500
219s - loss: 1.1828 - acc: 0.6497 - val_loss: 1.6047 - val_acc: 0.5695
Epoch 8/500
219s - loss: 1.1696 - acc: 0.6518 - val_loss: 1.6208 - val_acc: 0.4937
Epoch 9/500
218s - loss: 1.1079 - acc: 0.6876 - val_loss: 1.2310 - val_acc: 0.6105
Epoch 10/500
220s - loss: 1.0609 - acc: 0.6926 - val_loss: 1.5774 - val_acc: 0.5874
Epoch 11/500
219s - loss: 1.0825 - acc: 0.6895 - val_loss: 3.9186 - val_acc: 0.2242
Epoch 12/500
218s - loss: 1.0247 - acc: 0.7132 - val_loss: 2.1780 - val_acc: 0.4716
Epoch 13/500
219s - loss: 1.0491 - acc: 0.7095 - val_loss: 1.8925 - val_acc: 0.5326
Epoch 14/500
224s - loss: 1.0241 - acc: 0.7184 - val_loss: 1.3699 - val_acc: 0.6547
Epoch 15/500
219s - loss: 1.0101 - acc: 0.7347 - val_loss: 3.1368 - val_acc: 0.2674
Epoch 16/500
222s - loss: 0.9606 - acc: 0.7392 - val_loss: 2.1090 - val_acc: 0.4884
Epoch 17/500
222s - loss: 0.9255 - acc: 0.7432 - val_loss: 1.6247 - val_acc: 0.5747
Epoch 18/500
218s - loss: 0.9414 - acc: 0.7442 - val_loss: 1.2371 - val_acc: 0.7442
Epoch 19/500
220s - loss: 1.0048 - acc: 0.7447 - val_loss: 3.4060 - val_acc: 0.3726
Epoch 20/500
218s - loss: 0.9138 - acc: 0.7553 - val_loss: 1.5133 - val_acc: 0.6337
Epoch 21/500
217s - loss: 0.9046 - acc: 0.7603 - val_loss: 1.2625 - val_acc: 0.7032
Epoch 22/500
216s - loss: 0.9753 - acc: 0.7416 - val_loss: 1.0623 - val_acc: 0.7747
Epoch 23/500
217s - loss: 0.9792 - acc: 0.7484 - val_loss: 1.7564 - val_acc: 0.6484
Epoch 24/500
219s - loss: 0.9159 - acc: 0.7555 - val_loss: 3.0880 - val_acc: 0.4800
Epoch 25/500
218s - loss: 0.9560 - acc: 0.7611 - val_loss: 1.1679 - val_acc: 0.7547
Epoch 26/500
218s - loss: 1.1093 - acc: 0.7426 - val_loss: 1.6827 - val_acc: 0.6084
Epoch 27/500
216s - loss: 0.9822 - acc: 0.7503 - val_loss: 1.0665 - val_acc: 0.7568
Epoch 28/500
218s - loss: 0.9247 - acc: 0.7579 - val_loss: 1.4135 - val_acc: 0.6600
Epoch 29/500
217s - loss: 1.0308 - acc: 0.7395 - val_loss: 2.5844 - val_acc: 0.5716
Epoch 30/500
218s - loss: 1.0762 - acc: 0.7508 - val_loss: 2.3259 - val_acc: 0.4979
Epoch 31/500

Epoch 00030: reducing learning rate to 0.010000000149.
218s - loss: 1.0247 - acc: 0.7463 - val_loss: 1.6409 - val_acc: 0.6537
Epoch 32/500
219s - loss: 0.9218 - acc: 0.7711 - val_loss: 0.9351 - val_acc: 0.7726
Epoch 33/500
218s - loss: 0.7094 - acc: 0.8176 - val_loss: 0.8105 - val_acc: 0.7853
Epoch 34/500
218s - loss: 0.5940 - acc: 0.8366 - val_loss: 0.6170 - val_acc: 0.8316
Epoch 35/500
216s - loss: 0.6287 - acc: 0.8350 - val_loss: 0.6043 - val_acc: 0.8432
Epoch 36/500
217s - loss: 0.5723 - acc: 0.8503 - val_loss: 0.5857 - val_acc: 0.8453
Epoch 37/500
219s - loss: 0.5904 - acc: 0.8484 - val_loss: 0.5999 - val_acc: 0.8474
Epoch 38/500
217s - loss: 0.5745 - acc: 0.8482 - val_loss: 0.5823 - val_acc: 0.8463
Epoch 39/500
217s - loss: 0.5338 - acc: 0.8608 - val_loss: 0.6388 - val_acc: 0.8411
Epoch 40/500
217s - loss: 0.5109 - acc: 0.8632 - val_loss: 0.5640 - val_acc: 0.8495
Epoch 41/500
217s - loss: 0.4999 - acc: 0.8571 - val_loss: 0.5375 - val_acc: 0.8611
Epoch 42/500
216s - loss: 0.4769 - acc: 0.8705 - val_loss: 0.5887 - val_acc: 0.8526
Epoch 43/500
216s - loss: 0.4554 - acc: 0.8705 - val_loss: 0.5631 - val_acc: 0.8495
Epoch 44/500
229s - loss: 0.4656 - acc: 0.8726 - val_loss: 0.5443 - val_acc: 0.8558
Epoch 45/500
230s - loss: 0.4412 - acc: 0.8768 - val_loss: 0.5854 - val_acc: 0.8558
Epoch 46/500
230s - loss: 0.4592 - acc: 0.8742 - val_loss: 0.5754 - val_acc: 0.8558
Epoch 47/500
230s - loss: 0.3991 - acc: 0.8826 - val_loss: 0.5475 - val_acc: 0.8674
Epoch 48/500
230s - loss: 0.4205 - acc: 0.8755 - val_loss: 0.5279 - val_acc: 0.8653
Epoch 49/500
231s - loss: 0.4264 - acc: 0.8755 - val_loss: 0.5692 - val_acc: 0.8600
Epoch 50/500
231s - loss: 0.4386 - acc: 0.8803 - val_loss: 0.5549 - val_acc: 0.8589
Epoch 51/500
231s - loss: 0.4220 - acc: 0.8821 - val_loss: 0.5197 - val_acc: 0.8674
Epoch 52/500
232s - loss: 0.3887 - acc: 0.8779 - val_loss: 0.5212 - val_acc: 0.8705
Epoch 53/500
230s - loss: 0.3966 - acc: 0.8871 - val_loss: 0.5215 - val_acc: 0.8663
Epoch 54/500
230s - loss: 0.3889 - acc: 0.8887 - val_loss: 0.5134 - val_acc: 0.8611
Epoch 55/500
230s - loss: 0.3779 - acc: 0.8839 - val_loss: 0.5813 - val_acc: 0.8589
Epoch 56/500
231s - loss: 0.3560 - acc: 0.8903 - val_loss: 0.5620 - val_acc: 0.8653
Epoch 57/500
233s - loss: 0.3694 - acc: 0.8847 - val_loss: 0.5260 - val_acc: 0.8695
Epoch 58/500
232s - loss: 0.3782 - acc: 0.8897 - val_loss: 0.5183 - val_acc: 0.8695
Epoch 59/500
232s - loss: 0.3415 - acc: 0.8924 - val_loss: 0.5442 - val_acc: 0.8663
Epoch 60/500
232s - loss: 0.3424 - acc: 0.8932 - val_loss: 0.6232 - val_acc: 0.8611
Epoch 61/500

Epoch 00060: reducing learning rate to 0.000999999977648.
232s - loss: 0.3406 - acc: 0.8929 - val_loss: 0.5393 - val_acc: 0.8705
Epoch 62/500
230s - loss: 0.3375 - acc: 0.8955 - val_loss: 0.5130 - val_acc: 0.8684
Epoch 63/500
231s - loss: 0.3265 - acc: 0.8934 - val_loss: 0.4955 - val_acc: 0.8695
Epoch 64/500
232s - loss: 0.3158 - acc: 0.8971 - val_loss: 0.4890 - val_acc: 0.8726
Epoch 65/500
230s - loss: 0.3176 - acc: 0.8992 - val_loss: 0.4939 - val_acc: 0.8726
Epoch 66/500
230s - loss: 0.3160 - acc: 0.8974 - val_loss: 0.4980 - val_acc: 0.8695
Epoch 67/500
230s - loss: 0.3240 - acc: 0.8984 - val_loss: 0.4965 - val_acc: 0.8705
Epoch 68/500
232s - loss: 0.3235 - acc: 0.8963 - val_loss: 0.5050 - val_acc: 0.8695
Epoch 69/500
232s - loss: 0.3250 - acc: 0.8953 - val_loss: 0.5047 - val_acc: 0.8695
Epoch 70/500
232s - loss: 0.2927 - acc: 0.9013 - val_loss: 0.4990 - val_acc: 0.8705
Epoch 71/500
232s - loss: 0.3019 - acc: 0.8989 - val_loss: 0.4951 - val_acc: 0.8726
Epoch 72/500
233s - loss: 0.3238 - acc: 0.9000 - val_loss: 0.4921 - val_acc: 0.8747
Epoch 73/500
230s - loss: 0.3123 - acc: 0.9026 - val_loss: 0.4956 - val_acc: 0.8726
Epoch 74/500
231s - loss: 0.3179 - acc: 0.8997 - val_loss: 0.4866 - val_acc: 0.8737
Epoch 75/500
233s - loss: 0.3241 - acc: 0.8971 - val_loss: 0.4922 - val_acc: 0.8758
Epoch 76/500
230s - loss: 0.3416 - acc: 0.8937 - val_loss: 0.4916 - val_acc: 0.8747
Epoch 77/500
230s - loss: 0.3208 - acc: 0.8934 - val_loss: 0.4875 - val_acc: 0.8747
Epoch 78/500
231s - loss: 0.3219 - acc: 0.8968 - val_loss: 0.4946 - val_acc: 0.8716
Epoch 79/500
232s - loss: 0.3181 - acc: 0.8963 - val_loss: 0.4849 - val_acc: 0.8737
Epoch 80/500
232s - loss: 0.3362 - acc: 0.8908 - val_loss: 0.4805 - val_acc: 0.8737
Epoch 81/500
232s - loss: 0.3082 - acc: 0.8971 - val_loss: 0.4866 - val_acc: 0.8737
Epoch 82/500
232s - loss: 0.2929 - acc: 0.9045 - val_loss: 0.4886 - val_acc: 0.8758
Epoch 83/500
232s - loss: 0.3214 - acc: 0.8984 - val_loss: 0.4961 - val_acc: 0.8737
Epoch 84/500
232s - loss: 0.3123 - acc: 0.8971 - val_loss: 0.4824 - val_acc: 0.8779
Epoch 85/500
230s - loss: 0.3032 - acc: 0.8979 - val_loss: 0.4766 - val_acc: 0.8737
Epoch 86/500
231s - loss: 0.3167 - acc: 0.8979 - val_loss: 0.4796 - val_acc: 0.8758
Epoch 87/500
231s - loss: 0.3143 - acc: 0.9029 - val_loss: 0.4733 - val_acc: 0.8758
Epoch 88/500
232s - loss: 0.3203 - acc: 0.8932 - val_loss: 0.4831 - val_acc: 0.8726
Epoch 89/500
233s - loss: 0.2992 - acc: 0.9011 - val_loss: 0.4760 - val_acc: 0.8768
Epoch 90/500
233s - loss: 0.3279 - acc: 0.8961 - val_loss: 0.4818 - val_acc: 0.8737
Epoch 91/500
233s - loss: 0.2853 - acc: 0.9021 - val_loss: 0.4805 - val_acc: 0.8716
Epoch 92/500
232s - loss: 0.3126 - acc: 0.8966 - val_loss: 0.4805 - val_acc: 0.8737
Epoch 93/500

Epoch 00092: reducing learning rate to 9.99999931082e-05.
232s - loss: 0.3010 - acc: 0.8984 - val_loss: 0.4833 - val_acc: 0.8737
Epoch 94/500
233s - loss: 0.2997 - acc: 0.9016 - val_loss: 0.4910 - val_acc: 0.8726
Epoch 95/500
231s - loss: 0.3059 - acc: 0.9000 - val_loss: 0.4852 - val_acc: 0.8758
Epoch 96/500
230s - loss: 0.2926 - acc: 0.9018 - val_loss: 0.4844 - val_acc: 0.8758
Epoch 97/500
230s - loss: 0.3096 - acc: 0.8987 - val_loss: 0.4929 - val_acc: 0.8716
Epoch 98/500
231s - loss: 0.3123 - acc: 0.8979 - val_loss: 0.4871 - val_acc: 0.8758
Epoch 99/500
232s - loss: 0.3008 - acc: 0.8997 - val_loss: 0.4895 - val_acc: 0.8726
Epoch 100/500
232s - loss: 0.3337 - acc: 0.8984 - val_loss: 0.4846 - val_acc: 0.8747
Epoch 101/500

Epoch 00100: reducing learning rate to 9.99999901978e-06.
232s - loss: 0.3169 - acc: 0.8968 - val_loss: 0.4807 - val_acc: 0.8758
Epoch 102/500
232s - loss: 0.2747 - acc: 0.9053 - val_loss: 0.4804 - val_acc: 0.8768
Epoch 103/500
233s - loss: 0.2948 - acc: 0.9045 - val_loss: 0.4803 - val_acc: 0.8758
Epoch 104/500
232s - loss: 0.3045 - acc: 0.9013 - val_loss: 0.4832 - val_acc: 0.8737
Epoch 105/500
232s - loss: 0.3138 - acc: 0.8987 - val_loss: 0.4830 - val_acc: 0.8747
Epoch 106/500
233s - loss: 0.2968 - acc: 0.9003 - val_loss: 0.4876 - val_acc: 0.8747
Epoch 107/500
232s - loss: 0.2999 - acc: 0.9008 - val_loss: 0.4830 - val_acc: 0.8758
Epoch 108/500
232s - loss: 0.2884 - acc: 0.9011 - val_loss: 0.4923 - val_acc: 0.8726
Epoch 109/500

Epoch 00108: reducing learning rate to 1e-06.
232s - loss: 0.3273 - acc: 0.8984 - val_loss: 0.4879 - val_acc: 0.8747
Epoch 110/500
233s - loss: 0.2926 - acc: 0.9018 - val_loss: 0.4813 - val_acc: 0.8758
Epoch 111/500
232s - loss: 0.2989 - acc: 0.9029 - val_loss: 0.4851 - val_acc: 0.8758
Epoch 112/500
232s - loss: 0.2999 - acc: 0.8992 - val_loss: 0.4855 - val_acc: 0.8758
Epoch 113/500
233s - loss: 0.2893 - acc: 0.9045 - val_loss: 0.4869 - val_acc: 0.8737
Epoch 114/500
232s - loss: 0.3243 - acc: 0.8966 - val_loss: 0.4797 - val_acc: 0.8779
Epoch 115/500
232s - loss: 0.2923 - acc: 0.9016 - val_loss: 0.4806 - val_acc: 0.8758
Epoch 116/500
232s - loss: 0.3138 - acc: 0.8987 - val_loss: 0.4826 - val_acc: 0.8758
Epoch 117/500
232s - loss: 0.3088 - acc: 0.9013 - val_loss: 0.4834 - val_acc: 0.8747
Epoch 118/500
232s - loss: 0.3072 - acc: 0.9005 - val_loss: 0.4835 - val_acc: 0.8747
Epoch 119/500
232s - loss: 0.3119 - acc: 0.8987 - val_loss: 0.4897 - val_acc: 0.8737
Epoch 120/500
232s - loss: 0.3284 - acc: 0.8950 - val_loss: 0.4839 - val_acc: 0.8768
Epoch 121/500
232s - loss: 0.3163 - acc: 0.8984 - val_loss: 0.4826 - val_acc: 0.8758
Epoch 122/500
232s - loss: 0.2904 - acc: 0.9008 - val_loss: 0.4824 - val_acc: 0.8747
Epoch 123/500
233s - loss: 0.3156 - acc: 0.8995 - val_loss: 0.4777 - val_acc: 0.8779
Epoch 124/500
232s - loss: 0.3219 - acc: 0.8953 - val_loss: 0.4855 - val_acc: 0.8737
Epoch 125/500
232s - loss: 0.2847 - acc: 0.9068 - val_loss: 0.4886 - val_acc: 0.8737
Epoch 126/500
232s - loss: 0.3015 - acc: 0.8984 - val_loss: 0.4876 - val_acc: 0.8726
Epoch 127/500
232s - loss: 0.3024 - acc: 0.8968 - val_loss: 0.4787 - val_acc: 0.8768
Epoch 128/500
232s - loss: 0.3037 - acc: 0.8976 - val_loss: 0.4763 - val_acc: 0.8779
Epoch 129/500
232s - loss: 0.3270 - acc: 0.8984 - val_loss: 0.4802 - val_acc: 0.8758
Epoch 130/500
232s - loss: 0.3150 - acc: 0.9008 - val_loss: 0.4826 - val_acc: 0.8758
Epoch 131/500
232s - loss: 0.2935 - acc: 0.9032 - val_loss: 0.4889 - val_acc: 0.8737
Epoch 132/500
232s - loss: 0.2930 - acc: 0.9011 - val_loss: 0.4898 - val_acc: 0.8726
Epoch 133/500
232s - loss: 0.3095 - acc: 0.8966 - val_loss: 0.4789 - val_acc: 0.8768
Epoch 134/500
232s - loss: 0.3083 - acc: 0.8997 - val_loss: 0.4855 - val_acc: 0.8747
Epoch 135/500
232s - loss: 0.3198 - acc: 0.8976 - val_loss: 0.4848 - val_acc: 0.8737
Epoch 136/500
232s - loss: 0.2968 - acc: 0.9045 - val_loss: 0.4837 - val_acc: 0.8768
Epoch 137/500
232s - loss: 0.3227 - acc: 0.8979 - val_loss: 0.4871 - val_acc: 0.8726
Epoch 138/500
232s - loss: 0.3155 - acc: 0.9018 - val_loss: 0.4900 - val_acc: 0.8737
Epoch 139/500
232s - loss: 0.3042 - acc: 0.8968 - val_loss: 0.4825 - val_acc: 0.8758
Epoch 140/500
233s - loss: 0.2910 - acc: 0.9039 - val_loss: 0.4834 - val_acc: 0.8747
Epoch 141/500
232s - loss: 0.3255 - acc: 0.8992 - val_loss: 0.4961 - val_acc: 0.8737
Epoch 142/500
232s - loss: 0.3043 - acc: 0.9008 - val_loss: 0.4833 - val_acc: 0.8758
Epoch 143/500
232s - loss: 0.3084 - acc: 0.8989 - val_loss: 0.4839 - val_acc: 0.8747
Epoch 144/500
232s - loss: 0.3193 - acc: 0.8921 - val_loss: 0.4820 - val_acc: 0.8737
Epoch 145/500
231s - loss: 0.2947 - acc: 0.8989 - val_loss: 0.4867 - val_acc: 0.8726
Epoch 146/500
232s - loss: 0.3029 - acc: 0.9063 - val_loss: 0.4881 - val_acc: 0.8758
Epoch 147/500
232s - loss: 0.2802 - acc: 0.9034 - val_loss: 0.4792 - val_acc: 0.8758
Epoch 148/500
232s - loss: 0.3228 - acc: 0.8982 - val_loss: 0.4892 - val_acc: 0.8737
Training loss for fold 9 is 0.14572224435249442 with percent 93.23684210526316
Testing loss for fold 9 is 0.4824441597963634 with percent 87.78947370930722
 
Prediction dims (10, 794, 12)
Done training kfolds. Results:
[[0.19100658 0.92921053 0.70295816 0.87052632]
 [0.09542407 0.95842105 0.61775911 0.8831579 ]
 [0.32164158 0.87710526 0.73913095 0.81157895]
 [0.13731588 0.94631579 0.49013395 0.88736842]
 [0.16075859 0.93394737 0.50663857 0.88105263]
 [0.20572623 0.93315789 0.83905705 0.85684211]
 [0.20158538 0.93157895 0.59358081 0.88210526]
 [0.20711252 0.92526316 0.66721518 0.86947368]
 [0.27176535 0.91052632 0.74036799 0.86842105]
 [0.14572224 0.93236842 0.48244416 0.87789474]]
