Using TensorFlow backend.
/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Found data with correct size
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 100, 100, 3)       0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 98, 98, 16)        448       
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 96, 96, 16)        2320      
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 48, 48, 16)        0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 48, 48, 16)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 46, 46, 32)        4640      
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 44, 44, 32)        9248      
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 22, 22, 32)        0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 22, 22, 32)        0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 20, 20, 64)        18496     
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 18, 18, 64)        36928     
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 9, 9, 64)          0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 9, 9, 64)          0         
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 7, 7, 128)         73856     
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 3, 3, 128)         0         
_________________________________________________________________
dropout_4 (Dropout)          (None, 3, 3, 128)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 1152)              0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 1152)              4608      
_________________________________________________________________
dense_1 (Dense)              (None, 256)               295168    
_________________________________________________________________
dropout_5 (Dropout)          (None, 256)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 64)                16448     
_________________________________________________________________
dropout_6 (Dropout)          (None, 64)                0         
_________________________________________________________________
dense_3 (Dense)              (None, 12)                780       
=================================================================
Total params: 462,940
Trainable params: 460,636
Non-trainable params: 2,304
_________________________________________________________________
2018-08-01 10:05:41.754501: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-08-01 10:05:41.754527: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
(3562, 12) (1188, 12)
Train on 3562 samples, validate on 1188 samples
Epoch 1/300
216s - loss: nan - acc: 0.0550 - val_loss: nan - val_acc: 0.0572
Epoch 2/300
215s - loss: nan - acc: 0.0547 - val_loss: nan - val_acc: 0.0572
Epoch 3/300
215s - loss: nan - acc: 0.0547 - val_loss: nan - val_acc: 0.0572
Epoch 4/300
215s - loss: nan - acc: 0.0547 - val_loss: nan - val_acc: 0.0572
Epoch 5/300
214s - loss: nan - acc: 0.0547 - val_loss: nan - val_acc: 0.0572
Epoch 6/300
214s - loss: nan - acc: 0.0547 - val_loss: nan - val_acc: 0.0572
Epoch 7/300
215s - loss: nan - acc: 0.0547 - val_loss: nan - val_acc: 0.0572
Epoch 8/300
215s - loss: nan - acc: 0.0547 - val_loss: nan - val_acc: 0.0572
Epoch 9/300
216s - loss: nan - acc: 0.0547 - val_loss: nan - val_acc: 0.0572
Epoch 10/300
215s - loss: nan - acc: 0.0547 - val_loss: nan - val_acc: 0.0572
Epoch 11/300
215s - loss: nan - acc: 0.0547 - val_loss: nan - val_acc: 0.0572
Epoch 12/300
215s - loss: nan - acc: 0.0547 - val_loss: nan - val_acc: 0.0572
Epoch 13/300
214s - loss: nan - acc: 0.0547 - val_loss: nan - val_acc: 0.0572
Epoch 14/300
216s - loss: nan - acc: 0.0547 - val_loss: nan - val_acc: 0.0572
Epoch 15/300
214s - loss: nan - acc: 0.0547 - val_loss: nan - val_acc: 0.0572
Epoch 16/300
