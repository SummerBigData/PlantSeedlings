Using TensorFlow backend.
/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
 
Found train data with correct size
 
 
Found test data with correct size
 
/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 51, 51, 3)         0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 49, 49, 64)        1792      
_________________________________________________________________
batch_normalization_1 (Batch (None, 49, 49, 64)        256       
_________________________________________________________________
leaky_re_lu_1 (LeakyReLU)    (None, 49, 49, 64)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 47, 47, 64)        36928     
_________________________________________________________________
batch_normalization_2 (Batch (None, 47, 47, 64)        256       
_________________________________________________________________
leaky_re_lu_2 (LeakyReLU)    (None, 47, 47, 64)        0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 23, 23, 64)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 21, 21, 128)       73856     
_________________________________________________________________
batch_normalization_3 (Batch (None, 21, 21, 128)       512       
_________________________________________________________________
leaky_re_lu_3 (LeakyReLU)    (None, 21, 21, 128)       0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 19, 19, 128)       147584    
_________________________________________________________________
batch_normalization_4 (Batch (None, 19, 19, 128)       512       
_________________________________________________________________
leaky_re_lu_4 (LeakyReLU)    (None, 19, 19, 128)       0         
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 9, 9, 128)         0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 7, 7, 256)         295168    
_________________________________________________________________
batch_normalization_5 (Batch (None, 7, 7, 256)         1024      
_________________________________________________________________
leaky_re_lu_5 (LeakyReLU)    (None, 7, 7, 256)         0         
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 5, 5, 256)         590080    
_________________________________________________________________
batch_normalization_6 (Batch (None, 5, 5, 256)         1024      
_________________________________________________________________
leaky_re_lu_6 (LeakyReLU)    (None, 5, 5, 256)         0         
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 3, 3, 256)         590080    
_________________________________________________________________
batch_normalization_7 (Batch (None, 3, 3, 256)         1024      
_________________________________________________________________
leaky_re_lu_7 (LeakyReLU)    (None, 3, 3, 256)         0         
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 1, 1, 256)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 256)               0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 256)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               32896     
_________________________________________________________________
batch_normalization_8 (Batch (None, 128)               512       
_________________________________________________________________
activation_1 (Activation)    (None, 128)               0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 12)                1548      
_________________________________________________________________
batch_normalization_9 (Batch (None, 12)                48        
_________________________________________________________________
activation_2 (Activation)    (None, 12)                0         
=================================================================
Total params: 1,775,100
Trainable params: 1,772,516
Non-trainable params: 2,584
_________________________________________________________________
 
Pulling kfold 0 from previous runs
2018-08-09 09:57:11.885290: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-08-09 09:57:11.885324: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
 
Bad saved trial. Testing acc <0.9%. Rerunning ...
 
Train on 3800 samples, validate on 950 samples
Epoch 1/500
612s - loss: 1.0234 - acc: 0.6513 - val_loss: 1.6687 - val_acc: 0.4874
Epoch 2/500
609s - loss: 0.7434 - acc: 0.7421 - val_loss: 2.0756 - val_acc: 0.4853
Epoch 3/500
609s - loss: 0.5918 - acc: 0.7932 - val_loss: 1.0800 - val_acc: 0.6400
Epoch 4/500
610s - loss: 0.4892 - acc: 0.8379 - val_loss: 1.3913 - val_acc: 0.5632
Epoch 5/500
609s - loss: 0.4416 - acc: 0.8500 - val_loss: 0.7747 - val_acc: 0.7547
Epoch 6/500
607s - loss: 0.3858 - acc: 0.8647 - val_loss: 0.9888 - val_acc: 0.7105
Epoch 7/500
609s - loss: 0.3744 - acc: 0.8711 - val_loss: 0.7238 - val_acc: 0.7695
Epoch 8/500
604s - loss: 0.3212 - acc: 0.8797 - val_loss: 0.7161 - val_acc: 0.7874
Epoch 9/500
608s - loss: 0.3168 - acc: 0.8842 - val_loss: 0.6547 - val_acc: 0.7895
Epoch 10/500
608s - loss: 0.2730 - acc: 0.9024 - val_loss: 0.5226 - val_acc: 0.8179
Epoch 11/500
608s - loss: 0.2273 - acc: 0.9182 - val_loss: 0.8139 - val_acc: 0.7632
Epoch 12/500
608s - loss: 0.2226 - acc: 0.9145 - val_loss: 0.7719 - val_acc: 0.7874
Epoch 13/500
607s - loss: 0.2315 - acc: 0.9142 - val_loss: 0.6248 - val_acc: 0.7863
Epoch 14/500
609s - loss: 0.2309 - acc: 0.9129 - val_loss: 0.6626 - val_acc: 0.8126
Epoch 15/500
608s - loss: 0.2024 - acc: 0.9237 - val_loss: 0.9273 - val_acc: 0.7274
Epoch 16/500
608s - loss: 0.1916 - acc: 0.9268 - val_loss: 0.4116 - val_acc: 0.8737
Epoch 17/500
605s - loss: 0.1592 - acc: 0.9418 - val_loss: 1.8089 - val_acc: 0.5453
Epoch 18/500
608s - loss: 0.1782 - acc: 0.9337 - val_loss: 0.5071 - val_acc: 0.8547
Epoch 19/500
609s - loss: 0.1561 - acc: 0.9424 - val_loss: 0.6466 - val_acc: 0.7663
Epoch 20/500
608s - loss: 0.1463 - acc: 0.9497 - val_loss: 0.4070 - val_acc: 0.8768
Epoch 21/500
608s - loss: 0.1417 - acc: 0.9492 - val_loss: 0.6978 - val_acc: 0.7968
Epoch 22/500
608s - loss: 0.1227 - acc: 0.9532 - val_loss: 1.1421 - val_acc: 0.7032
Epoch 23/500
609s - loss: 0.1382 - acc: 0.9500 - val_loss: 0.5561 - val_acc: 0.8284
Epoch 24/500
608s - loss: 0.1180 - acc: 0.9587 - val_loss: 0.5887 - val_acc: 0.8389
Epoch 25/500
608s - loss: 0.1288 - acc: 0.9568 - val_loss: 0.6675 - val_acc: 0.8189
Epoch 26/500
602s - loss: 0.1148 - acc: 0.9613 - val_loss: 0.7375 - val_acc: 0.7863
Epoch 27/500
603s - loss: 0.1045 - acc: 0.9650 - val_loss: 0.5249 - val_acc: 0.8537
Epoch 28/500
601s - loss: 0.1076 - acc: 0.9611 - val_loss: 0.4438 - val_acc: 0.8747
Epoch 29/500
602s - loss: 0.0901 - acc: 0.9697 - val_loss: 0.3733 - val_acc: 0.8958
Epoch 30/500
607s - loss: 0.0882 - acc: 0.9703 - val_loss: 0.5051 - val_acc: 0.8537
Epoch 31/500
604s - loss: 0.0674 - acc: 0.9771 - val_loss: 0.7413 - val_acc: 0.7958
Epoch 32/500
602s - loss: 0.0601 - acc: 0.9797 - val_loss: 0.5389 - val_acc: 0.8579
Epoch 33/500
602s - loss: 0.0841 - acc: 0.9732 - val_loss: 0.7744 - val_acc: 0.8105
Epoch 34/500
602s - loss: 0.0686 - acc: 0.9776 - val_loss: 0.5223 - val_acc: 0.8526
Epoch 35/500
608s - loss: 0.0545 - acc: 0.9821 - val_loss: 0.5956 - val_acc: 0.8463
Epoch 36/500
608s - loss: 0.0699 - acc: 0.9782 - val_loss: 0.4649 - val_acc: 0.8716
Epoch 37/500
604s - loss: 0.0705 - acc: 0.9774 - val_loss: 0.5835 - val_acc: 0.8495
Epoch 38/500
578s - loss: 0.0553 - acc: 0.9837 - val_loss: 0.4877 - val_acc: 0.8789
Epoch 39/500
577s - loss: 0.0336 - acc: 0.9897 - val_loss: 0.5316 - val_acc: 0.8705
Epoch 40/500
578s - loss: 0.0594 - acc: 0.9797 - val_loss: 0.5743 - val_acc: 0.8600
Epoch 41/500
579s - loss: 0.0630 - acc: 0.9808 - val_loss: 0.8043 - val_acc: 0.8137
Epoch 42/500
577s - loss: 0.0433 - acc: 0.9874 - val_loss: 0.7126 - val_acc: 0.8337
Epoch 43/500
576s - loss: 0.0388 - acc: 0.9882 - val_loss: 0.5130 - val_acc: 0.8768
Epoch 44/500
577s - loss: 0.0524 - acc: 0.9816 - val_loss: 0.4350 - val_acc: 0.8863
Epoch 45/500
577s - loss: 0.0470 - acc: 0.9879 - val_loss: 0.8744 - val_acc: 0.7611
Epoch 46/500
577s - loss: 0.0426 - acc: 0.9858 - val_loss: 0.4863 - val_acc: 0.8842
Epoch 47/500
576s - loss: 0.0294 - acc: 0.9916 - val_loss: 0.5227 - val_acc: 0.8737
Epoch 48/500
577s - loss: 0.0429 - acc: 0.9868 - val_loss: 0.7275 - val_acc: 0.8432
Epoch 49/500
592s - loss: 0.0285 - acc: 0.9918 - val_loss: 0.7872 - val_acc: 0.8074
Epoch 50/500
603s - loss: 0.0429 - acc: 0.9874 - val_loss: 0.6685 - val_acc: 0.8611
Epoch 51/500
603s - loss: 0.0408 - acc: 0.9861 - val_loss: 0.8285 - val_acc: 0.7863
Epoch 52/500
603s - loss: 0.0441 - acc: 0.9866 - val_loss: 0.5710 - val_acc: 0.8821
Epoch 53/500
603s - loss: 0.0285 - acc: 0.9911 - val_loss: 0.4144 - val_acc: 0.8916
Epoch 54/500
603s - loss: 0.0334 - acc: 0.9905 - val_loss: 0.5563 - val_acc: 0.8768
Epoch 55/500
603s - loss: 0.0353 - acc: 0.9876 - val_loss: 0.5291 - val_acc: 0.8789
Epoch 56/500
608s - loss: 0.0374 - acc: 0.9882 - val_loss: 1.6333 - val_acc: 0.6547
Epoch 57/500
609s - loss: 0.0464 - acc: 0.9839 - val_loss: 0.3991 - val_acc: 0.8958
Epoch 58/500
603s - loss: 0.0309 - acc: 0.9908 - val_loss: 0.6113 - val_acc: 0.8726
Epoch 59/500
603s - loss: 0.0302 - acc: 0.9900 - val_loss: 0.4360 - val_acc: 0.8863
Epoch 60/500
603s - loss: 0.0256 - acc: 0.9918 - val_loss: 0.5247 - val_acc: 0.8832
Epoch 61/500
603s - loss: 0.0192 - acc: 0.9939 - val_loss: 0.5670 - val_acc: 0.8905
Epoch 62/500
603s - loss: 0.0272 - acc: 0.9903 - val_loss: 0.6578 - val_acc: 0.8695
Epoch 63/500
608s - loss: 0.0213 - acc: 0.9929 - val_loss: 0.5516 - val_acc: 0.8842
Epoch 64/500
606s - loss: 0.0313 - acc: 0.9916 - val_loss: 0.5933 - val_acc: 0.8611
Epoch 65/500
603s - loss: 0.0270 - acc: 0.9916 - val_loss: 0.5189 - val_acc: 0.8716
Epoch 66/500
603s - loss: 0.0117 - acc: 0.9966 - val_loss: 0.4200 - val_acc: 0.9095
Epoch 67/500
603s - loss: 0.0260 - acc: 0.9942 - val_loss: 0.6313 - val_acc: 0.8453
Epoch 68/500
603s - loss: 0.0202 - acc: 0.9934 - val_loss: 0.4925 - val_acc: 0.8863
Epoch 69/500
604s - loss: 0.0171 - acc: 0.9955 - val_loss: 0.4805 - val_acc: 0.8989
Epoch 70/500
609s - loss: 0.0094 - acc: 0.9971 - val_loss: 0.4859 - val_acc: 0.8895
Epoch 71/500
608s - loss: 0.0082 - acc: 0.9984 - val_loss: 0.4518 - val_acc: 0.9011
Epoch 72/500
606s - loss: 0.0099 - acc: 0.9976 - val_loss: 0.5436 - val_acc: 0.8937
Epoch 73/500
