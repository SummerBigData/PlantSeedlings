Using TensorFlow backend.
/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
 
Found train data with correct size
 
 
Found test data with correct size
 
/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 51, 51, 3)         0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 49, 49, 64)        1792      
_________________________________________________________________
batch_normalization_1 (Batch (None, 49, 49, 64)        256       
_________________________________________________________________
leaky_re_lu_1 (LeakyReLU)    (None, 49, 49, 64)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 47, 47, 64)        36928     
_________________________________________________________________
batch_normalization_2 (Batch (None, 47, 47, 64)        256       
_________________________________________________________________
leaky_re_lu_2 (LeakyReLU)    (None, 47, 47, 64)        0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 23, 23, 64)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 21, 21, 128)       73856     
_________________________________________________________________
batch_normalization_3 (Batch (None, 21, 21, 128)       512       
_________________________________________________________________
leaky_re_lu_3 (LeakyReLU)    (None, 21, 21, 128)       0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 19, 19, 128)       147584    
_________________________________________________________________
batch_normalization_4 (Batch (None, 19, 19, 128)       512       
_________________________________________________________________
leaky_re_lu_4 (LeakyReLU)    (None, 19, 19, 128)       0         
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 9, 9, 128)         0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 7, 7, 256)         295168    
_________________________________________________________________
batch_normalization_5 (Batch (None, 7, 7, 256)         1024      
_________________________________________________________________
leaky_re_lu_5 (LeakyReLU)    (None, 7, 7, 256)         0         
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 5, 5, 256)         590080    
_________________________________________________________________
batch_normalization_6 (Batch (None, 5, 5, 256)         1024      
_________________________________________________________________
leaky_re_lu_6 (LeakyReLU)    (None, 5, 5, 256)         0         
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 3, 3, 256)         590080    
_________________________________________________________________
batch_normalization_7 (Batch (None, 3, 3, 256)         1024      
_________________________________________________________________
leaky_re_lu_7 (LeakyReLU)    (None, 3, 3, 256)         0         
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 1, 1, 256)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 256)               0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 256)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               32896     
_________________________________________________________________
batch_normalization_8 (Batch (None, 128)               512       
_________________________________________________________________
activation_1 (Activation)    (None, 128)               0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 12)                1548      
_________________________________________________________________
batch_normalization_9 (Batch (None, 12)                48        
_________________________________________________________________
activation_2 (Activation)    (None, 12)                0         
=================================================================
Total params: 1,775,100
Trainable params: 1,772,516
Non-trainable params: 2,584
_________________________________________________________________
 
Pulling kfold 0 from previous runs
2018-08-09 09:57:11.885290: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-08-09 09:57:11.885324: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
 
Bad saved trial. Testing acc <0.9%. Rerunning ...
 
Train on 3800 samples, validate on 950 samples
Epoch 1/500
612s - loss: 1.0234 - acc: 0.6513 - val_loss: 1.6687 - val_acc: 0.4874
Epoch 2/500
609s - loss: 0.7434 - acc: 0.7421 - val_loss: 2.0756 - val_acc: 0.4853
Epoch 3/500
609s - loss: 0.5918 - acc: 0.7932 - val_loss: 1.0800 - val_acc: 0.6400
Epoch 4/500
610s - loss: 0.4892 - acc: 0.8379 - val_loss: 1.3913 - val_acc: 0.5632
Epoch 5/500
609s - loss: 0.4416 - acc: 0.8500 - val_loss: 0.7747 - val_acc: 0.7547
Epoch 6/500
607s - loss: 0.3858 - acc: 0.8647 - val_loss: 0.9888 - val_acc: 0.7105
Epoch 7/500
609s - loss: 0.3744 - acc: 0.8711 - val_loss: 0.7238 - val_acc: 0.7695
Epoch 8/500
604s - loss: 0.3212 - acc: 0.8797 - val_loss: 0.7161 - val_acc: 0.7874
Epoch 9/500
608s - loss: 0.3168 - acc: 0.8842 - val_loss: 0.6547 - val_acc: 0.7895
Epoch 10/500
608s - loss: 0.2730 - acc: 0.9024 - val_loss: 0.5226 - val_acc: 0.8179
Epoch 11/500
608s - loss: 0.2273 - acc: 0.9182 - val_loss: 0.8139 - val_acc: 0.7632
Epoch 12/500
608s - loss: 0.2226 - acc: 0.9145 - val_loss: 0.7719 - val_acc: 0.7874
Epoch 13/500
607s - loss: 0.2315 - acc: 0.9142 - val_loss: 0.6248 - val_acc: 0.7863
Epoch 14/500
609s - loss: 0.2309 - acc: 0.9129 - val_loss: 0.6626 - val_acc: 0.8126
Epoch 15/500
608s - loss: 0.2024 - acc: 0.9237 - val_loss: 0.9273 - val_acc: 0.7274
Epoch 16/500
608s - loss: 0.1916 - acc: 0.9268 - val_loss: 0.4116 - val_acc: 0.8737
Epoch 17/500
605s - loss: 0.1592 - acc: 0.9418 - val_loss: 1.8089 - val_acc: 0.5453
Epoch 18/500
608s - loss: 0.1782 - acc: 0.9337 - val_loss: 0.5071 - val_acc: 0.8547
Epoch 19/500
609s - loss: 0.1561 - acc: 0.9424 - val_loss: 0.6466 - val_acc: 0.7663
Epoch 20/500
608s - loss: 0.1463 - acc: 0.9497 - val_loss: 0.4070 - val_acc: 0.8768
Epoch 21/500
608s - loss: 0.1417 - acc: 0.9492 - val_loss: 0.6978 - val_acc: 0.7968
Epoch 22/500
608s - loss: 0.1227 - acc: 0.9532 - val_loss: 1.1421 - val_acc: 0.7032
Epoch 23/500
609s - loss: 0.1382 - acc: 0.9500 - val_loss: 0.5561 - val_acc: 0.8284
Epoch 24/500
608s - loss: 0.1180 - acc: 0.9587 - val_loss: 0.5887 - val_acc: 0.8389
Epoch 25/500
608s - loss: 0.1288 - acc: 0.9568 - val_loss: 0.6675 - val_acc: 0.8189
Epoch 26/500
602s - loss: 0.1148 - acc: 0.9613 - val_loss: 0.7375 - val_acc: 0.7863
Epoch 27/500
603s - loss: 0.1045 - acc: 0.9650 - val_loss: 0.5249 - val_acc: 0.8537
Epoch 28/500
601s - loss: 0.1076 - acc: 0.9611 - val_loss: 0.4438 - val_acc: 0.8747
Epoch 29/500
602s - loss: 0.0901 - acc: 0.9697 - val_loss: 0.3733 - val_acc: 0.8958
Epoch 30/500
607s - loss: 0.0882 - acc: 0.9703 - val_loss: 0.5051 - val_acc: 0.8537
Epoch 31/500
604s - loss: 0.0674 - acc: 0.9771 - val_loss: 0.7413 - val_acc: 0.7958
Epoch 32/500
602s - loss: 0.0601 - acc: 0.9797 - val_loss: 0.5389 - val_acc: 0.8579
Epoch 33/500
602s - loss: 0.0841 - acc: 0.9732 - val_loss: 0.7744 - val_acc: 0.8105
Epoch 34/500
602s - loss: 0.0686 - acc: 0.9776 - val_loss: 0.5223 - val_acc: 0.8526
Epoch 35/500
608s - loss: 0.0545 - acc: 0.9821 - val_loss: 0.5956 - val_acc: 0.8463
Epoch 36/500
608s - loss: 0.0699 - acc: 0.9782 - val_loss: 0.4649 - val_acc: 0.8716
Epoch 37/500
604s - loss: 0.0705 - acc: 0.9774 - val_loss: 0.5835 - val_acc: 0.8495
Epoch 38/500
578s - loss: 0.0553 - acc: 0.9837 - val_loss: 0.4877 - val_acc: 0.8789
Epoch 39/500
577s - loss: 0.0336 - acc: 0.9897 - val_loss: 0.5316 - val_acc: 0.8705
Epoch 40/500
578s - loss: 0.0594 - acc: 0.9797 - val_loss: 0.5743 - val_acc: 0.8600
Epoch 41/500
579s - loss: 0.0630 - acc: 0.9808 - val_loss: 0.8043 - val_acc: 0.8137
Epoch 42/500
577s - loss: 0.0433 - acc: 0.9874 - val_loss: 0.7126 - val_acc: 0.8337
Epoch 43/500
576s - loss: 0.0388 - acc: 0.9882 - val_loss: 0.5130 - val_acc: 0.8768
Epoch 44/500
577s - loss: 0.0524 - acc: 0.9816 - val_loss: 0.4350 - val_acc: 0.8863
Epoch 45/500
577s - loss: 0.0470 - acc: 0.9879 - val_loss: 0.8744 - val_acc: 0.7611
Epoch 46/500
577s - loss: 0.0426 - acc: 0.9858 - val_loss: 0.4863 - val_acc: 0.8842
Epoch 47/500
576s - loss: 0.0294 - acc: 0.9916 - val_loss: 0.5227 - val_acc: 0.8737
Epoch 48/500
577s - loss: 0.0429 - acc: 0.9868 - val_loss: 0.7275 - val_acc: 0.8432
Epoch 49/500
592s - loss: 0.0285 - acc: 0.9918 - val_loss: 0.7872 - val_acc: 0.8074
Epoch 50/500
603s - loss: 0.0429 - acc: 0.9874 - val_loss: 0.6685 - val_acc: 0.8611
Epoch 51/500
603s - loss: 0.0408 - acc: 0.9861 - val_loss: 0.8285 - val_acc: 0.7863
Epoch 52/500
603s - loss: 0.0441 - acc: 0.9866 - val_loss: 0.5710 - val_acc: 0.8821
Epoch 53/500
603s - loss: 0.0285 - acc: 0.9911 - val_loss: 0.4144 - val_acc: 0.8916
Epoch 54/500
603s - loss: 0.0334 - acc: 0.9905 - val_loss: 0.5563 - val_acc: 0.8768
Epoch 55/500
603s - loss: 0.0353 - acc: 0.9876 - val_loss: 0.5291 - val_acc: 0.8789
Epoch 56/500
608s - loss: 0.0374 - acc: 0.9882 - val_loss: 1.6333 - val_acc: 0.6547
Epoch 57/500
609s - loss: 0.0464 - acc: 0.9839 - val_loss: 0.3991 - val_acc: 0.8958
Epoch 58/500
603s - loss: 0.0309 - acc: 0.9908 - val_loss: 0.6113 - val_acc: 0.8726
Epoch 59/500
603s - loss: 0.0302 - acc: 0.9900 - val_loss: 0.4360 - val_acc: 0.8863
Epoch 60/500
603s - loss: 0.0256 - acc: 0.9918 - val_loss: 0.5247 - val_acc: 0.8832
Epoch 61/500
603s - loss: 0.0192 - acc: 0.9939 - val_loss: 0.5670 - val_acc: 0.8905
Epoch 62/500
603s - loss: 0.0272 - acc: 0.9903 - val_loss: 0.6578 - val_acc: 0.8695
Epoch 63/500
608s - loss: 0.0213 - acc: 0.9929 - val_loss: 0.5516 - val_acc: 0.8842
Epoch 64/500
606s - loss: 0.0313 - acc: 0.9916 - val_loss: 0.5933 - val_acc: 0.8611
Epoch 65/500
603s - loss: 0.0270 - acc: 0.9916 - val_loss: 0.5189 - val_acc: 0.8716
Epoch 66/500
603s - loss: 0.0117 - acc: 0.9966 - val_loss: 0.4200 - val_acc: 0.9095
Epoch 67/500
603s - loss: 0.0260 - acc: 0.9942 - val_loss: 0.6313 - val_acc: 0.8453
Epoch 68/500
603s - loss: 0.0202 - acc: 0.9934 - val_loss: 0.4925 - val_acc: 0.8863
Epoch 69/500
604s - loss: 0.0171 - acc: 0.9955 - val_loss: 0.4805 - val_acc: 0.8989
Epoch 70/500
609s - loss: 0.0094 - acc: 0.9971 - val_loss: 0.4859 - val_acc: 0.8895
Epoch 71/500
608s - loss: 0.0082 - acc: 0.9984 - val_loss: 0.4518 - val_acc: 0.9011
Epoch 72/500
606s - loss: 0.0099 - acc: 0.9976 - val_loss: 0.5436 - val_acc: 0.8937
Epoch 73/500
603s - loss: 0.0120 - acc: 0.9974 - val_loss: 0.4972 - val_acc: 0.8916
Epoch 74/500
603s - loss: 0.0088 - acc: 0.9976 - val_loss: 0.5183 - val_acc: 0.8926
Epoch 75/500
603s - loss: 0.0165 - acc: 0.9947 - val_loss: 0.5903 - val_acc: 0.8811
Epoch 76/500
603s - loss: 0.0281 - acc: 0.9911 - val_loss: 0.5489 - val_acc: 0.8811
Epoch 77/500
608s - loss: 0.0205 - acc: 0.9939 - val_loss: 0.4460 - val_acc: 0.9021
Epoch 78/500
603s - loss: 0.0098 - acc: 0.9982 - val_loss: 0.4761 - val_acc: 0.8874
Epoch 79/500
604s - loss: 0.0127 - acc: 0.9971 - val_loss: 0.4680 - val_acc: 0.8968
Epoch 80/500
605s - loss: 0.0070 - acc: 0.9984 - val_loss: 0.4782 - val_acc: 0.9042
Epoch 81/500
610s - loss: 0.0102 - acc: 0.9968 - val_loss: 0.5130 - val_acc: 0.8874
Epoch 82/500
610s - loss: 0.0125 - acc: 0.9963 - val_loss: 0.5549 - val_acc: 0.8811
Epoch 83/500
609s - loss: 0.0122 - acc: 0.9966 - val_loss: 0.4625 - val_acc: 0.8947
Epoch 84/500
610s - loss: 0.0153 - acc: 0.9955 - val_loss: 0.6927 - val_acc: 0.8632
Epoch 85/500
610s - loss: 0.0160 - acc: 0.9953 - val_loss: 0.4454 - val_acc: 0.8884
Epoch 86/500
609s - loss: 0.0147 - acc: 0.9963 - val_loss: 0.4875 - val_acc: 0.8916
Epoch 87/500
610s - loss: 0.0153 - acc: 0.9961 - val_loss: 0.4708 - val_acc: 0.8895
Epoch 88/500
609s - loss: 0.0103 - acc: 0.9974 - val_loss: 0.5005 - val_acc: 0.9000
Epoch 89/500
603s - loss: 0.0152 - acc: 0.9955 - val_loss: 0.5086 - val_acc: 0.8979
Epoch 90/500
603s - loss: 0.0297 - acc: 0.9913 - val_loss: 0.6595 - val_acc: 0.8421
Epoch 91/500
606s - loss: 0.0236 - acc: 0.9924 - val_loss: 0.5346 - val_acc: 0.8821
Epoch 92/500
604s - loss: 0.0110 - acc: 0.9966 - val_loss: 0.4515 - val_acc: 0.8968
Epoch 93/500
579s - loss: 0.0070 - acc: 0.9979 - val_loss: 0.4869 - val_acc: 0.9032
Epoch 94/500
606s - loss: 0.0111 - acc: 0.9971 - val_loss: 0.4753 - val_acc: 0.8916
Epoch 95/500
604s - loss: 0.0135 - acc: 0.9961 - val_loss: 0.4789 - val_acc: 0.8874
Epoch 96/500
608s - loss: 0.0113 - acc: 0.9976 - val_loss: 0.4418 - val_acc: 0.9074
Epoch 97/500
615s - loss: 0.0060 - acc: 0.9984 - val_loss: 0.4313 - val_acc: 0.9000
Epoch 98/500
614s - loss: 0.0069 - acc: 0.9982 - val_loss: 0.8354 - val_acc: 0.8411
Epoch 99/500
615s - loss: 0.0045 - acc: 0.9995 - val_loss: 0.4734 - val_acc: 0.9074
Epoch 100/500
615s - loss: 0.0086 - acc: 0.9974 - val_loss: 0.5039 - val_acc: 0.8874
Epoch 101/500
615s - loss: 0.0103 - acc: 0.9955 - val_loss: 0.7001 - val_acc: 0.8621
Epoch 102/500
615s - loss: 0.0168 - acc: 0.9947 - val_loss: 0.5037 - val_acc: 0.8958
Epoch 103/500
615s - loss: 0.0131 - acc: 0.9961 - val_loss: 0.6213 - val_acc: 0.8926
Epoch 104/500
616s - loss: 0.0097 - acc: 0.9968 - val_loss: 0.4504 - val_acc: 0.9042
Epoch 105/500
617s - loss: 0.0037 - acc: 0.9989 - val_loss: 0.4711 - val_acc: 0.9032
Epoch 106/500
617s - loss: 0.0058 - acc: 0.9979 - val_loss: 0.4734 - val_acc: 0.8905
Epoch 107/500
617s - loss: 0.0037 - acc: 0.9989 - val_loss: 0.4357 - val_acc: 0.9011
Epoch 108/500
617s - loss: 0.0053 - acc: 0.9989 - val_loss: 0.4738 - val_acc: 0.9000
Epoch 109/500
617s - loss: 0.0150 - acc: 0.9961 - val_loss: 0.5224 - val_acc: 0.8874
Epoch 110/500
607s - loss: 0.0045 - acc: 0.9989 - val_loss: 0.4407 - val_acc: 0.9063
Epoch 111/500
589s - loss: 0.0054 - acc: 0.9989 - val_loss: 0.6345 - val_acc: 0.8568
Epoch 112/500
589s - loss: 0.0133 - acc: 0.9968 - val_loss: 0.4636 - val_acc: 0.9084
Epoch 113/500
589s - loss: 0.0092 - acc: 0.9971 - val_loss: 0.5396 - val_acc: 0.8989
Epoch 114/500
590s - loss: 0.0074 - acc: 0.9976 - val_loss: 0.5148 - val_acc: 0.8853
Epoch 115/500
590s - loss: 0.0064 - acc: 0.9979 - val_loss: 0.4837 - val_acc: 0.9021
Epoch 116/500
591s - loss: 0.0051 - acc: 0.9987 - val_loss: 0.5000 - val_acc: 0.9032
Epoch 117/500
590s - loss: 0.0066 - acc: 0.9982 - val_loss: 0.4617 - val_acc: 0.9074
Epoch 118/500
590s - loss: 0.0037 - acc: 0.9989 - val_loss: 0.4266 - val_acc: 0.9158
Epoch 119/500
590s - loss: 0.0028 - acc: 0.9997 - val_loss: 0.4832 - val_acc: 0.9168
Epoch 120/500
590s - loss: 0.0074 - acc: 0.9979 - val_loss: 0.5952 - val_acc: 0.8811
Epoch 121/500
590s - loss: 0.0098 - acc: 0.9968 - val_loss: 0.5426 - val_acc: 0.8989
Epoch 122/500
590s - loss: 0.0028 - acc: 0.9997 - val_loss: 0.4891 - val_acc: 0.9042
Epoch 123/500
590s - loss: 0.0022 - acc: 0.9992 - val_loss: 0.5332 - val_acc: 0.9032
Epoch 124/500
590s - loss: 0.0039 - acc: 0.9992 - val_loss: 0.5930 - val_acc: 0.9042
Epoch 125/500
590s - loss: 0.0036 - acc: 0.9989 - val_loss: 0.4331 - val_acc: 0.9116
Epoch 126/500
590s - loss: 0.0059 - acc: 0.9979 - val_loss: 0.4687 - val_acc: 0.9116
Epoch 127/500
590s - loss: 0.0041 - acc: 0.9992 - val_loss: 0.5690 - val_acc: 0.9095
Epoch 128/500
590s - loss: 0.0037 - acc: 0.9992 - val_loss: 0.4576 - val_acc: 0.9042
Epoch 129/500
590s - loss: 0.0035 - acc: 0.9992 - val_loss: 0.4606 - val_acc: 0.9095
Epoch 130/500
590s - loss: 0.0073 - acc: 0.9979 - val_loss: 0.5282 - val_acc: 0.9084
Epoch 131/500
590s - loss: 0.0021 - acc: 0.9997 - val_loss: 0.4903 - val_acc: 0.9126
Epoch 132/500
590s - loss: 0.0033 - acc: 0.9995 - val_loss: 0.5426 - val_acc: 0.9095
Epoch 133/500
590s - loss: 0.0161 - acc: 0.9945 - val_loss: 0.5141 - val_acc: 0.9021
Epoch 134/500
590s - loss: 0.0084 - acc: 0.9966 - val_loss: 0.6413 - val_acc: 0.8789
Epoch 135/500
590s - loss: 0.0096 - acc: 0.9979 - val_loss: 0.7258 - val_acc: 0.8579
Epoch 136/500
590s - loss: 0.0102 - acc: 0.9968 - val_loss: 0.5601 - val_acc: 0.9000
Epoch 137/500
590s - loss: 0.0030 - acc: 0.9997 - val_loss: 0.4710 - val_acc: 0.9042
Epoch 138/500
592s - loss: 0.0047 - acc: 0.9989 - val_loss: 0.4841 - val_acc: 0.9211
Epoch 139/500
590s - loss: 0.0073 - acc: 0.9976 - val_loss: 0.4805 - val_acc: 0.9084
Epoch 140/500
590s - loss: 0.0053 - acc: 0.9984 - val_loss: 0.6058 - val_acc: 0.8895
Epoch 141/500
590s - loss: 0.0106 - acc: 0.9971 - val_loss: 0.8037 - val_acc: 0.8411
Epoch 142/500
591s - loss: 0.0133 - acc: 0.9966 - val_loss: 0.6384 - val_acc: 0.8979
Epoch 143/500
590s - loss: 0.0075 - acc: 0.9979 - val_loss: 0.5402 - val_acc: 0.8884
Epoch 144/500
590s - loss: 0.0041 - acc: 0.9984 - val_loss: 0.4810 - val_acc: 0.9074
Epoch 145/500
590s - loss: 0.0076 - acc: 0.9979 - val_loss: 0.4964 - val_acc: 0.8979
Epoch 146/500
590s - loss: 0.0036 - acc: 0.9989 - val_loss: 0.5757 - val_acc: 0.9042
Epoch 147/500
590s - loss: 0.0032 - acc: 0.9989 - val_loss: 0.4653 - val_acc: 0.9105
Epoch 148/500
590s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.5267 - val_acc: 0.9032
Epoch 149/500
591s - loss: 0.0034 - acc: 0.9992 - val_loss: 0.5318 - val_acc: 0.9042
Epoch 150/500
590s - loss: 0.0021 - acc: 0.9997 - val_loss: 0.4599 - val_acc: 0.9021
Epoch 151/500
590s - loss: 0.0020 - acc: 0.9997 - val_loss: 0.4961 - val_acc: 0.9074
Epoch 152/500
590s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.5082 - val_acc: 0.9021
Epoch 153/500
590s - loss: 0.0031 - acc: 0.9992 - val_loss: 0.4963 - val_acc: 0.9095
Epoch 154/500
590s - loss: 0.0043 - acc: 0.9982 - val_loss: 0.4948 - val_acc: 0.9105
Epoch 155/500
590s - loss: 0.0034 - acc: 0.9992 - val_loss: 0.5874 - val_acc: 0.9042
Epoch 156/500
590s - loss: 0.0020 - acc: 0.9997 - val_loss: 0.5107 - val_acc: 0.9084
Epoch 157/500
590s - loss: 0.0084 - acc: 0.9976 - val_loss: 0.6407 - val_acc: 0.8916
Epoch 158/500
590s - loss: 0.0170 - acc: 0.9955 - val_loss: 0.8900 - val_acc: 0.8232
Epoch 159/500
590s - loss: 0.0275 - acc: 0.9905 - val_loss: 0.7746 - val_acc: 0.8453
Epoch 160/500
590s - loss: 0.0193 - acc: 0.9942 - val_loss: 0.5261 - val_acc: 0.8926
Epoch 161/500
590s - loss: 0.0074 - acc: 0.9976 - val_loss: 0.5258 - val_acc: 0.9053
Epoch 162/500
590s - loss: 0.0034 - acc: 0.9997 - val_loss: 0.4664 - val_acc: 0.9116
Epoch 163/500
590s - loss: 0.0055 - acc: 0.9987 - val_loss: 0.4297 - val_acc: 0.9105
Epoch 164/500
590s - loss: 0.0037 - acc: 0.9989 - val_loss: 0.6039 - val_acc: 0.8947
Epoch 165/500
590s - loss: 0.0085 - acc: 0.9963 - val_loss: 0.5673 - val_acc: 0.8695
Epoch 166/500
589s - loss: 0.0035 - acc: 0.9995 - val_loss: 0.4751 - val_acc: 0.8979
Epoch 167/500
590s - loss: 0.0094 - acc: 0.9982 - val_loss: 0.5973 - val_acc: 0.8916
Epoch 168/500
590s - loss: 0.0024 - acc: 0.9992 - val_loss: 0.5023 - val_acc: 0.8884
Epoch 169/500
590s - loss: 0.0016 - acc: 0.9997 - val_loss: 0.4674 - val_acc: 0.9074
Epoch 170/500
590s - loss: 0.0035 - acc: 0.9984 - val_loss: 0.4370 - val_acc: 0.9147
Epoch 171/500
590s - loss: 0.0018 - acc: 0.9997 - val_loss: 0.4499 - val_acc: 0.9063
Epoch 172/500
590s - loss: 0.0011 - acc: 1.0000 - val_loss: 0.4413 - val_acc: 0.9116
Epoch 173/500
590s - loss: 0.0018 - acc: 0.9995 - val_loss: 0.4613 - val_acc: 0.9126
Epoch 174/500
590s - loss: 0.0015 - acc: 0.9997 - val_loss: 0.5323 - val_acc: 0.9053
Epoch 175/500
590s - loss: 0.0052 - acc: 0.9989 - val_loss: 0.5546 - val_acc: 0.8916
Epoch 176/500
590s - loss: 0.0036 - acc: 0.9989 - val_loss: 0.5074 - val_acc: 0.9032
Epoch 177/500
590s - loss: 0.0087 - acc: 0.9976 - val_loss: 0.7829 - val_acc: 0.8642
Epoch 178/500
590s - loss: 0.0059 - acc: 0.9979 - val_loss: 0.5049 - val_acc: 0.8853
Epoch 179/500
589s - loss: 0.0025 - acc: 0.9997 - val_loss: 0.4724 - val_acc: 0.9021
Epoch 180/500
590s - loss: 0.0028 - acc: 0.9997 - val_loss: 0.4460 - val_acc: 0.9053
Epoch 181/500
590s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.5033 - val_acc: 0.9105
Epoch 182/500
590s - loss: 0.0014 - acc: 1.0000 - val_loss: 0.4507 - val_acc: 0.9158
Epoch 183/500
591s - loss: 0.0013 - acc: 0.9997 - val_loss: 0.4525 - val_acc: 0.9126
Epoch 184/500
591s - loss: 0.0013 - acc: 1.0000 - val_loss: 0.4691 - val_acc: 0.9137
Epoch 185/500
590s - loss: 9.9650e-04 - acc: 0.9997 - val_loss: 0.4772 - val_acc: 0.9105
Epoch 186/500
590s - loss: 8.2889e-04 - acc: 0.9997 - val_loss: 0.4953 - val_acc: 0.9158
Epoch 187/500
590s - loss: 0.0014 - acc: 0.9995 - val_loss: 0.5399 - val_acc: 0.9074
Epoch 188/500
590s - loss: 0.0014 - acc: 0.9995 - val_loss: 0.4727 - val_acc: 0.9221
Epoch 189/500
590s - loss: 0.0011 - acc: 0.9997 - val_loss: 0.4682 - val_acc: 0.9158
Epoch 190/500
590s - loss: 0.0042 - acc: 0.9984 - val_loss: 0.5071 - val_acc: 0.9105
Epoch 191/500
590s - loss: 0.0018 - acc: 0.9992 - val_loss: 0.4492 - val_acc: 0.8979
Epoch 192/500
590s - loss: 0.0056 - acc: 0.9982 - val_loss: 0.5543 - val_acc: 0.8853
Epoch 193/500
590s - loss: 0.0016 - acc: 1.0000 - val_loss: 0.4830 - val_acc: 0.9032
Epoch 194/500
590s - loss: 0.0016 - acc: 0.9992 - val_loss: 0.4753 - val_acc: 0.9126
Epoch 195/500
590s - loss: 0.0015 - acc: 0.9995 - val_loss: 0.5388 - val_acc: 0.9021
Epoch 196/500
591s - loss: 0.0064 - acc: 0.9984 - val_loss: 0.5362 - val_acc: 0.8884
Epoch 197/500
590s - loss: 0.0051 - acc: 0.9987 - val_loss: 0.5880 - val_acc: 0.9042
Epoch 198/500
590s - loss: 0.0050 - acc: 0.9976 - val_loss: 0.8500 - val_acc: 0.8221
Epoch 199/500
590s - loss: 0.0063 - acc: 0.9987 - val_loss: 0.5497 - val_acc: 0.8832
Epoch 200/500
590s - loss: 0.0039 - acc: 0.9992 - val_loss: 0.6032 - val_acc: 0.9053
Epoch 201/500
590s - loss: 0.0048 - acc: 0.9982 - val_loss: 0.5788 - val_acc: 0.8842
Epoch 202/500
590s - loss: 0.0028 - acc: 0.9987 - val_loss: 0.5414 - val_acc: 0.9011
Epoch 203/500
589s - loss: 0.0027 - acc: 0.9997 - val_loss: 0.5533 - val_acc: 0.9000
Epoch 204/500
590s - loss: 0.0046 - acc: 0.9984 - val_loss: 0.4644 - val_acc: 0.8989
Epoch 205/500
590s - loss: 0.0016 - acc: 0.9995 - val_loss: 0.4357 - val_acc: 0.9137
Epoch 206/500
591s - loss: 0.0019 - acc: 0.9992 - val_loss: 0.5462 - val_acc: 0.8979
Epoch 207/500
589s - loss: 0.0032 - acc: 0.9992 - val_loss: 0.4755 - val_acc: 0.9095
Epoch 208/500
590s - loss: 0.0014 - acc: 0.9997 - val_loss: 0.4522 - val_acc: 0.9116
Epoch 209/500
587s - loss: 0.0076 - acc: 0.9976 - val_loss: 0.5216 - val_acc: 0.9032
Epoch 210/500
581s - loss: 0.0016 - acc: 0.9997 - val_loss: 0.4809 - val_acc: 0.9147
Epoch 211/500
581s - loss: 0.0032 - acc: 0.9989 - val_loss: 0.4918 - val_acc: 0.8937
Epoch 212/500
581s - loss: 0.0067 - acc: 0.9974 - val_loss: 0.6615 - val_acc: 0.8811
Epoch 213/500
2018-08-10 21:11:13.695001: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[25,21,21,128]
Traceback (most recent call last):
  File "cnn.py", line 103, in <module>
    print "Bad saved trial. Testing acc <"+str(cutoff)+"%. Rerunning ..."
  File "/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/keras/engine/training.py", line 1507, in fit
    initial_epoch=initial_epoch)
  File "/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/keras/engine/training.py", line 1156, in _fit_loop
    outs = f(ins_batch)
  File "/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py", line 2269, in __call__
    **self.session_kwargs)
  File "/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run
    run_metadata_ptr)
  File "/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1124, in _run
    feed_dict_tensor, options, run_metadata)
  File "/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1321, in _do_run
    options, run_metadata)
  File "/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[25,21,21,128]
	 [[Node: batch_normalization_3/moments/SquaredDifference = SquaredDifference[T=DT_FLOAT, _device="/job:localhost/replica:0/task:0/cpu:0"](conv2d_3/BiasAdd, batch_normalization_3/moments/StopGradient)]]

Caused by op u'batch_normalization_3/moments/SquaredDifference', defined at:
  File "cnn.py", line 65, in <module>
    model = dataPrep.getcnnKERAS(dim)
  File "/users/PAS1383/osu10171/PlantSeedlings/dataPrep.py", line 305, in getcnnKERAS
    input1 = Input(shape=(imgsize, imgsize, 3))
  File "/users/PAS1383/osu10171/PlantSeedlings/dataPrep.py", line 294, in conv_layer
    # Conv. layers set
  File "/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/keras/engine/topology.py", line 596, in __call__
    output = self.call(inputs, **kwargs)
  File "/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/keras/layers/normalization.py", line 177, in call
    epsilon=self.epsilon)
  File "/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py", line 1648, in normalize_batch_in_training
    shift=None, name=None, keep_dims=False)
  File "/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_impl.py", line 621, in moments
    math_ops.squared_difference(y, array_ops.stop_gradient(mean)),
  File "/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py", line 2610, in squared_difference
    result = _op_def_lib.apply_op("SquaredDifference", x=x, y=y, name=name)
  File "/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 767, in apply_op
    op_def=op_def)
  File "/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File "/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[25,21,21,128]
	 [[Node: batch_normalization_3/moments/SquaredDifference = SquaredDifference[T=DT_FLOAT, _device="/job:localhost/replica:0/task:0/cpu:0"](conv2d_3/BiasAdd, batch_normalization_3/moments/StopGradient)]]

