Using TensorFlow backend.
/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
 
Found train data with correct size
 
 
Found test data with correct size
 
/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.
  warn("The default mode, 'constant', will be changed to 'reflect' in "
Augmentation data size (4750, 2) (794, 2)
/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_1 (InputLayer)             (None, 51, 51, 3)     0                                            
____________________________________________________________________________________________________
conv2d_1 (Conv2D)                (None, 49, 49, 64)    1792        input_1[0][0]                    
____________________________________________________________________________________________________
batch_normalization_1 (BatchNorm (None, 49, 49, 64)    256         conv2d_1[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_1 (LeakyReLU)        (None, 49, 49, 64)    0           batch_normalization_1[0][0]      
____________________________________________________________________________________________________
conv2d_2 (Conv2D)                (None, 47, 47, 64)    36928       leaky_re_lu_1[0][0]              
____________________________________________________________________________________________________
batch_normalization_2 (BatchNorm (None, 47, 47, 64)    256         conv2d_2[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_2 (LeakyReLU)        (None, 47, 47, 64)    0           batch_normalization_2[0][0]      
____________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)   (None, 23, 23, 64)    0           leaky_re_lu_2[0][0]              
____________________________________________________________________________________________________
conv2d_3 (Conv2D)                (None, 21, 21, 128)   73856       max_pooling2d_1[0][0]            
____________________________________________________________________________________________________
batch_normalization_3 (BatchNorm (None, 21, 21, 128)   512         conv2d_3[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_3 (LeakyReLU)        (None, 21, 21, 128)   0           batch_normalization_3[0][0]      
____________________________________________________________________________________________________
conv2d_4 (Conv2D)                (None, 19, 19, 128)   147584      leaky_re_lu_3[0][0]              
____________________________________________________________________________________________________
batch_normalization_4 (BatchNorm (None, 19, 19, 128)   512         conv2d_4[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_4 (LeakyReLU)        (None, 19, 19, 128)   0           batch_normalization_4[0][0]      
____________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)   (None, 9, 9, 128)     0           leaky_re_lu_4[0][0]              
____________________________________________________________________________________________________
conv2d_5 (Conv2D)                (None, 7, 7, 256)     295168      max_pooling2d_2[0][0]            
____________________________________________________________________________________________________
batch_normalization_5 (BatchNorm (None, 7, 7, 256)     1024        conv2d_5[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_5 (LeakyReLU)        (None, 7, 7, 256)     0           batch_normalization_5[0][0]      
____________________________________________________________________________________________________
conv2d_6 (Conv2D)                (None, 5, 5, 256)     590080      leaky_re_lu_5[0][0]              
____________________________________________________________________________________________________
batch_normalization_6 (BatchNorm (None, 5, 5, 256)     1024        conv2d_6[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_6 (LeakyReLU)        (None, 5, 5, 256)     0           batch_normalization_6[0][0]      
____________________________________________________________________________________________________
conv2d_7 (Conv2D)                (None, 3, 3, 256)     590080      leaky_re_lu_6[0][0]              
____________________________________________________________________________________________________
batch_normalization_7 (BatchNorm (None, 3, 3, 256)     1024        conv2d_7[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_7 (LeakyReLU)        (None, 3, 3, 256)     0           batch_normalization_7[0][0]      
____________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)   (None, 1, 1, 256)     0           leaky_re_lu_7[0][0]              
____________________________________________________________________________________________________
flatten_1 (Flatten)              (None, 256)           0           max_pooling2d_3[0][0]            
____________________________________________________________________________________________________
input_2 (InputLayer)             (None, 2)             0                                            
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 258)           0           flatten_1[0][0]                  
                                                                   input_2[0][0]                    
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 258)           0           concatenate_1[0][0]              
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 128)           33152       dropout_1[0][0]                  
____________________________________________________________________________________________________
batch_normalization_8 (BatchNorm (None, 128)           512         dense_1[0][0]                    
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 128)           0           batch_normalization_8[0][0]      
____________________________________________________________________________________________________
dropout_2 (Dropout)              (None, 128)           0           activation_1[0][0]               
________________________________________________________________________________________________________
dense_2 (Dense)                  (None, 12)            1548        dropout_2[0][0]                  
____________________________________________________________________________________________________
batch_normalization_9 (BatchNorm (None, 12)            48          dense_2[0][0]                    
____________________________________________________________________________________________________
activation_2 (Activation)        (None, 12)            0           batch_normalization_9[0][0]      
====================================================================================================
Total params: 1,788,156
Trainable params: 1,785,572
Non-trainable params: 2,584
____________________________________________________________________________________________________
 
Pulling kfold 0 from previous runs
2018-08-09 12:16:42.505439: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-08-09 12:16:42.505692: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
 
Bad saved trial. Testing acc <0.9%. Rerunning ...
 
Train on 3800 samples, validate on 950 samples
Epoch 1/500
577s - loss: 0.9800 - acc: 0.6666 - val_loss: 1.3187 - val_acc: 0.5684
Epoch 2/500
576s - loss: 0.7820 - acc: 0.7289 - val_loss: 1.8504 - val_acc: 0.4926
Epoch 3/500
576s - loss: 0.6290 - acc: 0.7787 - val_loss: 0.9237 - val_acc: 0.6842
Epoch 4/500
576s - loss: 0.5568 - acc: 0.8084 - val_loss: 1.3568 - val_acc: 0.5400
Epoch 5/500
576s - loss: 0.4861 - acc: 0.8279 - val_loss: 0.9304 - val_acc: 0.7200
Epoch 6/500
575s - loss: 0.4185 - acc: 0.8568 - val_loss: 0.9912 - val_acc: 0.6821
Epoch 7/500
575s - loss: 0.4191 - acc: 0.8516 - val_loss: 0.5350 - val_acc: 0.8368
Epoch 8/500
575s - loss: 0.3732 - acc: 0.8632 - val_loss: 0.5801 - val_acc: 0.8200
Epoch 9/500
575s - loss: 0.3590 - acc: 0.8637 - val_loss: 0.5175 - val_acc: 0.8263
Epoch 10/500
575s - loss: 0.2972 - acc: 0.8842 - val_loss: 0.6859 - val_acc: 0.7874
Epoch 11/500
575s - loss: 0.2804 - acc: 0.8968 - val_loss: 1.0468 - val_acc: 0.6821
Epoch 12/500
575s - loss: 0.2606 - acc: 0.9061 - val_loss: 0.4722 - val_acc: 0.8453
Epoch 13/500
575s - loss: 0.2496 - acc: 0.9074 - val_loss: 0.6951 - val_acc: 0.8126
Epoch 14/500
575s - loss: 0.2358 - acc: 0.9111 - val_loss: 0.6566 - val_acc: 0.8232
Epoch 15/500
575s - loss: 0.2171 - acc: 0.9182 - val_loss: 0.6297 - val_acc: 0.8126
Epoch 16/500
575s - loss: 0.2016 - acc: 0.9211 - val_loss: 0.5540 - val_acc: 0.8200
Epoch 17/500
575s - loss: 0.1724 - acc: 0.9334 - val_loss: 0.7216 - val_acc: 0.8116
Epoch 18/500
575s - loss: 0.1803 - acc: 0.9316 - val_loss: 0.4533 - val_acc: 0.8621
Epoch 19/500
577s - loss: 0.1200 - acc: 0.9603 - val_loss: 0.3251 - val_acc: 0.9053
Epoch 20/500
575s - loss: 0.0876 - acc: 0.9708 - val_loss: 0.3160 - val_acc: 0.9084
Epoch 21/500
575s - loss: 0.0843 - acc: 0.9703 - val_loss: 0.3201 - val_acc: 0.9105
Epoch 22/500
575s - loss: 0.0762 - acc: 0.9739 - val_loss: 0.3343 - val_acc: 0.9074
Epoch 23/500
575s - loss: 0.0748 - acc: 0.9745 - val_loss: 0.3620 - val_acc: 0.9011
Epoch 24/500
575s - loss: 0.0682 - acc: 0.9771 - val_loss: 0.3477 - val_acc: 0.9147
Epoch 25/500
575s - loss: 0.0703 - acc: 0.9792 - val_loss: 0.3438 - val_acc: 0.9126
Epoch 26/500
574s - loss: 0.0641 - acc: 0.9782 - val_loss: 0.3312 - val_acc: 0.9116
Epoch 27/500
575s - loss: 0.0591 - acc: 0.9808 - val_loss: 0.3408 - val_acc: 0.9168
Epoch 28/500
575s - loss: 0.0520 - acc: 0.9858 - val_loss: 0.3538 - val_acc: 0.8979
Epoch 29/500
574s - loss: 0.0575 - acc: 0.9805 - val_loss: 0.3354 - val_acc: 0.9105
Epoch 30/500
575s - loss: 0.0562 - acc: 0.9824 - val_loss: 0.3189 - val_acc: 0.9179
Epoch 31/500
575s - loss: 0.0502 - acc: 0.9853 - val_loss: 0.3520 - val_acc: 0.9147
Epoch 32/500
575s - loss: 0.0475 - acc: 0.9861 - val_loss: 0.3668 - val_acc: 0.9105
Epoch 33/500
575s - loss: 0.0494 - acc: 0.9858 - val_loss: 0.3528 - val_acc: 0.9189
Epoch 34/500
575s - loss: 0.0436 - acc: 0.9889 - val_loss: 0.3298 - val_acc: 0.9242
Epoch 35/500
575s - loss: 0.0326 - acc: 0.9924 - val_loss: 0.3263 - val_acc: 0.9263
Epoch 36/500
575s - loss: 0.0357 - acc: 0.9924 - val_loss: 0.3271 - val_acc: 0.9232
Epoch 37/500
575s - loss: 0.0404 - acc: 0.9884 - val_loss: 0.3328 - val_acc: 0.9242
Epoch 38/500
575s - loss: 0.0324 - acc: 0.9926 - val_loss: 0.3366 - val_acc: 0.9232
Epoch 39/500
575s - loss: 0.0313 - acc: 0.9918 - val_loss: 0.3423 - val_acc: 0.9232
Epoch 40/500
575s - loss: 0.0313 - acc: 0.9897 - val_loss: 0.3430 - val_acc: 0.9242
Epoch 41/500
575s - loss: 0.0311 - acc: 0.9929 - val_loss: 0.3412 - val_acc: 0.9232
Epoch 42/500
575s - loss: 0.0252 - acc: 0.9950 - val_loss: 0.3461 - val_acc: 0.9263
Epoch 43/500
575s - loss: 0.0270 - acc: 0.9934 - val_loss: 0.3411 - val_acc: 0.9242
Epoch 44/500
575s - loss: 0.0300 - acc: 0.9918 - val_loss: 0.3386 - val_acc: 0.9242
Epoch 45/500
575s - loss: 0.0265 - acc: 0.9947 - val_loss: 0.3383 - val_acc: 0.9253
Epoch 46/500
575s - loss: 0.0299 - acc: 0.9929 - val_loss: 0.3460 - val_acc: 0.9221
Epoch 47/500
575s - loss: 0.0336 - acc: 0.9924 - val_loss: 0.3565 - val_acc: 0.9189
Epoch 48/500
575s - loss: 0.0307 - acc: 0.9916 - val_loss: 0.3530 - val_acc: 0.9211
Epoch 49/500
575s - loss: 0.0264 - acc: 0.9932 - val_loss: 0.3523 - val_acc: 0.9242
Epoch 50/500
575s - loss: 0.0233 - acc: 0.9953 - val_loss: 0.3562 - val_acc: 0.9221
Epoch 51/500
575s - loss: 0.0279 - acc: 0.9939 - val_loss: 0.3508 - val_acc: 0.9211
Epoch 52/500
574s - loss: 0.0272 - acc: 0.9929 - val_loss: 0.3518 - val_acc: 0.9232
Epoch 53/500
574s - loss: 0.0286 - acc: 0.9934 - val_loss: 0.3544 - val_acc: 0.9221
Epoch 54/500
575s - loss: 0.0254 - acc: 0.9934 - val_loss: 0.3524 - val_acc: 0.9211
Epoch 55/500
575s - loss: 0.0230 - acc: 0.9968 - val_loss: 0.3585 - val_acc: 0.9221
Epoch 56/500
575s - loss: 0.0276 - acc: 0.9932 - val_loss: 0.3537 - val_acc: 0.9232
Epoch 57/500
575s - loss: 0.0249 - acc: 0.9929 - val_loss: 0.3548 - val_acc: 0.9242
Epoch 58/500
575s - loss: 0.0231 - acc: 0.9947 - val_loss: 0.3576 - val_acc: 0.9232
Epoch 59/500
575s - loss: 0.0280 - acc: 0.9932 - val_loss: 0.3582 - val_acc: 0.9242
Epoch 60/500
575s - loss: 0.0241 - acc: 0.9947 - val_loss: 0.3587 - val_acc: 0.9242
Epoch 61/500
576s - loss: 0.0265 - acc: 0.9937 - val_loss: 0.3548 - val_acc: 0.9221
Epoch 62/500
575s - loss: 0.0258 - acc: 0.9934 - val_loss: 0.3539 - val_acc: 0.9242
Epoch 63/500
575s - loss: 0.0257 - acc: 0.9950 - val_loss: 0.3558 - val_acc: 0.9242
Epoch 64/500
575s - loss: 0.0230 - acc: 0.9958 - val_loss: 0.3534 - val_acc: 0.9242
Epoch 65/500
575s - loss: 0.0289 - acc: 0.9929 - val_loss: 0.3546 - val_acc: 0.9253
Epoch 66/500
575s - loss: 0.0233 - acc: 0.9955 - val_loss: 0.3561 - val_acc: 0.9232
Epoch 67/500
575s - loss: 0.0246 - acc: 0.9961 - val_loss: 0.3525 - val_acc: 0.9232
Epoch 68/500
575s - loss: 0.0222 - acc: 0.9953 - val_loss: 0.3504 - val_acc: 0.9253
Epoch 69/500
575s - loss: 0.0239 - acc: 0.9953 - val_loss: 0.3525 - val_acc: 0.9242
Epoch 70/500
575s - loss: 0.0193 - acc: 0.9968 - val_loss: 0.3527 - val_acc: 0.9242
Epoch 71/500
575s - loss: 0.0246 - acc: 0.9939 - val_loss: 0.3525 - val_acc: 0.9221
Epoch 72/500
575s - loss: 0.0271 - acc: 0.9942 - val_loss: 0.3536 - val_acc: 0.9221
Epoch 73/500
575s - loss: 0.0244 - acc: 0.9942 - val_loss: 0.3534 - val_acc: 0.9232
Epoch 74/500
575s - loss: 0.0264 - acc: 0.9921 - val_loss: 0.3536 - val_acc: 0.9242
Epoch 75/500
575s - loss: 0.0255 - acc: 0.9945 - val_loss: 0.3534 - val_acc: 0.9221
Epoch 76/500
575s - loss: 0.0268 - acc: 0.9924 - val_loss: 0.3510 - val_acc: 0.9232
Epoch 77/500
575s - loss: 0.0239 - acc: 0.9958 - val_loss: 0.3539 - val_acc: 0.9263
Epoch 78/500
575s - loss: 0.0268 - acc: 0.9929 - val_loss: 0.3523 - val_acc: 0.9232
Epoch 79/500
575s - loss: 0.0209 - acc: 0.9958 - val_loss: 0.3556 - val_acc: 0.9232
Epoch 80/500
575s - loss: 0.0222 - acc: 0.9950 - val_loss: 0.3541 - val_acc: 0.9232
Epoch 81/500
575s - loss: 0.0306 - acc: 0.9903 - val_loss: 0.3546 - val_acc: 0.9232
Epoch 82/500
574s - loss: 0.0233 - acc: 0.9942 - val_loss: 0.3546 - val_acc: 0.9221
Epoch 83/500
575s - loss: 0.0278 - acc: 0.9918 - val_loss: 0.3552 - val_acc: 0.9221
Epoch 84/500
575s - loss: 0.0292 - acc: 0.9916 - val_loss: 0.3538 - val_acc: 0.9232
Epoch 85/500
575s - loss: 0.0218 - acc: 0.9939 - val_loss: 0.3538 - val_acc: 0.9253
Epoch 86/500
575s - loss: 0.0315 - acc: 0.9913 - val_loss: 0.3537 - val_acc: 0.9242
Epoch 87/500
575s - loss: 0.0268 - acc: 0.9937 - val_loss: 0.3530 - val_acc: 0.9232
Epoch 88/500
575s - loss: 0.0246 - acc: 0.9939 - val_loss: 0.3536 - val_acc: 0.9242
Epoch 89/500
575s - loss: 0.0243 - acc: 0.9953 - val_loss: 0.3539 - val_acc: 0.9242
Epoch 90/500
575s - loss: 0.0319 - acc: 0.9918 - val_loss: 0.3538 - val_acc: 0.9242
Epoch 91/500
575s - loss: 0.0258 - acc: 0.9934 - val_loss: 0.3525 - val_acc: 0.9232
Epoch 92/500
575s - loss: 0.0228 - acc: 0.9953 - val_loss: 0.3520 - val_acc: 0.9232
Epoch 93/500
575s - loss: 0.0243 - acc: 0.9945 - val_loss: 0.3536 - val_acc: 0.9232
Epoch 94/500
575s - loss: 0.0271 - acc: 0.9929 - val_loss: 0.3539 - val_acc: 0.9232
Epoch 95/500
575s - loss: 0.0267 - acc: 0.9945 - val_loss: 0.3537 - val_acc: 0.9253
Epoch 96/500
575s - loss: 0.0294 - acc: 0.9926 - val_loss: 0.3540 - val_acc: 0.9242
Epoch 97/500
575s - loss: 0.0247 - acc: 0.9947 - val_loss: 0.3546 - val_acc: 0.9253
Epoch 98/500
575s - loss: 0.0271 - acc: 0.9924 - val_loss: 0.3523 - val_acc: 0.9242
Epoch 99/500
575s - loss: 0.0226 - acc: 0.9961 - val_loss: 0.3532 - val_acc: 0.9242
Epoch 100/500
575s - loss: 0.0246 - acc: 0.9945 - val_loss: 0.3533 - val_acc: 0.9242
Epoch 101/500
575s - loss: 0.0268 - acc: 0.9916 - val_loss: 0.3544 - val_acc: 0.9232
Epoch 102/500
575s - loss: 0.0237 - acc: 0.9939 - val_loss: 0.3543 - val_acc: 0.9253
Epoch 103/500
575s - loss: 0.0268 - acc: 0.9934 - val_loss: 0.3553 - val_acc: 0.9232
Epoch 104/500
575s - loss: 0.0268 - acc: 0.9947 - val_loss: 0.3535 - val_acc: 0.9232
Epoch 105/500
575s - loss: 0.0228 - acc: 0.9950 - val_loss: 0.3534 - val_acc: 0.9232
Epoch 106/500
575s - loss: 0.0253 - acc: 0.9939 - val_loss: 0.3522 - val_acc: 0.9242
Epoch 107/500
576s - loss: 0.0249 - acc: 0.9937 - val_loss: 0.3523 - val_acc: 0.9263
Epoch 108/500
575s - loss: 0.0260 - acc: 0.9916 - val_loss: 0.3528 - val_acc: 0.9232
Epoch 109/500
575s - loss: 0.0253 - acc: 0.9937 - val_loss: 0.3523 - val_acc: 0.9232
Epoch 110/500
575s - loss: 0.0219 - acc: 0.9953 - val_loss: 0.3532 - val_acc: 0.9242
Epoch 111/500
575s - loss: 0.0248 - acc: 0.9945 - val_loss: 0.3530 - val_acc: 0.9221
Epoch 112/500
575s - loss: 0.0274 - acc: 0.9918 - val_loss: 0.3538 - val_acc: 0.9221
Epoch 113/500
575s - loss: 0.0311 - acc: 0.9937 - val_loss: 0.3545 - val_acc: 0.9253
Epoch 114/500
575s - loss: 0.0276 - acc: 0.9939 - val_loss: 0.3527 - val_acc: 0.9232
Epoch 115/500
575s - loss: 0.0268 - acc: 0.9932 - val_loss: 0.3534 - val_acc: 0.9242
Epoch 116/500
575s - loss: 0.0232 - acc: 0.9945 - val_loss: 0.3550 - val_acc: 0.9221
Epoch 117/500
575s - loss: 0.0197 - acc: 0.9955 - val_loss: 0.3529 - val_acc: 0.9232
Epoch 118/500
575s - loss: 0.0257 - acc: 0.9942 - val_loss: 0.3556 - val_acc: 0.9232
Epoch 119/500
575s - loss: 0.0253 - acc: 0.9947 - val_loss: 0.3563 - val_acc: 0.9232
Epoch 120/500
575s - loss: 0.0220 - acc: 0.9947 - val_loss: 0.3548 - val_acc: 0.9232
Epoch 121/500
574s - loss: 0.0221 - acc: 0.9953 - val_loss: 0.3521 - val_acc: 0.9253
Epoch 122/500
575s - loss: 0.0250 - acc: 0.9950 - val_loss: 0.3557 - val_acc: 0.9232
Epoch 123/500
575s - loss: 0.0199 - acc: 0.9961 - val_loss: 0.3519 - val_acc: 0.9242
Epoch 124/500
575s - loss: 0.0290 - acc: 0.9916 - val_loss: 0.3529 - val_acc: 0.9232
Epoch 125/500
575s - loss: 0.0228 - acc: 0.9955 - val_loss: 0.3532 - val_acc: 0.9232
Epoch 126/500
575s - loss: 0.0242 - acc: 0.9942 - val_loss: 0.3541 - val_acc: 0.9242
Epoch 127/500
575s - loss: 0.0259 - acc: 0.9934 - val_loss: 0.3528 - val_acc: 0.9242
Epoch 128/500
575s - loss: 0.0236 - acc: 0.9937 - val_loss: 0.3533 - val_acc: 0.9232
Epoch 129/500
575s - loss: 0.0248 - acc: 0.9937 - val_loss: 0.3527 - val_acc: 0.9232
Epoch 130/500
574s - loss: 0.0292 - acc: 0.9926 - val_loss: 0.3520 - val_acc: 0.9242
Epoch 131/500
574s - loss: 0.0226 - acc: 0.9961 - val_loss: 0.3535 - val_acc: 0.9232
Training loss for fold 0 is 0.003966647668766152 with percent 99.92105263157895
Testing loss for fold 0 is 0.3538658574850936 with percent 92.63157894736842
 
No saved trial for kfold.
Train on 3800 samples, validate on 950 samples
Epoch 1/500
574s - loss: 0.1103 - acc: 0.9724 - val_loss: 0.0500 - val_acc: 0.9905
Epoch 2/500
574s - loss: 0.1223 - acc: 0.9697 - val_loss: 0.0506 - val_acc: 0.9905
Epoch 3/500
574s - loss: 0.1211 - acc: 0.9697 - val_loss: 0.0506 - val_acc: 0.9905
Epoch 4/500
574s - loss: 0.1126 - acc: 0.9713 - val_loss: 0.0506 - val_acc: 0.9905
Epoch 5/500
574s - loss: 0.1104 - acc: 0.9721 - val_loss: 0.0502 - val_acc: 0.9905
Epoch 6/500
574s - loss: 0.1102 - acc: 0.9737 - val_loss: 0.0500 - val_acc: 0.9905
Epoch 7/500
574s - loss: 0.1054 - acc: 0.9737 - val_loss: 0.0499 - val_acc: 0.9905
Epoch 8/500
574s - loss: 0.1232 - acc: 0.9713 - val_loss: 0.0504 - val_acc: 0.9905
Epoch 9/500
574s - loss: 0.1215 - acc: 0.9716 - val_loss: 0.0500 - val_acc: 0.9905
Epoch 10/500
574s - loss: 0.1197 - acc: 0.9689 - val_loss: 0.0506 - val_acc: 0.9905
Epoch 11/500
574s - loss: 0.1206 - acc: 0.9718 - val_loss: 0.0503 - val_acc: 0.9905
Epoch 12/500
574s - loss: 0.1200 - acc: 0.9726 - val_loss: 0.0501 - val_acc: 0.9905
Epoch 13/500
573s - loss: 0.1148 - acc: 0.9755 - val_loss: 0.0496 - val_acc: 0.9905
Epoch 14/500
574s - loss: 0.1058 - acc: 0.9750 - val_loss: 0.0505 - val_acc: 0.9905
Epoch 15/500
574s - loss: 0.1196 - acc: 0.9724 - val_loss: 0.0508 - val_acc: 0.9905
Epoch 16/500
574s - loss: 0.1196 - acc: 0.9700 - val_loss: 0.0506 - val_acc: 0.9905
Epoch 17/500
574s - loss: 0.1153 - acc: 0.9729 - val_loss: 0.0497 - val_acc: 0.9905
Epoch 18/500
574s - loss: 0.1202 - acc: 0.9734 - val_loss: 0.0499 - val_acc: 0.9905
Epoch 19/500
574s - loss: 0.1147 - acc: 0.9689 - val_loss: 0.0498 - val_acc: 0.9905
Epoch 20/500
574s - loss: 0.1148 - acc: 0.9716 - val_loss: 0.0504 - val_acc: 0.9905
Epoch 21/500
574s - loss: 0.1134 - acc: 0.9750 - val_loss: 0.0507 - val_acc: 0.9905
Epoch 22/500
574s - loss: 0.1084 - acc: 0.9734 - val_loss: 0.0505 - val_acc: 0.9905
Epoch 23/500
574s - loss: 0.1175 - acc: 0.9705 - val_loss: 0.0499 - val_acc: 0.9905
Epoch 24/500
574s - loss: 0.1183 - acc: 0.9729 - val_loss: 0.0501 - val_acc: 0.9905
Epoch 25/500
574s - loss: 0.1183 - acc: 0.9721 - val_loss: 0.0502 - val_acc: 0.9905
Epoch 26/500
574s - loss: 0.1200 - acc: 0.9737 - val_loss: 0.0509 - val_acc: 0.9905
Epoch 27/500
574s - loss: 0.1198 - acc: 0.9739 - val_loss: 0.0500 - val_acc: 0.9905
Epoch 28/500
574s - loss: 0.1134 - acc: 0.9734 - val_loss: 0.0509 - val_acc: 0.9905
Epoch 29/500
574s - loss: 0.1101 - acc: 0.9739 - val_loss: 0.0505 - val_acc: 0.9905
Epoch 30/500
574s - loss: 0.1170 - acc: 0.9716 - val_loss: 0.0506 - val_acc: 0.9905
Epoch 31/500
574s - loss: 0.1249 - acc: 0.9703 - val_loss: 0.0504 - val_acc: 0.9905
Epoch 32/500
574s - loss: 0.1179 - acc: 0.9716 - val_loss: 0.0505 - val_acc: 0.9905
Epoch 33/500
574s - loss: 0.1170 - acc: 0.9711 - val_loss: 0.0504 - val_acc: 0.9905
Epoch 34/500
574s - loss: 0.1129 - acc: 0.9729 - val_loss: 0.0503 - val_acc: 0.9905
Epoch 35/500
574s - loss: 0.1102 - acc: 0.9758 - val_loss: 0.0504 - val_acc: 0.9905
Epoch 36/500
574s - loss: 0.1132 - acc: 0.9726 - val_loss: 0.0501 - val_acc: 0.9905
Epoch 37/500
574s - loss: 0.1131 - acc: 0.9747 - val_loss: 0.0503 - val_acc: 0.9905
Epoch 38/500
574s - loss: 0.1111 - acc: 0.9716 - val_loss: 0.0499 - val_acc: 0.9905
Epoch 39/500
574s - loss: 0.1161 - acc: 0.9697 - val_loss: 0.0503 - val_acc: 0.9905
Epoch 40/500
574s - loss: 0.1204 - acc: 0.9737 - val_loss: 0.0511 - val_acc: 0.9905
Epoch 41/500
574s - loss: 0.1158 - acc: 0.9711 - val_loss: 0.0500 - val_acc: 0.9905
Epoch 42/500
574s - loss: 0.1218 - acc: 0.9716 - val_loss: 0.0505 - val_acc: 0.9905
Epoch 43/500
574s - loss: 0.1087 - acc: 0.9737 - val_loss: 0.0508 - val_acc: 0.9905
Epoch 44/500
574s - loss: 0.1230 - acc: 0.9689 - val_loss: 0.0502 - val_acc: 0.9905
Epoch 45/500
574s - loss: 0.1108 - acc: 0.9755 - val_loss: 0.0503 - val_acc: 0.9905
Epoch 46/500
