Using TensorFlow backend.
/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
 
Found train data with correct size
 
 
Found test data with correct size
 
/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.
  warn("The default mode, 'constant', will be changed to 'reflect' in "
Augmentation data size (4750, 2) (794, 2)
/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_1 (InputLayer)             (None, 51, 51, 3)     0                                            
____________________________________________________________________________________________________
conv2d_1 (Conv2D)                (None, 49, 49, 64)    1792        input_1[0][0]                    
____________________________________________________________________________________________________
batch_normalization_1 (BatchNorm (None, 49, 49, 64)    256         conv2d_1[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_1 (LeakyReLU)        (None, 49, 49, 64)    0           batch_normalization_1[0][0]      
____________________________________________________________________________________________________
conv2d_2 (Conv2D)                (None, 47, 47, 64)    36928       leaky_re_lu_1[0][0]              
____________________________________________________________________________________________________
batch_normalization_2 (BatchNorm (None, 47, 47, 64)    256         conv2d_2[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_2 (LeakyReLU)        (None, 47, 47, 64)    0           batch_normalization_2[0][0]      
____________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)   (None, 23, 23, 64)    0           leaky_re_lu_2[0][0]              
____________________________________________________________________________________________________
conv2d_3 (Conv2D)                (None, 21, 21, 128)   73856       max_pooling2d_1[0][0]            
____________________________________________________________________________________________________
batch_normalization_3 (BatchNorm (None, 21, 21, 128)   512         conv2d_3[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_3 (LeakyReLU)        (None, 21, 21, 128)   0           batch_normalization_3[0][0]      
____________________________________________________________________________________________________
conv2d_4 (Conv2D)                (None, 19, 19, 128)   147584      leaky_re_lu_3[0][0]              
____________________________________________________________________________________________________
batch_normalization_4 (BatchNorm (None, 19, 19, 128)   512         conv2d_4[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_4 (LeakyReLU)        (None, 19, 19, 128)   0           batch_normalization_4[0][0]      
____________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)   (None, 9, 9, 128)     0           leaky_re_lu_4[0][0]              
____________________________________________________________________________________________________
conv2d_5 (Conv2D)                (None, 7, 7, 256)     295168      max_pooling2d_2[0][0]            
____________________________________________________________________________________________________
batch_normalization_5 (BatchNorm (None, 7, 7, 256)     1024        conv2d_5[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_5 (LeakyReLU)        (None, 7, 7, 256)     0           batch_normalization_5[0][0]      
____________________________________________________________________________________________________
conv2d_6 (Conv2D)                (None, 5, 5, 256)     590080      leaky_re_lu_5[0][0]              
____________________________________________________________________________________________________
batch_normalization_6 (BatchNorm (None, 5, 5, 256)     1024        conv2d_6[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_6 (LeakyReLU)        (None, 5, 5, 256)     0           batch_normalization_6[0][0]      
____________________________________________________________________________________________________
conv2d_7 (Conv2D)                (None, 3, 3, 256)     590080      leaky_re_lu_6[0][0]              
____________________________________________________________________________________________________
batch_normalization_7 (BatchNorm (None, 3, 3, 256)     1024        conv2d_7[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_7 (LeakyReLU)        (None, 3, 3, 256)     0           batch_normalization_7[0][0]      
____________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)   (None, 1, 1, 256)     0           leaky_re_lu_7[0][0]              
____________________________________________________________________________________________________
flatten_1 (Flatten)              (None, 256)           0           max_pooling2d_3[0][0]            
____________________________________________________________________________________________________
input_2 (InputLayer)             (None, 2)             0                                            
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 258)           0           flatten_1[0][0]                  
                                                                   input_2[0][0]                    
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 258)           0           concatenate_1[0][0]              
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 128)           33152       dropout_1[0][0]                  
____________________________________________________________________________________________________
batch_normalization_8 (BatchNorm (None, 128)           512         dense_1[0][0]                    
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 128)           0           batch_normalization_8[0][0]      
____________________________________________________________________________________________________
dropout_2 (Dropout)              (None, 128)           0           activation_1[0][0]               
________________________________________________________________________________________________________
dense_2 (Dense)                  (None, 12)            1548        dropout_2[0][0]                  
____________________________________________________________________________________________________
batch_normalization_9 (BatchNorm (None, 12)            48          dense_2[0][0]                    
____________________________________________________________________________________________________
activation_2 (Activation)        (None, 12)            0           batch_normalization_9[0][0]      
====================================================================================================
Total params: 1,788,156
Trainable params: 1,785,572
Non-trainable params: 2,584
____________________________________________________________________________________________________
 
Pulling kfold 0 from previous runs
2018-08-09 12:16:42.505439: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-08-09 12:16:42.505692: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
 
Bad saved trial. Testing acc <0.9%. Rerunning ...
 
Train on 3800 samples, validate on 950 samples
Epoch 1/500
577s - loss: 0.9800 - acc: 0.6666 - val_loss: 1.3187 - val_acc: 0.5684
Epoch 2/500
576s - loss: 0.7820 - acc: 0.7289 - val_loss: 1.8504 - val_acc: 0.4926
Epoch 3/500
576s - loss: 0.6290 - acc: 0.7787 - val_loss: 0.9237 - val_acc: 0.6842
Epoch 4/500
576s - loss: 0.5568 - acc: 0.8084 - val_loss: 1.3568 - val_acc: 0.5400
Epoch 5/500
576s - loss: 0.4861 - acc: 0.8279 - val_loss: 0.9304 - val_acc: 0.7200
Epoch 6/500
575s - loss: 0.4185 - acc: 0.8568 - val_loss: 0.9912 - val_acc: 0.6821
Epoch 7/500
575s - loss: 0.4191 - acc: 0.8516 - val_loss: 0.5350 - val_acc: 0.8368
Epoch 8/500
575s - loss: 0.3732 - acc: 0.8632 - val_loss: 0.5801 - val_acc: 0.8200
Epoch 9/500
575s - loss: 0.3590 - acc: 0.8637 - val_loss: 0.5175 - val_acc: 0.8263
Epoch 10/500
575s - loss: 0.2972 - acc: 0.8842 - val_loss: 0.6859 - val_acc: 0.7874
Epoch 11/500
575s - loss: 0.2804 - acc: 0.8968 - val_loss: 1.0468 - val_acc: 0.6821
Epoch 12/500
575s - loss: 0.2606 - acc: 0.9061 - val_loss: 0.4722 - val_acc: 0.8453
Epoch 13/500
575s - loss: 0.2496 - acc: 0.9074 - val_loss: 0.6951 - val_acc: 0.8126
Epoch 14/500
575s - loss: 0.2358 - acc: 0.9111 - val_loss: 0.6566 - val_acc: 0.8232
Epoch 15/500
575s - loss: 0.2171 - acc: 0.9182 - val_loss: 0.6297 - val_acc: 0.8126
Epoch 16/500
575s - loss: 0.2016 - acc: 0.9211 - val_loss: 0.5540 - val_acc: 0.8200
Epoch 17/500
575s - loss: 0.1724 - acc: 0.9334 - val_loss: 0.7216 - val_acc: 0.8116
Epoch 18/500
575s - loss: 0.1803 - acc: 0.9316 - val_loss: 0.4533 - val_acc: 0.8621
Epoch 19/500
577s - loss: 0.1200 - acc: 0.9603 - val_loss: 0.3251 - val_acc: 0.9053
Epoch 20/500
575s - loss: 0.0876 - acc: 0.9708 - val_loss: 0.3160 - val_acc: 0.9084
Epoch 21/500
575s - loss: 0.0843 - acc: 0.9703 - val_loss: 0.3201 - val_acc: 0.9105
Epoch 22/500
575s - loss: 0.0762 - acc: 0.9739 - val_loss: 0.3343 - val_acc: 0.9074
Epoch 23/500
575s - loss: 0.0748 - acc: 0.9745 - val_loss: 0.3620 - val_acc: 0.9011
Epoch 24/500
575s - loss: 0.0682 - acc: 0.9771 - val_loss: 0.3477 - val_acc: 0.9147
Epoch 25/500
575s - loss: 0.0703 - acc: 0.9792 - val_loss: 0.3438 - val_acc: 0.9126
Epoch 26/500
574s - loss: 0.0641 - acc: 0.9782 - val_loss: 0.3312 - val_acc: 0.9116
Epoch 27/500
575s - loss: 0.0591 - acc: 0.9808 - val_loss: 0.3408 - val_acc: 0.9168
Epoch 28/500
575s - loss: 0.0520 - acc: 0.9858 - val_loss: 0.3538 - val_acc: 0.8979
Epoch 29/500
574s - loss: 0.0575 - acc: 0.9805 - val_loss: 0.3354 - val_acc: 0.9105
Epoch 30/500
575s - loss: 0.0562 - acc: 0.9824 - val_loss: 0.3189 - val_acc: 0.9179
Epoch 31/500
575s - loss: 0.0502 - acc: 0.9853 - val_loss: 0.3520 - val_acc: 0.9147
Epoch 32/500
575s - loss: 0.0475 - acc: 0.9861 - val_loss: 0.3668 - val_acc: 0.9105
Epoch 33/500
575s - loss: 0.0494 - acc: 0.9858 - val_loss: 0.3528 - val_acc: 0.9189
Epoch 34/500
575s - loss: 0.0436 - acc: 0.9889 - val_loss: 0.3298 - val_acc: 0.9242
Epoch 35/500
575s - loss: 0.0326 - acc: 0.9924 - val_loss: 0.3263 - val_acc: 0.9263
Epoch 36/500
575s - loss: 0.0357 - acc: 0.9924 - val_loss: 0.3271 - val_acc: 0.9232
Epoch 37/500
575s - loss: 0.0404 - acc: 0.9884 - val_loss: 0.3328 - val_acc: 0.9242
Epoch 38/500
575s - loss: 0.0324 - acc: 0.9926 - val_loss: 0.3366 - val_acc: 0.9232
Epoch 39/500
575s - loss: 0.0313 - acc: 0.9918 - val_loss: 0.3423 - val_acc: 0.9232
Epoch 40/500
575s - loss: 0.0313 - acc: 0.9897 - val_loss: 0.3430 - val_acc: 0.9242
Epoch 41/500
575s - loss: 0.0311 - acc: 0.9929 - val_loss: 0.3412 - val_acc: 0.9232
Epoch 42/500
575s - loss: 0.0252 - acc: 0.9950 - val_loss: 0.3461 - val_acc: 0.9263
Epoch 43/500
575s - loss: 0.0270 - acc: 0.9934 - val_loss: 0.3411 - val_acc: 0.9242
Epoch 44/500
575s - loss: 0.0300 - acc: 0.9918 - val_loss: 0.3386 - val_acc: 0.9242
Epoch 45/500
575s - loss: 0.0265 - acc: 0.9947 - val_loss: 0.3383 - val_acc: 0.9253
Epoch 46/500
575s - loss: 0.0299 - acc: 0.9929 - val_loss: 0.3460 - val_acc: 0.9221
Epoch 47/500
575s - loss: 0.0336 - acc: 0.9924 - val_loss: 0.3565 - val_acc: 0.9189
Epoch 48/500
575s - loss: 0.0307 - acc: 0.9916 - val_loss: 0.3530 - val_acc: 0.9211
Epoch 49/500
575s - loss: 0.0264 - acc: 0.9932 - val_loss: 0.3523 - val_acc: 0.9242
Epoch 50/500
575s - loss: 0.0233 - acc: 0.9953 - val_loss: 0.3562 - val_acc: 0.9221
Epoch 51/500
575s - loss: 0.0279 - acc: 0.9939 - val_loss: 0.3508 - val_acc: 0.9211
Epoch 52/500
574s - loss: 0.0272 - acc: 0.9929 - val_loss: 0.3518 - val_acc: 0.9232
Epoch 53/500
574s - loss: 0.0286 - acc: 0.9934 - val_loss: 0.3544 - val_acc: 0.9221
Epoch 54/500
575s - loss: 0.0254 - acc: 0.9934 - val_loss: 0.3524 - val_acc: 0.9211
Epoch 55/500
575s - loss: 0.0230 - acc: 0.9968 - val_loss: 0.3585 - val_acc: 0.9221
Epoch 56/500
575s - loss: 0.0276 - acc: 0.9932 - val_loss: 0.3537 - val_acc: 0.9232
Epoch 57/500
575s - loss: 0.0249 - acc: 0.9929 - val_loss: 0.3548 - val_acc: 0.9242
Epoch 58/500
575s - loss: 0.0231 - acc: 0.9947 - val_loss: 0.3576 - val_acc: 0.9232
Epoch 59/500
575s - loss: 0.0280 - acc: 0.9932 - val_loss: 0.3582 - val_acc: 0.9242
Epoch 60/500
575s - loss: 0.0241 - acc: 0.9947 - val_loss: 0.3587 - val_acc: 0.9242
Epoch 61/500
576s - loss: 0.0265 - acc: 0.9937 - val_loss: 0.3548 - val_acc: 0.9221
Epoch 62/500
