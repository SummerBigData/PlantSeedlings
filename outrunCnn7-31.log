Using TensorFlow backend.
/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Found data with correct size
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 100, 100, 3)       0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 98, 98, 16)        448       
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 96, 96, 16)        2320      
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 48, 48, 16)        0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 48, 48, 16)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 46, 46, 32)        4640      
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 44, 44, 32)        9248      
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 22, 22, 32)        0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 22, 22, 32)        0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 20, 20, 64)        18496     
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 18, 18, 64)        36928     
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 9, 9, 64)          0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 9, 9, 64)          0         
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 7, 7, 128)         73856     
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 3, 3, 128)         0         
_________________________________________________________________
dropout_4 (Dropout)          (None, 3, 3, 128)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 1152)              0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 1152)              4608      
_________________________________________________________________
dense_1 (Dense)              (None, 256)               295168    
_________________________________________________________________
dropout_5 (Dropout)          (None, 256)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 64)                16448     
_________________________________________________________________
dropout_6 (Dropout)          (None, 64)                0         
_________________________________________________________________
dense_3 (Dense)              (None, 12)                780       
=================================================================
Total params: 462,940
Trainable params: 460,636
Non-trainable params: 2,304
_________________________________________________________________
2018-07-31 17:09:15.664463: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-07-31 17:09:15.664490: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
(3562,)
Train on 3562 samples, validate on 1188 samples
Epoch 1/300
216s - loss: 2.4916 - acc: 0.1221 - val_loss: 3.7631 - val_acc: 0.0918
Epoch 2/300
214s - loss: 2.3224 - acc: 0.1662 - val_loss: 2.6107 - val_acc: 0.1330
Epoch 3/300
215s - loss: 2.1821 - acc: 0.2019 - val_loss: 2.2836 - val_acc: 0.2138
Epoch 4/300
215s - loss: 2.0726 - acc: 0.2572 - val_loss: 8.3192 - val_acc: 0.0572
Epoch 5/300
214s - loss: 2.0318 - acc: 0.2611 - val_loss: 3.2950 - val_acc: 0.2231
Epoch 6/300
216s - loss: 1.9827 - acc: 0.2448 - val_loss: 3.7196 - val_acc: 0.1515
Epoch 7/300
216s - loss: 1.9126 - acc: 0.2476 - val_loss: 2.6701 - val_acc: 0.2088
Epoch 8/300
216s - loss: 1.8572 - acc: 0.2715 - val_loss: 1.6945 - val_acc: 0.3241
Epoch 9/300
216s - loss: 1.8407 - acc: 0.2886 - val_loss: 2.6994 - val_acc: 0.2567
Epoch 10/300
216s - loss: 1.7390 - acc: 0.3307 - val_loss: 1.5887 - val_acc: 0.3258
Epoch 11/300
214s - loss: 1.6514 - acc: 0.3324 - val_loss: 1.4943 - val_acc: 0.4689
Epoch 12/300
214s - loss: 1.5738 - acc: 0.4012 - val_loss: 1.5460 - val_acc: 0.4747
Epoch 13/300
216s - loss: 1.5057 - acc: 0.4728 - val_loss: 1.4077 - val_acc: 0.5236
Epoch 14/300
215s - loss: 1.4213 - acc: 0.5059 - val_loss: 1.2921 - val_acc: 0.5539
Epoch 15/300
215s - loss: 1.3472 - acc: 0.5295 - val_loss: 1.2059 - val_acc: 0.5673
Epoch 16/300
215s - loss: 1.5088 - acc: 0.4891 - val_loss: 2.3685 - val_acc: 0.2492
Epoch 17/300
215s - loss: 1.4519 - acc: 0.4930 - val_loss: 1.3299 - val_acc: 0.5337
Epoch 18/300
215s - loss: 1.3180 - acc: 0.5328 - val_loss: 1.2092 - val_acc: 0.5825
Epoch 19/300
215s - loss: 2.1651 - acc: 0.2622 - val_loss: 7.3311 - val_acc: 0.1288
Epoch 20/300
215s - loss: 2.2398 - acc: 0.2266 - val_loss: 2.2788 - val_acc: 0.1995
Epoch 21/300
213s - loss: 2.1843 - acc: 0.2544 - val_loss: 3.8919 - val_acc: 0.0993
Epoch 22/300
213s - loss: 2.0787 - acc: 0.2889 - val_loss: 4.5882 - val_acc: 0.1818
Epoch 23/300
213s - loss: 1.9409 - acc: 0.3307 - val_loss: 4.1424 - val_acc: 0.2374
Epoch 24/300
213s - loss: 1.8513 - acc: 0.3630 - val_loss: 4.0256 - val_acc: 0.2332
Epoch 25/300
213s - loss: 1.8218 - acc: 0.3638 - val_loss: 3.2381 - val_acc: 0.2879
Epoch 26/300
213s - loss: 1.7938 - acc: 0.3739 - val_loss: 4.0346 - val_acc: 0.1524
Epoch 27/300
214s - loss: 1.9661 - acc: 0.3206 - val_loss: 4.0878 - val_acc: 0.1288
Epoch 28/300
214s - loss: 1.9388 - acc: 0.3349 - val_loss: 3.4620 - val_acc: 0.1953
Epoch 29/300
213s - loss: 1.9114 - acc: 0.3442 - val_loss: 4.4565 - val_acc: 0.1338
Epoch 30/300
213s - loss: 1.8972 - acc: 0.3470 - val_loss: 3.2969 - val_acc: 0.2138
Epoch 31/300
213s - loss: 1.8726 - acc: 0.3492 - val_loss: 4.2473 - val_acc: 0.1835
Epoch 32/300
213s - loss: 1.8338 - acc: 0.3551 - val_loss: 4.0156 - val_acc: 0.1995
Epoch 33/300
213s - loss: 1.7976 - acc: 0.3703 - val_loss: nan - val_acc: 0.2618
Epoch 34/300
214s - loss: 1.7646 - acc: 0.3829 - val_loss: nan - val_acc: 0.2138
Epoch 35/300
213s - loss: 1.7242 - acc: 0.3838 - val_loss: nan - val_acc: 0.2155
Epoch 36/300
213s - loss: 1.7187 - acc: 0.4017 - val_loss: nan - val_acc: 0.2163
Epoch 37/300
213s - loss: 1.6453 - acc: 0.4264 - val_loss: nan - val_acc: 0.2567
Epoch 38/300
213s - loss: 1.6544 - acc: 0.4194 - val_loss: nan - val_acc: 0.2710
Epoch 39/300
213s - loss: 1.5904 - acc: 0.4326 - val_loss: nan - val_acc: 0.2938
Epoch 40/300
213s - loss: 1.5562 - acc: 0.4517 - val_loss: nan - val_acc: 0.3384
Epoch 41/300
213s - loss: 1.5087 - acc: 0.4576 - val_loss: nan - val_acc: 0.3367
Epoch 42/300
215s - loss: 1.4719 - acc: 0.4882 - val_loss: nan - val_acc: 0.2929
Epoch 43/300
214s - loss: 1.4470 - acc: 0.4789 - val_loss: nan - val_acc: 0.3426
Epoch 44/300
213s - loss: 1.4333 - acc: 0.4919 - val_loss: nan - val_acc: 0.3468
Epoch 45/300
213s - loss: 1.4091 - acc: 0.4930 - val_loss: nan - val_acc: 0.3064
Epoch 46/300
213s - loss: 1.3921 - acc: 0.5000 - val_loss: 2.3888 - val_acc: 0.3342
Epoch 47/300
213s - loss: 1.3715 - acc: 0.5115 - val_loss: nan - val_acc: 0.4891
Epoch 48/300
213s - loss: 1.3732 - acc: 0.5065 - val_loss: 2.2301 - val_acc: 0.3375
Epoch 49/300
213s - loss: 1.3718 - acc: 0.5087 - val_loss: nan - val_acc: 0.2786
Epoch 50/300
213s - loss: 1.3426 - acc: 0.5152 - val_loss: 1.3409 - val_acc: 0.5556
Epoch 51/300
213s - loss: 1.3140 - acc: 0.5314 - val_loss: 1.3707 - val_acc: 0.5286
Epoch 52/300
213s - loss: 1.3016 - acc: 0.5376 - val_loss: nan - val_acc: 0.3914
Epoch 53/300
213s - loss: 1.2500 - acc: 0.5500 - val_loss: nan - val_acc: 0.4394
Epoch 54/300
213s - loss: 1.2307 - acc: 0.5553 - val_loss: nan - val_acc: 0.4764
Epoch 55/300
213s - loss: 1.1880 - acc: 0.5786 - val_loss: nan - val_acc: 0.5926
Epoch 56/300
213s - loss: 1.1975 - acc: 0.5831 - val_loss: 1.3202 - val_acc: 0.5471
Traceback (most recent call last):
  File "cnn.py", line 57, in <module>
    scores = model.evaluate(xtr, ytr, verbose=0)
  File "/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/keras/engine/training.py", line 1541, in evaluate
    batch_size=batch_size)
  File "/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/keras/engine/training.py", line 1315, in _standardize_user_data
    exception_prefix='target')
  File "/users/PAS1383/osu10171/.conda/envs/local/lib/python2.7/site-packages/keras/engine/training.py", line 139, in _standardize_input_data
    str(array.shape))
ValueError: Error when checking target: expected dense_3 to have shape (None, 12) but got array with shape (3562, 1)
